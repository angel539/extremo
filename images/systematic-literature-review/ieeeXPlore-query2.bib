@ARTICLE{5222797,
author={N. Bolloju},
journal={IEEE Software},
title={Conceptual Modeling of Systems Integration Requirements},
year={2009},
volume={26},
number={5},
pages={66-74},
abstract={Integrated systems enable capabilities and performance that independent systems cannot offer. Systems integration, or enterprise application integration, establishes linkages between different computer-based information systems and databases and thus is often required to achieve business integration.There is no conceptual modeling technique designed to help systems integrators elicit, represent, and analyze enterprise-wide integration requirements. The technique proposed here is shown to be structured and easy to use.},
keywords={business data processing;formal specification;information systems;software architecture;systems analysis;Web services;system integration requirement;conceptual modeling;computer-based information system;computer-based database;business integration;enterprise-wide integration requirement;formal specification;service-oriented architecture;Application software;Unified modeling language;Couplings;Information systems;Computer architecture;Databases;Information analysis;Publish-subscribe;Pattern analysis;Books;conceptual modeling;requirements specification;integration requirements},
doi={10.1109/MS.2009.123},
ISSN={0740-7459},
month={Sept},}
@ARTICLE{8316849,
author={G. Yu and J. Liu and J. Du and M. Hu and V. Sugumaran},
journal={IEEE Access},
title={An Integrated Approach for Massive Sequential Data Processing in Civil Infrastructure Operation and Maintenance},
year={2018},
volume={6},
number={},
pages={19739-19751},
abstract={This paper presents an extract-transform-load (ETL) approach based on multilayer task execution for processing massive sequential data collected from infrastructure operation and maintenance. The proposed approach consists of ETL task partition, execution mode selection, and ETL modeling. The task partition focuses on dividing the ETL process into four tasks to be executed in accordance with different organizational forms of data. Sequenced or non-sequenced load mode is optional, which is independent of the data standardization. In addition, the ETL modeling phase implements conceptual, logical, and physical modeling for the multi-dimensional model. Our main objective is to integrate massive sequential data, enhancing decision-making performance for the intelligent management platform. Traffic data for two years were collected from various systems and acquisition tools of different providers to evaluate the data integration capability of the proposed approach. Furthermore, Kettle software was used to perform transformation and job modules for the multilayer tasks. In addition, a machine learning algorithm was used to generate traffic warning in the tunnels based on the integrated data. The proposed approach is promising for the management and analysis of massive sequential data generated in operation, the maintenance of transportation tunnels, and effective decision-making.},
keywords={data integration;decision making;learning (artificial intelligence);traffic engineering computing;tunnels;physical modeling;multidimensional model;traffic data;data integration capability;multilayer tasks;integrated approach;massive sequential data processing;civil infrastructure operation;extract-transform-load approach;multilayer task execution;ETL task partition;execution mode selection;ETL process;load mode;data standardization;traffic warning;machine learning algorithm;Kettle software;decision-making performance;ETL modeling phase;civil infrastructure maintenance;Data warehouses;Maintenance engineering;Unified modeling language;Data integration;Tools;Data models;Task analysis;Civil infrastructure;massive data integration;sequential analysis;maintenance and operation management},
doi={10.1109/ACCESS.2018.2816001},
ISSN={2169-3536},
month={},}
@ARTICLE{7093036,
author={J. Di Rocco and D. Di Ruscio and L. Iovino and A. Pierantonio},
journal={IEEE Software},
title={Collaborative Repositories in Model-Driven Engineering [Software Technology]},
year={2015},
volume={32},
number={3},
pages={28-34},
abstract={Recently proposed model repositories aim to support specific needs--for example, collaborative modeling, the ability to use different modeling tools in software life-cycle management, tool interoperability, increased model reuse, and the integration of heterogeneous models.},
keywords={open systems;software development management;software reusability;collaborative repositories;model-driven engineering;collaborative modeling;software life-cycle management;tool interoperability;model reuse;heterogeneous model integration;Unified modeling language;Software engineering;Adaptation models;Collaboration;Model driven engineering;Interoperability;Software development;model-driven engineering;MDE;model repositories;software engineering;software development;MDEForge},
doi={10.1109/MS.2015.61},
ISSN={0740-7459},
month={May},}
@ARTICLE{7862127,
author={B. Ma and T. Jiang and X. Zhou and F. Zhao and Y. Yang},
journal={IEEE Access},
title={A Novel Data Integration Framework Based on Unified Concept Model},
year={2017},
volume={5},
number={},
pages={5713-5722},
abstract={Nowadays, data is being generated, collected, and analyzed at an unprecedented scale, data integration is the problem of combining data from heterogeneous, autonomous data sources, and providing users with a unified view of integrated data. To design a data integration framework, we need to address challenges, such as schema mapping, data cleaning, record linkage, and data fusion. In this paper, we briefly introduce the traditional data integration approaches, and then, a novel graph-based data integration framework based on unified concept model (UCM) is proposed to address real-world refueling data integration problems. Within this framework, schema mapping was carried out and metadata from heterogeneous sources is integrated in a UCM. UCM has the benefits of being easy to update. It is also important for effective schema mapping and data transformation. By following the structure of UCM, data from different sources is automatically transformed into instance data and linked together by using semantic similarity computation metrics, finally the data is stored in graph database. Experiments are carried out based on heterogeneous data from refueling records, social networks of astroturfers, and vehicle trajectories. Experimental results and reference implementation demonstrations show good precision and recall of the proposed framework.},
keywords={data integration;graph theory;unified concept model;novel graph-based data integration framework;UCM;schema mapping;metadata;semantic similarity computation metrics;graph database;Data integration;Data models;Couplings;Distributed databases;Data visualization;Physics;Chemistry;Data integration;schema mapping;graph model;semantic similarity computation},
doi={10.1109/ACCESS.2017.2672822},
ISSN={2169-3536},
month={},}
@ARTICLE{8306984,
author={A. Amjad and F. Azam and M. W. Anwar and W. H. Butt and M. Rashid},
journal={IEEE Access},
title={Event-Driven Process Chain for Modeling and Verification of Business Requirementsâ€“A Systematic Literature Review},
year={2018},
volume={6},
number={},
pages={9027-9048},
abstract={Automation of any business process primarily requires the identification of clear and precise requirements. However, the initially collected business requirements are usually expressed in natural language that creates ambiguities among different stakeholders. To overcome this problem, various business process modeling languages (BPMLs) have been introduced to represent the business requirements graphically. In this context, event-driven process chain (EPC) is a well-known BPML that supports the modeling and verification of business requirements in early automation phases. Although EPC is frequently researched to improve its modeling and verification capabilities, there is no study available yet to the best of our knowledge that examines and summarizes the latest EPC developments. Therefore, in this article, we comprehensively investigate the latest EPC approaches, trends, and tools for the modeling and verification of business requirements. Particularly, a systematic literature review is carried out to select and analyze 73 research studies published during 1998-2017. Consequently, the selected studies are classified into six categories, i.e., modeling (14), transformation (13), verification (17), general (20), semantics (5), and requirement (4). Moreover, latest EPC modeling approaches are identified and analyzed, i.e., UML (2), meta-model (3), integration (5), and EPC notations (4). Furthermore, EPC verification methods are also investigated, i.e., EPC (6), petri-nets (8), and other languages (3). Finally, 25 leading EPC tools have been presented, i.e., existing tools (14), proposed/developed tools, (5) and additional tools (6). It has been concluded that EPC provides adequate approaches and tool support for the modeling and verification of simple business requirements through atomic events. However, the complex business requirements cannot be modeled and verified through EPC due to the lack of complex event processing. Consequently, there is a strong need to include the support for the modeling and verification of complex events in EPC to manage multifaceted business requirements.},
keywords={business data processing;formal specification;Petri nets;Unified Modeling Language;meta-model;EPC notations;EPC verification methods;complex event processing;Petri-nets;event-driven process chain;Business Process Modeling Languages;BPML;business requirements verification;UML;Unified modeling language;Business;Tools;Analytical models;Semantics;Data models;Object oriented modeling;BPML;EPC;SLR;EPC verification;EPC tools},
doi={10.1109/ACCESS.2018.2791666},
ISSN={2169-3536},
month={},}
@ARTICLE{6867358,
author={M. Å½itnik and B. Zupan},
journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
title={Data Fusion by Matrix Factorization},
year={2015},
volume={37},
number={1},
pages={41-53},
abstract={For most problems in science and engineering we can obtain data sets that describe the observed system from various perspectives and record the behavior of its individual components. Heterogeneous data sets can be collectively mined by data fusion. Fusion can focus on a specific target relation and exploit directly associated data together with contextual data and data about system's constraints. In the paper we describe a data fusion approach with penalized matrix tri-factorization (DFMF) that simultaneously factorizes data matrices to reveal hidden associations. The approach can directly consider any data that can be expressed in a matrix, including those from feature-based representations, ontologies, associations and networks. We demonstrate the utility of DFMF for gene function prediction task with eleven different data sources and for prediction of pharmacologic actions by fusing six data sources. Our data fusion algorithm compares favorably to alternative data integration approaches and achieves higher accuracy than can be obtained from any single data source alone.},
keywords={biology computing;data integration;genetics;matrix decomposition;ontologies (artificial intelligence);sensor fusion;data fusion;matrix factorization;heterogeneous data sets;target relation;contextual data;penalized matrix trifactorization;DFMF;feature-based representations;ontologies;gene function prediction task;pharmacologic actions;data integration approaches;Data integration;Data models;Convergence;Approximation methods;Diseases;Linear programming;Predictive models;Data fusion;intermediate data integration;matrix factorization;data mining;bioinformatics;cheminformatics;Algorithms;Informatics;Models, Theoretical},
doi={10.1109/TPAMI.2014.2343973},
ISSN={0162-8828},
month={Jan},}
@ARTICLE{7983363,
author={A. A. Khan and J. W. Keung and Fazal-E-Amin and M. Abdullah-Al-Wadud},
journal={IEEE Access},
title={SPIIMM: Toward a Model for Software Process Improvement Implementation and Management in Global Software Development},
year={2017},
volume={5},
number={},
pages={13720-13741},
abstract={Software development organizations are globalizing their activities by adopting the phenomenon of global software development (GSD), mainly due to the significant return on investment it offers. Various challenges are associated with the software process improvement (SPI). The aim of this paper is to develop a software process improvement implementation and management model (SPIIMM) that can assist GSD organizations in assessing and improving their SPI activities. A thorough systematic literature review (SLR) study was performed to identify the critical success factors (CSFs), critical barriers (CBs), and the relevant practices of SPI. An empirical study of the industry was conducted with 111 SPI experts using a survey questionnaire to verify the outcomes of the SLR. The final CSFs and CBs were categorized into five maturity levels based on the implementation maturity model, the software outsourcing vendor readiness model, and capability maturity model integration. Each maturity level consisted of different CSFs and CBs to assess and improve the SPI-related maturity level of an organization. Three case studies were conducted to evaluate the effectiveness of the proposed model. The results revealed that SPIIMM can provide a robust framework to assess and improve SPI activities in GSD organizations.},
keywords={Capability Maturity Model;globalisation;outsourcing;software development management;SPIIMM;software process improvement implementation and management;global software development;software development organizations;return on investment;systematic literature review;critical success factors;critical barriers;implementation maturity model;software outsourcing vendor readiness model;capability maturity model integration;SPI-related maturity level;Software;Organizations;Standards organizations;Capability maturity model;Industries;ISO Standards;Software process improvement;global software development;success factors;barriers;practices;systematic literature review},
doi={10.1109/ACCESS.2017.2728603},
ISSN={2169-3536},
month={},}
@ARTICLE{6489979,
author={T. Mu and J. Y. Goulermas},
journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
title={Automatic Generation of Co-Embeddings from Relational Data with Adaptive Shaping},
year={2013},
volume={35},
number={10},
pages={2340-2356},
abstract={In this paper, we study the co-embedding problem of how to map different types of patterns into one common low-dimensional space, given only the associations (relation values) between samples. We conduct a generic analysis to discover the commonalities between existing co-embedding algorithms and indirectly related approaches and investigate possible factors controlling the shapes and distributions of the co-embeddings. The primary contribution of this work is a novel method for computing co--embeddings, termed the automatic co-embedding with adaptive shaping (ACAS) algorithm, based on an efficient transformation of the co-embedding problem. Its advantages include flexible model adaptation to the given data, an economical set of model variables leading to a parametric co-embedding formulation, and a robust model fitting criterion for model optimization based on a quantization procedure. The secondary contribution of this work is the introduction of a set of generic schemes for the qualitative analysis and quantitative assessment of the output of co-embedding algorithms, using existing labeled benchmark datasets. Experiments with synthetic and real-world datasets show that the proposed algorithm is very competitive compared to existing ones.},
keywords={data visualisation;optimisation;pattern classification;relational databases;automatic coembedding generation;relational data;pattern mapping;low dimensional space;commonality discovery;automatic coembedding with adaptive shaping algorithm;ACAS algorithm;coembedding problem transformation;flexible model adaptation;economical model variable set;parametric coembedding formulation;robust model fitting criterion;model optimization;quantization procedure;qualitative analysis;quantitative assessment;labeled benchmark datasets;data visualization;Vectors;Computational modeling;Algorithm design and analysis;Eigenvalues and eigenfunctions;Large scale integration;Adaptation models;Data models;Relational data;data co-embedding;heterogeneous embedding;data visualization;structural matching;Algorithms;Artificial Intelligence;Image Interpretation, Computer-Assisted;Pattern Recognition, Automated;Reproducibility of Results;Sensitivity and Specificity},
doi={10.1109/TPAMI.2013.66},
ISSN={0162-8828},
month={Oct},}
@ARTICLE{7725234,
author={P. O. Antonino and A. Morgenstern and T. Kuhn},
journal={IEEE Software},
title={Embedded-Software Architects: It's Not Only about the Software},
year={2016},
volume={33},
number={6},
pages={56-62},
abstract={Owing to the increasing amount of computation in electromechanical devices, the role of software architect is often found in embedded-systems development. However, because computer scientists usually have limited knowledge of embedded-systems concepts such as controllers, actuators, and buses, embedded-software architects are often engineers with no education in software architecture basics, which is normally a topic in computer science courses. In these environments, serious architectural problems can occur, such as contradictory architecture decisions and inconsistencies between the architecture design and the architecture drivers. This article discusses the current profile of embedded-software architects, characteristics of embedded architectures designed by architects with no computer science background, and the shortcomings of architects whose knowledge is limited to information systems. The authors also discuss how to overcome these challenges.},
keywords={embedded systems;personnel;software architecture;software development management;embedded software architects;electromechanical devices;embedded systems development;software architecture;Computer architecture;Software architecture;Electromechanical devices;Computational modeling;system architecture;integration and modeling;software architecture;real-time and embedded systems;domain-specific architectures;software development;software engineering;software architect},
doi={10.1109/MS.2016.142},
ISSN={0740-7459},
month={Nov},}
@ARTICLE{6786502,
author={M. Filippone and M. Girolami},
journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
title={Pseudo-Marginal Bayesian Inference for Gaussian Processes},
year={2014},
volume={36},
number={11},
pages={2214-2226},
abstract={The main challenges that arise when adopting Gaussian process priors in probabilistic modeling are how to carry out exact Bayesian inference and how to account for uncertainty on model parameters when making model-based predictions on out-of-sample data. Using probit regression as an illustrative working example, this paper presents a general and effective methodology based on the pseudo-marginal approach to Markov chain Monte Carlo that efficiently addresses both of these issues. The results presented in this paper show improvements over existing sampling methods to simulate from the posterior distribution over the parameters defining the covariance function of the Gaussian Process prior. This is particularly important as it offers a powerful tool to carry out full Bayesian inference of Gaussian Process based hierarchic statistical models in general. The results also demonstrate that Monte Carlo based integration of all model parameters is actually feasible in this class of models providing a superior quantification of uncertainty in predictions. Extensive comparisons with respect to state-of-the-art probabilistic classifiers confirm this assertion.},
keywords={belief networks;Gaussian processes;inference mechanisms;Markov processes;Monte Carlo methods;pattern classification;pseudo-marginal Bayesian inference;probabilistic modeling;model-based predictions;probit regression;Markov chain Monte Carlo;Gaussian process based hierarchic statistical models;Monte Carlo based integration;probabilistic classifiers;Approximation methods;Monte Carlo methods;Bayes methods;Predictive models;Gaussian processes;Data models;Uncertainty;Hierarchic Bayesian models;Gaussian processes;Markov chain Monte Carlo;pseudo-marginal Monte Carlo;Kernel methods;approximate Bayesian inference},
doi={10.1109/TPAMI.2014.2316530},
ISSN={0162-8828},
month={Nov},}
@ARTICLE{8402217,
author={Z. Zhang and T. Si and S. Liu},
journal={IEEE Access},
title={Integration Convolutional Neural Network for Person Re-Identification in Camera Networks},
year={2018},
volume={6},
number={},
pages={36887-36896},
abstract={In this paper, we propose a novel deep model named integration convolutional neural network (ICNN) for person re-identification in camera networks, which jointly learns global and local features in a unified framework. To this end, the proposed ICNN simultaneously applies two kinds of loss functions. Specifically, we propose the soft triplet loss to learn global features which automatically adjusts the margin threshold within one batch. The soft triplet loss could alleviate the difficult in tuning parameters and therefore learns discriminative global features. In order to avoid the part misalignment problem, we learn latent local features by conducting local horizontal average pooling on the convolutional maps. Afterward, we implement the identification task on each local feature. We concatenate global and local features using a weighted strategy to present the pedestrian images. We evaluate the proposed ICNN on three large-scale databases. Our method achieves rank-1 accuracy of 92.13% on Market 1501, 61.4% onCUHK03 and 85.3% on DukeMTMC-reID, and the results outperform the state-of-the-art methods.},
keywords={cameras;convolution;feature extraction;feedforward neural nets;learning (artificial intelligence);visual databases;large scale databases;pedestrian images;misalignment problem;tuning parameters;margin threshold;unified framework;person re identification;soft triplet loss;camera networks;integration convolutional neural network;ICNN;local horizontal average pooling;latent local features;Feature extraction;Task analysis;Cameras;Convolutional neural networks;Loss measurement;Machine learning;Databases;Camera networks;person re-identification;convolutional neural network},
doi={10.1109/ACCESS.2018.2852712},
ISSN={2169-3536},
month={},}
@ARTICLE{6112742,
author={M. Shahbaz and K. C. Shashidhar and R. Eschbach},
journal={IEEE Software},
title={Specification Inference Using Systematic Reverse-Engineering Methodologies: An Automotive Industry Application},
year={2012},
volume={29},
number={6},
pages={62-69},
abstract={Lack of precise specification is a well-known problem in the software industry. This article covers some peculiar aspects of the problem and its causes in the automotive software industry. The authors describe how the situation motivates engineers to grasp reverse-engineering methodologies to comprehend third-party components. They developed a novel approach for reverse-engineering components, which they applied to a recent project on testing embedded systems of a modern vehicle.},
keywords={automobile industry;DP industry;formal specification;inference mechanisms;program testing;reverse engineering;road vehicles;specification inference;systematic reverse-engineering methodology;automotive software industry application;third-party components;modern vehicle embedded system testing;Software engineering;Automotive engineering;Embedded systems;Reverse engineering;System analysis and design;Modeling;software reverse engineering;system analysis;model inference;system integration;system specification},
doi={10.1109/MS.2011.159},
ISSN={0740-7459},
month={Nov},}
@ARTICLE{8329954,
author={S. Chen and B. Song and J. Guo},
journal={IEEE Access},
title={Attention Alignment Multimodal LSTM for Fine-Gained Common Space Learning},
year={2018},
volume={6},
number={},
pages={20195-20208},
abstract={We address the problem common space learning approach that maps all related multimodal information into a common space for multimodal data. To establish a fine-grained common space, the aligned relevant local information of different modalities is used to learn a common subspace where the projected fragmented information is further integrated according to intra-modal semantic relationships. Specifically, we propose a novel multimodal LSTM with an attention alignment mechanism, namely attention alignment multimodal LSTM (AAM-LSTM), which mainly includes attentional alignment recurrent network (AA-R) and hierarchical multimodal LSTM (HM-LSTM). Different from the traditional methods which operate on the full modal data directly, the proposed model exploits the inter-modal and intra-modal semantic relationships of local information, to jointly establish a uniform representation of multi-modal data. Specifically, AA-R automatically captures semantic-aligned local information to learn common subspace without the need of supervised labels, then HM-LSTM leverages the potential relationships of these local information to learn a fine-grained common space. The experimental results on Filker30K, Filker8K, and Filker30K entities verify the performance and effectiveness of our model, which compares favorably with the state-of-the-art methods. In particular, the experiment of phrase localization on AA-R with Filker30K entities shows the expected accurate attention alignment. Moreover, from the experiment results of image-sentence retrieval tasks, it can be concluded that the proposed AAM-LSTM outperforms benchmark algorithms by a large margin.},
keywords={learning (artificial intelligence);recurrent neural nets;sensor fusion;AA-R;AAM-LSTM;fine-gained common space learning;multimodal data;aligned relevant local information;projected fragmented information;attention alignment mechanism;attention alignment multimodal LSTM;attentional alignment recurrent network;hierarchical multimodal LSTM;multimodal information;HM-LSTM;intramodal semantic relationships;multimodal data fusion;Semantics;Task analysis;Data models;Data integration;Media;Data mining;Computational modeling;Multimodal data fusion;phrase localization;fine-grained common space;attention alignment;hierarchical multimodal LSTM},
doi={10.1109/ACCESS.2018.2822663},
ISSN={2169-3536},
month={},}

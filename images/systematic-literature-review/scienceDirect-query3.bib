@article{DESMEDT201840,
title = "Discovering hidden dependencies in constraint-based declarative process models for improving understandability",
journal = "Information Systems",
volume = "74",
pages = "40 - 52",
year = "2018",
note = "Information Systems Engineering: selected papers from CAiSE 2016",
issn = "0306-4379",
doi = "https://doi.org/10.1016/j.is.2018.01.001",
url = "http://www.sciencedirect.com/science/article/pii/S0306437916304732",
author = "Johannes De Smedt and Jochen De Weerdt and Estefanía Serral and Jan Vanthienen",
keywords = "Declarative process modeling, Declare, Hidden dependencies, Constraint-based process models, Model comprehension, Empirical research",
abstract = "Flexible systems and services require a solid approach for modeling and enacting dynamic behavior. Declarative process models gained plenty of traction lately as they have proven to provide a good fit for the problem at hand, i.e. visualizing and executing flexible business processes. These models are based on constraints that impose behavioral restrictions on process behavior. Essentially, a declarative model is a set of constraints defined over the set of activities in a process. While allowing for very flexible process specifications, a major downside is that the combination of constraints can lead to behavioral restrictions not explicitly visible when reading a model. These restrictions, so-called hidden dependencies, make the models much more difficult to understand. This paper presents a technique for discovering hidden dependencies and making them explicit by means of dependency structures. Experiments with novice process modelers demonstrate that the proposed technique lowers the cognitive effort necessary to comprehend a constraint-based process model."
}
@article{HARMAN2016242,
title = "Augmenting process elicitation with visual priming: An empirical exploration of user behaviour and modelling outcomes",
journal = "Information Systems",
volume = "62",
pages = "242 - 255",
year = "2016",
issn = "0306-4379",
doi = "https://doi.org/10.1016/j.is.2016.01.005",
url = "http://www.sciencedirect.com/science/article/pii/S030643791530106X",
author = "Joel Harman and Ross Brown and Daniel Johnson and Stefanie Rinderle-Ma and Udo Kannengiesser",
keywords = "Business process management, Process elicitation, Subject-oriented business process management, 3D virtual worlds, Human–computer interaction",
abstract = "Business process models have become an effective way of examining business practices to identify areas for improvement. While common information gathering approaches are generally efficacious, they can be quite time consuming and have the risk of developing inaccuracies when information is forgotten or incorrectly interpreted by analysts. In this study, the potential of a role-playing approach to process elicitation and specification has been examined. This method allows stakeholders to enter a virtual world and role-play actions similarly to how they would in reality. As actions are completed, a model is automatically developed, removing the need for stakeholders to learn and understand a modelling grammar. An empirical investigation comparing both the modelling outputs and participant behaviour of this virtual world role-play elicitor with an S-BPM process modelling tool found that while the modelling approaches of the two groups varied greatly, the virtual world elicitor may not only improve both the number of individual process task steps remembered and the correctness of task ordering, but also provide a reduction in the time required for stakeholders to model a process view."
}
@article{TEKLI2018133,
title = "Full-fledged semantic indexing and querying model designed for seamless integration in legacy RDBMS",
journal = "Data & Knowledge Engineering",
volume = "117",
pages = "133 - 173",
year = "2018",
issn = "0169-023X",
doi = "https://doi.org/10.1016/j.datak.2018.07.007",
url = "http://www.sciencedirect.com/science/article/pii/S0169023X16301835",
author = "Joe Tekli and Richard Chbeir and Agma J.M. Traina and Caetano Traina and Kokou Yetongnon and Carlos Raymundo Ibanez and Marc Al Assad and Christian Kallas",
keywords = "Semantic queries, Inverted index, NoSQL indexing, Semantic network, Semantic-aware data processing, Textual databases",
abstract = "In the past decade, there has been an increasing need for semantic-aware data search and indexing in textual (structured and NoSQL) databases, as full-text search systems became available to non-experts where users have no knowledge about the data being searched and often formulate query keywords which are different from those used by the authors in indexing relevant documents, thus producing noisy and sometimes irrelevant results. In this paper, we address the problem of semantic-aware querying and provide a general framework for modeling and processing semantic-based keyword queries in textual databases, i.e., considering the lexical and semantic similarities/disparities when matching user query and data index terms. To do so, we design and construct a semantic-aware inverted index structure called SemIndex, extending the standard inverted index by constructing a tightly coupled inverted index graph that combines two main resources: a semantic network and a standard inverted index on a collection of textual data. We then provide a general keyword query model with specially tailored query processing algorithms built on top of SemIndex, in order to produce semantic-aware results, allowing the user to choose the results' semantic coverage and expressiveness based on her needs. To investigate the practicality and effectiveness of SemIndex, we discuss its physical design within a standard commercial RDBMS allowing to create, store, and query its graph structure, thus enabling the system to easily scale up and handle large volumes of data. We have conducted a battery of experiments to test the performance of SemIndex, evaluating its construction time, storage size, query processing time, and result quality, in comparison with legacy inverted index. Results highlight both the effectiveness and scalability of our approach."
}
@article{LEITER201891,
title = "Accelerated scale-bridging through adaptive surrogate model evaluation",
journal = "Journal of Computational Science",
volume = "27",
pages = "91 - 106",
year = "2018",
issn = "1877-7503",
doi = "https://doi.org/10.1016/j.jocs.2018.04.010",
url = "http://www.sciencedirect.com/science/article/pii/S1877750317313807",
author = "Kenneth W. Leiter and Brian C. Barnes and Richard Becker and Jaroslaw Knap",
keywords = "Multiscale modeling, Surrogate modeling, Gaussian process regression, Energetic materials",
abstract = "Multiscale modeling is a systematic approach for the development of high-fidelity models of complex systems. However, multiscale models are often extremely computationally demanding, which precludes their use for practical applications. In this article, we introduce a computational framework for scale-bridging combined with an algorithm to automatically and adaptively replace at-scale models within a multiscale model hierarchy with surrogate models in order to reduce the computational cost of multiscale simulations. A standalone module is introduced and it is responsible for the on-the-fly construction and evaluation of surrogate models within the framework. Such an approach allows multiscale models to easily incorporate surrogate models with minimal code modifications. We employ the framework to construct a multiscale model of 1,3,5-trinitrohexahydro-s-triazine, in which a continuum finite element macroscale solver acquires equation of state through evaluation of a microscale dissipative particle dynamics model. We utilize the model for the simulation of a Taylor impact experiment and demonstrate that the error in the solution incurred by the dynamic use of surrogate models is controllable. Furthermore, we show that the use of surrogate models leads to a reduction in computational cost of between 1/20 and 1/5000 compared to a simulation evaluated without the surrogate modeling approach. In addition, we present a high-resolution simulation of a Taylor impact experiment, which is intractable without surrogate models. We illustrate how the dynamic nature of surrogate model evaluation in these simulations, while reducing computational cost, also increases load imbalance. Finally, we end with a discussion on how the inherent variability in these simulations may constitute a challenge for the current high performance computer systems given their static nature."
}
@article{SENG2009529,
title = "A generic construct based workload model for web search",
journal = "Information Processing & Management",
volume = "45",
number = "5",
pages = "529 - 554",
year = "2009",
issn = "0306-4573",
doi = "https://doi.org/10.1016/j.ipm.2009.04.004",
url = "http://www.sciencedirect.com/science/article/pii/S0306457309000429",
author = "Jia-Lang Seng and I-Feng Ko and Binshan Lin",
keywords = "Web search, Information retrieval, Generic construct, Benchmark method, Workload model, Performance measurement, Evaluation, Comparison",
abstract = "Benchmarks are vital tools in the performance measurement, evaluation, and comparison of computer hardware and software systems. Standard benchmarks such as the TREC, TPC, SPEC, SAP, Oracle, Microsoft, IBM, Wisconsin, AS3AP, OO1, OO7, XOO7 benchmarks have been used to assess the system performance. These benchmarks are domain-specific and domain-dependent in that they model typical applications and tie to a problem domain. Test results from these benchmarks are estimates of possible system performance for certain pre-determined problem types. When the user domain differs from the standard problem domain or when the application workload is divergent from the standard workload, they do not provide an accurate way to measure the system performance of the user problem domain. System performance of the actual problem domain in terms of data and transactions may vary significantly from the standard benchmarks. In this research, we address the issue of generalization and precision of benchmark workload model for web search technology. The current performance measurement and evaluation method suffers from the rough estimate of system performance which varies widely when the problem domain changes. The performance results provided by the vendors cannot be reproduced nor reused in the real users’ environment. Hence, in this research, we tackle the issue of domain boundness and workload boundness which represents the root of the problem of imprecise, ir-representative, and ir-reproducible performance results. We address the issue by presenting a domain-independent and workload-independent workload model benchmark method which is developed from the perspective of the user requirements and generic constructs. We present a user-driven workload model to develop a benchmark in a process of workload requirements representation, transformation, and generation via the common carrier of generic constructs. We aim to create a more generalized and precise evaluation method which derives test suites from the actual user domain and application setting. The workload model benchmark method comprises three main components. They are a high-level workload specification scheme, a translator of the scheme, and a set of generators to generate the test database and the test suite. They are based on the generic constructs. The specification scheme is used to formalize the workload requirements. The translator is used to transform the specification. The generator is used to produce the test database and the test workload. We determine the generic constructs via the analysis of search methods. The generic constructs form a page model, a query model, and a control model in the workload model development. The page model describes the web page structure. The query model defines the logics to query the web. The control model defines the control variables to set up the experiments. In this study, we have conducted ten baseline research experiments to validate the feasibility and validity of the benchmark method. An experimental prototype is built to execute these experiments. Experimental results demonstrate that the method based on generic constructs and driven by the perspective of user requirements is capable of modeling the standard benchmarks as well as more general benchmark requirements."
}
@article{LI2017,
title = "Social emotion classification based on noise-aware training",
journal = "Data & Knowledge Engineering",
year = "2017",
issn = "0169-023X",
doi = "https://doi.org/10.1016/j.datak.2017.07.008",
url = "http://www.sciencedirect.com/science/article/pii/S0169023X17303506",
author = "Xin Li and Yanghui Rao and Haoran Xie and Xuebo Liu and Tak-Lam Wong and Fu Lee Wang",
keywords = "Social emotion classification, Emotional concentration, Convolutional neural network, Topic modeling",
abstract = "Social emotion classification draws many natural language processing researchers’ attention in recent years, since analyzing user-generated emotional documents on the Web is quite useful in recommending products, gathering public opinions, and predicting election results. However, the documents that evoke prominent social emotions are usually mixed with noisy instances, and it is also challenging to capture the textual meaning of short messages. In this work, we focus on reducing the impact of noisy instances and learning a better representation of sentences. For the former, we introduce an “emotional concentration” indicator, which is derived from emotional ratings to weight documents. For the latter, we propose a new architecture named PCNN, which utilizes two cascading convolutional layers to model the word-phrase relation and the phrase-sentence relation. This model regards continuous tokens as phrases based on an assumption that neighboring words are very likely to have internal relations, and semantic feature vectors are generated based on the phrase representation. We also present a Bayesian-based model named WMCM to learn document-level semantic features. Both PCNN and WMCM classify social emotions by capturing semantic regularities in language. Experiments on two real-world datasets indicate that the quality of learned semantic vectors and the performance of social emotion classification can be improved by our models."
}

@article{BENSASSI201727,
title = "Context-aware recommender systems in mobile environment: On the road of future research",
journal = "Information Systems",
volume = "72",
pages = "27 - 61",
year = "2017",
issn = "0306-4379",
doi = "https://doi.org/10.1016/j.is.2017.09.001",
url = "http://www.sciencedirect.com/science/article/pii/S0306437916303106",
author = "Imen Ben Sassi and Sehl Mellouli and Sadok Ben Yahia",
keywords = "Recommender systems, Mobile environment, Contextual factors, Recommendation process, Systematic review, Mobile context-aware recommender",
abstract = "Recommender systems have recently been singled out as a fascinating area of research, owing to the technological progress in mobile devices, such as smartphones and tablets, as well as to the rapid growth of social networking. In this respect, the main purpose of recommender systems is to suggest items that help users to make decisions from a large number of possible actions such as what place to visit, what movie to watch, or which friend to add to a social network system. In mobile environment, many personal, social and environmental contextual factors can be integrated into the recommendation process in order to provide the correct recommendation to a special user, at the perfect moment, in the appropriate location based on his/her emotional state, his/her current activity and past behavior. This paper provides an overview of context-aware recommender systems in mobile environment. The objective of this systematic review is to investigate the current state of the art in context-aware recommender systems and classify the reviewed research papers. This study aims equally to identify the possible future directions in this research area."
}
@article{WANG2018198,
title = "Hyperparameter selection of one-class support vector machine by self-adaptive data shifting",
journal = "Pattern Recognition",
volume = "74",
pages = "198 - 211",
year = "2018",
issn = "0031-3203",
doi = "https://doi.org/10.1016/j.patcog.2017.09.012",
url = "http://www.sciencedirect.com/science/article/pii/S0031320317303564",
author = "Siqi Wang and Qiang Liu and En Zhu and Fatih Porikli and Jianping Yin",
keywords = "One-class SVM, Hyperparameter selection, Data shifting",
abstract = "With flexible data description ability, one-class Support Vector Machine (OCSVM) is one of the most popular and widely-used methods for one-class classification (OCC). Nevertheless, the performance of OCSVM strongly relies on its hyperparameter selection, which is still a challenging open problem due to the absence of outlier data. This paper proposes a fully automatic OCSVM hyperparameter selection method, which requires no tuning of additional hyperparameter, based on a novel self-adaptive “data shifting” mechanism: Firstly, by efficient edge pattern detection (EPD) and “negatively” shifting edge patterns along the negative direction of estimated data density gradient, a constrained number of high-quality pseudo outliers are self-adaptively generated at more desirable locations, which readily avoids two major difficulties in previous outlier generation methods. Secondly, to avoid time-consuming cross-validation and enhance robustness to noise in the given training data, a pseudo target set is generated for model validation by “positively” shifting each given target datum along the positive direction of data density gradient. Experiments on synthetic and benchmark datasets demonstrate the effectiveness of the proposed method."
}
@article{NGUYEN2018129,
title = "A heuristics approach to mine behavioural data logs in mobile malware detection system",
journal = "Data & Knowledge Engineering",
volume = "115",
pages = "129 - 151",
year = "2018",
issn = "0169-023X",
doi = "https://doi.org/10.1016/j.datak.2018.03.002",
url = "http://www.sciencedirect.com/science/article/pii/S0169023X17303063",
author = "Giang Nguyen and Binh Minh Nguyen and Dang Tran and Ladislav Hluchy",
keywords = "Mobile security, Situational awareness, Anomaly detection, Incremental machine learning, Natural language processing, Scalable solution design",
abstract = "Nowadays, in the era of Internet of Things when everything is connected via the Internet, the number of mobile devices has risen exponentially up to billions around the world. In line with this increase, the volume of data generated is enormous and has attracted malefactors who do ill deeds to others. For hackers, one of the popular threads to mobile devices is to spread malware. These actions are very difficult to prevent because the application installation and configuration rights are set by owners, who usually have very low knowledge or do not care about the security. In this study, our aim is to improve security in the environment of mobile devices by proposing a novel system to detect malware intrusions automatically. Our solution is based on modelling user behaviours and applying the heuristic analysis approach to mobile logs generated during the device operation process. Although behaviours of individual users have a significant impact on the social cyber-security, to achieve the user awareness has still remained one of the major challenges today. For this task, there is proposed a light-weight semantic formalization in the form of physical and logical taxonomy for classifying the collected raw log data. Then a set of techniques is used, like sliding windows, lemmatization, feature selection, term weighting, and so on, to process data. Meanwhile, malware detection tasks are performed based on incremental machine learning mechanisms, because of the potential complexity of this tasks. The solution is developed in the manner to allow the scalability with several blocks that cover pre-processing raw collected logs from mobile devices, automatically creating datasets for machine learning methods, using the best selected model for detecting suspicious activity surrounding malware intrusions, and supporting decision making using a predictive risk factor. We experimented cautiously with the proposal and achieved test results confirm the effectiveness and feasibility of the proposed system in applying to the large-scale mobile environment."
}
@article{DAS20123373,
title = "A robust alignment-free fingerprint hashing algorithm based on minimum distance graphs",
journal = "Pattern Recognition",
volume = "45",
number = "9",
pages = "3373 - 3388",
year = "2012",
note = "Best Papers of Iberian Conference on Pattern Recognition and Image Analysis (IbPRIA'2011)",
issn = "0031-3203",
doi = "https://doi.org/10.1016/j.patcog.2012.02.022",
url = "http://www.sciencedirect.com/science/article/pii/S0031320312001008",
author = "Priyanka Das and Kannan Karthik and Boul Chandra Garai",
keywords = "Fingerprint, Hash, Minimum distance graph, Corresponding search algorithm, Security",
abstract = "Abstraction of a fingerprint in the form of a hash can be used for secure authentication. The main challenge is in finding the right choice of features which remain relatively invariant to distortions such as rotation, translation and minutiae insertions and deletions, while at the same time capturing the diversity across users. In this paper, an alignment-free novel fingerprint hashing algorithm is proposed which uses a graph comprising of the inter-minutia minimum distance vectors originating from the core point as a feature set called the minimum distance graph. Matching of hashes has been implemented using a corresponding search algorithm. Based on the experiments conducted on the FVC2002-DB1a and FVC2002-DB2a databases, we obtained an equal error rate of 2.27%. The computational cost associated with our fingerprint hash generation and matching processes is relatively low, despite its success in capturing the minutia positional variations across users."
}
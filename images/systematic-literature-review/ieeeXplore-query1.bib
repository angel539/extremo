@ARTICLE{6547630,
author={X. Wu and X. Zhu and G. Wu and W. Ding},
journal={IEEE Transactions on Knowledge and Data Engineering},
title={Data mining with big data},
year={2014},
volume={26},
number={1},
pages={97-107},
abstract={Big Data concern large-volume, complex, growing data sets with multiple, autonomous sources. With the fast development of networking, data storage, and the data collection capacity, Big Data are now rapidly expanding in all science and engineering domains, including physical, biological and biomedical sciences. This paper presents a HACE theorem that characterizes the features of the Big Data revolution, and proposes a Big Data processing model, from the data mining perspective. This data-driven model involves demand-driven aggregation of information sources, mining and analysis, user interest modeling, and security and privacy considerations. We analyze the challenging issues in the data-driven model and also in the Big Data revolution.},
keywords={data mining;user modelling;data mining;growing data sets;networking;data storage;data collection capacity;HACE theorem;Big Data revolution;Big Data processing model;data driven model;demand driven aggregation;information sources;user interest modeling;Information management;Data handling;Data storage systems;Data privacy;Data models;Distributed databases;Big Data;data mining;heterogeneity;autonomous sources;complex and evolving associations},
doi={10.1109/TKDE.2013.109},
ISSN={1041-4347},
month={Jan},}
@ARTICLE{8374410,
author={B. Jang and S. Park and J. Lee and S. Hahn},
journal={IEEE Access},
title={Three Hierarchical Levels of Big-Data Market Model Over Multiple Data Sources for Internet of Things},
year={2018},
volume={6},
number={},
pages={31269-31280},
abstract={This paper proposes three hierarchical levels of a competitive big-data market model. We consider that a service provider gathers data from multiple data sources and provides valuable information from refined data as a service to its customers. Under our approach, a service provider determines optimal data procurement from multiple data sources within its budget constraint. The multiple data sources follow the service provider's action by independently submitting bidding prices to the service provider. Further, customers decide whether to subscribe or not based on the subscription fee, their willingness-to-pay, and the quality of the refined data. We study the economic benefits of such a market model by analyzing the hierarchical decision making procedures as a Stackelberg game. We show the existence and the uniqueness of the Nash equilibrium (NE), and the NE solution is given as a closed form. Finally, we reveal that the obtained unique equilibrium solution maximizes the payoff of all market participants.},
keywords={Big Data;decision making;game theory;Internet;Internet of Things;pricing;tendering;multiple data sources;optimal data procurement;big-data market model;service provider;bidding prices;willingness-to-pay;hierarchical decision making procedures;Stackelberg game;Nash equilibrium;NE;Data models;Games;Analytical models;Internet of Things;Data mining;Nash equilibrium;Numerical models;Data market;Internet of Things;stackelberg game;industrial informatics},
doi={10.1109/ACCESS.2018.2845105},
ISSN={2169-3536},
month={},}
@ARTICLE{8047940,
author={W. Lee and J. Huang and H. Chang and K. Lee and C. Lai},
journal={IEEE Access},
title={Predicting Drug Side Effects Using Data Analytics and the Integration of Multiple Data Sources},
year={2017},
volume={5},
number={},
pages={20449-20462},
abstract={The development of automated approaches employing computational methods using data from publicly available drugs datasets for the prediction of drug side effects has been proposed. This paper presents the use of a hybrid machine learning approach to construct side effect classifiers using an appropriate set of data features. The presented approach utilizes the perspective of data analytics to investigate the effect of drug distribution in the feature space, categorize side effects into several intervals, adopt suitable strategies for each interval, and construct data models accordingly. To verify the applicability of the presented method in side effect prediction, a series of experiments were conducted. The results showed that this approach was able to take into account the characteristics of different types of side effects, thereby achieve better predictive performance. Moreover, different feature selection schemes were coupled with the modeling methods to examine the corresponding effects. In addition, analyses were performed to investigate the task difficulty in terms of data distance and similarity. Examples of visualized networks of associations between drugs and side effects are also discussed to further evaluate the results.},
keywords={data analysis;data models;data visualisation;drugs;learning (artificial intelligence);medical computing;pattern classification;drug distribution;data models;data distance;data analytics;publicly available drugs datasets;side effect classifiers;drug side effect prediction;multiple data sources integration;hybrid machine learning;data similarity;feature selection schemes;Drugs;Chemicals;Proteins;Data analysis;Compounds;Correlation;Drug side effect;data analytics;machine learning;predictive modeling;feature selection},
doi={10.1109/ACCESS.2017.2755045},
ISSN={2169-3536},
month={},}
@ARTICLE{6095552,
author={P. Buche and J. Dibie-Barthelemy and L. Ibanescu and L. Soler},
journal={IEEE Transactions on Knowledge and Data Engineering},
title={Fuzzy Web Data Tables Integration Guided by an Ontological and Terminological Resource},
year={2013},
volume={25},
number={4},
pages={805-819},
abstract={In this paper, we present the design of ONDINE system which allows the loading and the querying of a data warehouse opened on the Web, guided by an Ontological and Terminological Resource (OTR). The data warehouse, composed of data tables extracted from Web documents, has been built to supplement existing local data sources. First, we present the main steps of our semiautomatic method to annotate data tables driven by an OTR. The output of this method is an XML/RDF data warehouse composed of XML documents representing data tables with their fuzzy RDF annotations. We then present our flexible querying system which allows the local data sources and the data warehouse to be simultaneously and uniformly queried, using the OTR. This system relies on SPARQL and allows approximate answers to be retrieved by comparing preferences expressed as fuzzy sets with fuzzy RDF annotations.},
keywords={collections of physical data;data integration;data warehouses;fuzzy set theory;Internet;ontologies (artificial intelligence);query processing;SQL;XML;fuzzy Web data tables integration;ONDINE system;ontological and terminological resource;OTR;Web documents;local data sources;semiautomatic method;XML data warehouse;RDF data warehouse;XML documents;fuzzy RDF annotations;flexible querying system;SPARQL;approximate answers;fuzzy sets;query processing;Ontologies;Resource description framework;Semantics;Data mining;OWL;XML;Data warehouses;Knowledge and data engineering tools and techniques;XML/XSL/RDF;uncertainty;"fuzzy";and probabilistic reasoning;representations;data structures;and transforms;knowledge modeling},
doi={10.1109/TKDE.2011.245},
ISSN={1041-4347},
month={April},}
@ARTICLE{8314125,
author={J. Wang and C. Deng and X. Li},
journal={IEEE Access},
title={Two Privacy-Preserving Approaches for Publishing Transactional Data Streams},
year={2018},
volume={6},
number={},
pages={23648-23658},
abstract={Recently, data mining over transactional data streams has become an attractive research area. However, releasing raw transactional data streams, in which only explicit identifying information must be removed, may compromise individual privacy. Many privacy-preserving approaches have been proposed for publishing static transactional data. Due to the characteristics of data streams, which must be processed quickly, static data anonymization methods cannot be directly applied to data streams. In this paper, we first analyze the privacy problem in publishing transactional data streams based on a sliding window. Then, we present two dynamic algorithms with generalization and suppression to anonymize continuously a sliding window to make it satisfy ρ-uncertainty by structuring an affected sensitive rules trie, because the removal and addition of transactions may make the current sliding window fail to satisfy ρ-uncertainty. Experimental results show that our methods are more efficient than sliding window anonymization with batch processing by using existing static anonymization methods.},
keywords={data mining;data privacy;transaction processing;privacy-preserving approaches;publishing transactional data streams;data mining;raw transactional data streams;individual privacy;publishing static transactional data;privacy problem;sliding window;static data anonymization methods;dynamic algorithms;Publishing;Data privacy;Heuristic algorithms;Data models;Privacy;Microsoft Windows;Data publishing;privacy preservation;sliding window;transactional data stream},
doi={10.1109/ACCESS.2018.2814622},
ISSN={2169-3536},
month={},}
@ARTICLE{8412190,
author={H. T. El Kassabi and M. A. Serhani and R. Dssouli and B. Benatallah},
journal={IEEE Access},
title={A Multi-Dimensional Trust Model for Processing Big Data Over Competing Clouds},
year={2018},
volume={6},
number={},
pages={39989-40007},
abstract={Cloud computing has emerged as a powerful paradigm for delivering data-intensive services over the Internet. Cloud computing has enabled the implementation and success of big data, a recent phenomenon handling huge data being generated from different sources. Competing clouds have made it challenging to select a cloud provider that guarantees quality of cloud service (QoCS). Also, cloud providers' claims of guaranteeing QoCS are exaggerated for marketing purposes; hence, they cannot often be trusted. Therefore, a comprehensive trust model is necessary to evaluate the QoCS prior to making any selection decision. In this paper, we propose a multi-dimensional trust model for big data workflow processing over different clouds. It evaluates the trustworthiness of cloud providers based on: the most up-to-date cloud resource capabilities, the reputation evidence measured by neighboring users, and a recorded personal history of experiences with the cloud provider. The ultimate goal is to ensure an efficient selection of trustworthiness cloud provider who eventually will guarantee high QoCS and fulfills key big data workflow requirements. Various experiments were conducted to validate our proposed model. The results show that our model captures the different components of trust, ensures high QoCS, and effectively adapts to the dynamic nature of the cloud.},
keywords={Big Data;cloud computing;Internet;quality of service;trusted computing;multidimensional trust model;competing clouds;cloud computing;data-intensive services;cloud service;cloud providers;comprehensive trust model;up-to-date cloud resource capabilities;trustworthiness cloud provider;QoCS;Big Data workflow processing;Cloud computing;Big Data;Computational modeling;Data models;Adaptation models;Task analysis;Big data;big data processing;cloud computing;cloud selection;trust model;quality of cloud services;service evaluation;community},
doi={10.1109/ACCESS.2018.2856623},
ISSN={2169-3536},
month={},}
@ARTICLE{8260919,
author={C. Li and P. Zhou and L. Xiong and Q. Wang and T. Wang},
journal={IEEE Transactions on Knowledge and Data Engineering},
title={Differentially Private Distributed Online Learning},
year={2018},
volume={30},
number={8},
pages={1440-1453},
abstract={In the big data era, the generation of data presents some new characteristics, including wide distribution, high velocity, high dimensionality, and privacy concern. To address these challenges for big data analytics, we develop a privacy-preserving distributed online learning framework on the data collected from distributed data sources. Specifically, each node (i.e., data source) has the capacity of learning a model from its local dataset, and exchanges intermediate parameters with a random part of their own neighboring (logically connected) nodes. Hence, the topology of the communications in our distributed computing framework is unfixed in practice. As online learning always performs on the sensitive data, we introduce the notion of differential privacy (DP) into our distributed online learning algorithm (DOLA) to protect the data privacy during the learning, which prevents an adversary from inferring any significant sensitive information. Our model is of general value for big data analytics in the distributed setting, because it can provide rigorous and scalable privacy proof and have much less computational complexity when compared to classic schemes, e.g., secure multiparty computation (SMC). To tackle high-dimensional incoming data entries, we study a sparse version of the DOLA with novel DP techniques to save the computing resources and improve the utility. Furthermore, we present two modified private DOLAs to meet the need of practical applications. One is to convert the DOLA to distributed stochastic optimization in an offline setting, the other is to use the mini-batches approach to reduce the amount of the perturbation noise and improve the utility. We conduct experiments on real datasets in a configured distributed platform. Numerical experiment results validate the feasibility of our private DOLAs.},
keywords={Big Data;computational complexity;data analysis;data privacy;learning (artificial intelligence);big data era;high velocity;high dimensionality;privacy concern;big data analytics;privacy-preserving;online learning framework;distributed data sources;data source;exchanges intermediate parameters;distributed computing framework;sensitive data;differential privacy;distributed online learning algorithm;data privacy;distributed setting;rigorous privacy proof;scalable privacy proof;secure multiparty computation;high-dimensional incoming data entries;modified private DOLAs;configured distributed platform;differentially private distributed online learning;SMC;Data privacy;Distributed databases;Privacy;Optimization;Big Data;Computational modeling;Perturbation methods;Differential privacy;distributed optimization;online learning;sparse;mini-batch;big data},
doi={10.1109/TKDE.2018.2794384},
ISSN={1041-4347},
month={Aug},}
@ARTICLE{8478144,
author={H. Harb and A. Makhoul and C. Abou Jaoude},
journal={IEEE Access},
title={A Real-Time Massive Data Processing Technique for Densely Distributed Sensor Networks},
year={2018},
volume={6},
number={},
pages={56551-56561},
abstract={Today, we are awash in a flood of data coming from different data generating sources. Wireless sensor networks (WSNs) are one of the big data contributors, where data are being collected at unprecedented scale. Unfortunately, much of these data are of no interest, meaningless, and redundant. Hence, data reduction is becoming fundamental operation in order to decrease the communication costs and enhance data mining in WSNs. In this paper, we propose a two-level data reduction approach for sensor networks. The first level operated by the sensor nodes consists of compressing collected data while using the Pearson coefficient. The second level is executed at intermediate nodes (e.g., aggregators, cluster heads, and so on). The objective of the second level is to eliminate redundant data generated by neighboring nodes using two adapted clustering methods: EKmeans and TopK. Through both simulations and real experiments on real telosB sensors, we show the relevance of our approach in terms of minimizing the big data collected in WSNs and enhancing network lifetime, compared to other existing techniques.},
keywords={Big Data;data mining;data reduction;distributed sensors;sensor placement;telecommunication power management;wireless sensor networks;wireless sensor networks;big data contributors;communication costs;data mining;two-level data reduction approach;sensor nodes;intermediate nodes;neighboring nodes;telosB sensors;distributed sensor networks;WSN;network lifetime;EKmeans clustering method;TopK clustering method;Pearson coefficient;real-time massive data processing technique;Wireless sensor networks;Clustering algorithms;Data compression;Correlation;Data models;Big Data;Data mining;Wireless sensor network (WSN);sensory data processing;clustering techniques;big-data sensing;data compression},
doi={10.1109/ACCESS.2018.2872687},
ISSN={2169-3536},
month={},}
@ARTICLE{8344505,
author={G. Upadhyaya and H. Rajan},
journal={IEEE Transactions on Software Engineering},
title={On Accelerating Source Code Analysis at Massive Scale},
year={2018},
volume={44},
number={7},
pages={669-688},
abstract={Encouraged by the success of data-driven software engineering (SE) techniques that have found numerous applications e.g., in defect prediction, specification inference, the demand for mining and analyzing source code repositories at scale has significantly increased. However, analyzing source code at scale remains expensive to the extent that data-driven solutions to certain SE problems are beyond our reach today. Extant techniques have focused on leveraging distributed computing to solve this problem, but with a concomitant increase in computational resource needs. This work proposes a technique that reduces the amount of computation performed by the ultra-large-scale source code mining task, especially those that make use of control and data flow analyses. Our key idea is to analyze the mining task to identify and remove the irrelevant portions of the source code, prior to running the mining task. We show a realization of our insight for mining and analyzing massive collections of control flow graphs of source codes. Our evaluation using 16 classical control-/data-flow analyses that are typical components of mining tasks and 7 Million CFGs shows that our technique can achieve on average a 40 percent reduction in the task computation time. Our case studies demonstrates the applicability of our technique to massive scale source code mining tasks.},
keywords={data flow analysis;data mining;software engineering;source code (software);control-flow analyses;ultralarge-scale source code mining task;source code repository analysis;massive scale source code mining tasks;task computation time;control flow graphs;data flow analyses;specification inference;defect prediction;data-driven software engineering techniques;Task analysis;Data mining;Acceleration;Static analysis;Software engineering;Distributed computing;Software;Source code analysis;mining software repositories;ultra-large-scale mining;data-driven software engineering},
doi={10.1109/TSE.2018.2828848},
ISSN={0098-5589},
month={July},}
@ARTICLE{6782396,
author={J. G. Wolff},
journal={IEEE Access},
title={Big Data and the SP Theory of Intelligence},
year={2014},
volume={2},
number={},
pages={301-315},
abstract={This paper is about how the SP theory of intelligence and its realization in the SP machine may, with advantage, be applied to the management and analysis of big data. The SP system-introduced in this paper and fully described elsewhere-may help to overcome the problem of variety in big data; it has potential as a universal framework for the representation and processing of diverse kinds of knowledge, helping to reduce the diversity of formalisms and formats for knowledge, and the different ways in which they are processed. It has strengths in the unsupervised learning or discovery of structure in data, in pattern recognition, in the parsing and production of natural language, in several kinds of reasoning, and more. It lends itself to the analysis of streaming data, helping to overcome the problem of velocity in big data. Central in the workings of the system is lossless compression of information: making big data smaller and reducing problems of storage and management. There is potential for substantial economies in the transmission of data, for big cuts in the use of energy in computing, for faster processing, and for smaller and lighter computers. The system provides a handle on the problem of veracity in big data, with potential to assist in the management of errors and uncertainties in data. It lends itself to the visualization of knowledge structures and inferential processes. A high-parallel, open-source version of the SP machine would provide a means for researchers everywhere to explore what can be done with the system and to create new versions of it.},
keywords={Big Data;data analysis;data compression;data mining;data structures;natural language processing;unsupervised learning;Big Data management;SP theory of intelligence;SP machine;Bid Data analysis;unsupervised learning;data structure discovery;natural language production;pattern recognition;streaming data analysis;lossless compression;knowledge structure visualization;inferential processes;high-parallel open-source version;error management;Unsupervised learning;Pattern recognition;Data storage systems;Data compression;Computational efficiency;Cognitive science;Artificial intelligence;Artificial intelligence;big data;cognitive science;computational efficiency;data compression;data-centric computing;energy efficiency;pattern recognition;uncertainty;unsupervised learning},
doi={10.1109/ACCESS.2014.2315297},
ISSN={2169-3536},
month={},}
@ARTICLE{8384233,
author={L. Xiang and G. Zhao and Q. Li and W. Hao and F. Li},
journal={IEEE Access},
title={TUMK-ELM: A Fast Unsupervised Heterogeneous Data Learning Approach},
year={2018},
volume={6},
number={},
pages={35305-35315},
abstract={Advanced unsupervised learning techniques are an emerging challenge in the big data era due to the increasing requirements of extracting knowledge from a large amount of unlabeled heterogeneous data. Recently, many efforts of unsupervised learning have been done to effectively capture information from heterogeneous data. However, most of them are with huge time consumption, which obstructs their further application in the big data analytics scenarios, where an enormous amount of heterogeneous data are provided but real-time learning are strongly demanded. In this paper, we address this problem by proposing a fast unsupervised heterogeneous data learning algorithm, namely two-stage unsupervised multiple kernel extreme learning machine (TUMK-ELM). TUMK-ELM alternatively extracts information from multiple sources and learns the heterogeneous data representation with closed-form solutions, which enables its extremely fast speed. As justified by theoretical evidence, TUMK-ELM has low computational complexity at each stage, and the iteration of its two stages can be converged within finite steps. As experimentally demonstrated on 13 real-life data sets, TUMK-ELM gains a large efficiency improvement compared with three state-of-the-art unsupervised heterogeneous data learning methods (up to 140 000 times) while it achieves a comparable performance in terms of effectiveness.},
keywords={Big Data;computational complexity;data analysis;unsupervised learning;big data;knowledge extraction;TUMK-ELM;computational complexity;two-stage unsupervised multiple kernel extreme learning machine;real-time learning;big data analytics scenarios;unlabeled heterogeneous data;advanced unsupervised learning techniques;fast unsupervised heterogeneous data learning approach;state-of-the-art unsupervised heterogeneous data learning methods;real-life data sets;heterogeneous data representation;Kernel;Task analysis;Unsupervised learning;Big Data;Data mining;Machine learning;Unsupervised learning;heterogeneous data;clustering;extreme learning machine;multiple kernel learning},
doi={10.1109/ACCESS.2018.2847037},
ISSN={2169-3536},
month={},}
@ARTICLE{8396259,
author={X. Gong and L. Mao and Y. Liu and Q. Lin},
journal={IEEE Access},
title={A Jacobi Generalized Orthogonal Joint Diagonalization Algorithm for Joint Blind Source Separation},
year={2018},
volume={6},
number={},
pages={38464-38474},
abstract={Joint blind source separation (J-BSS) has emerged as a data-driven technique for multi-set data fusion applications. In this paper, we propose a Jacobi generalized orthogonal joint diagonalization (GOJD) algorithm for J-BSS of multiset signals. By the use of second-order statistics, we can obtain multiple sets of auto-covariance and cross-covariance matrices from the multi-set signals, which together admit a GOJD formulation. For computing the GOJD, we propose a computationally efficient Jacobi algorithm, which uses a sequence of Givens rotations to simultaneously diagonalize the covariance matrices. In comparison with other GOJD algorithms, the proposed algorithm is shown to have fast convergence. Moreover, as the optimal Givens rotation matrix in each update is calculated in closed-form, this algorithm is computationally very efficient. In the application aspect, we have considered the scenario where different data sets in J-BSS may have different number of sources, among which there exist both similar components that are consistently present in multiple data sets, and diverse components that are uniquely present in each data set. We have shown how J-BSS based on the proposed GOJD algorithm can effectively extract both similar and diverse source components. Simulation results are given to show the nice performance of the proposed algorithm, with regards to both speed and accuracy, in comparison with other algorithms of similar type.},
keywords={blind source separation;covariance matrices;data analysis;optimisation;sensor fusion;statistical analysis;optimal givens rotation matrix;fast convergence;second-order statistics;Jacobi algorithm;auto-covariance matrices;GOJD formulation;cross-covariance matrices;multiset signals;multiset data fusion applications;data-driven technique;joint blind source separation;Jacobi generalized orthogonal joint diagonalization algorithm;diverse source components;multiple data sets;J-BSS;Jacobian matrices;Covariance matrices;Loading;Matrix decomposition;Blind source separation;Indexes;Data models;Joint blind source separation;joint diagonalization;Jacobi;Givens rotation},
doi={10.1109/ACCESS.2018.2850784},
ISSN={2169-3536},
month={},}
@ARTICLE{5936065,
author={F. Zhuang and P. Luo and Z. Shen and Q. He and Y. Xiong and Z. Shi and H. Xiong},
journal={IEEE Transactions on Knowledge and Data Engineering},
title={Mining Distinction and Commonality across Multiple Domains Using Generative Model for Text Classification},
year={2012},
volume={24},
number={11},
pages={2025-2039},
abstract={The distribution difference among multiple domains has been exploited for cross-domain text categorization in recent years. Along this line, we show two new observations in this study. First, the data distribution difference is often due to the fact that different domains use different index words to express the same concept. Second, the association between the conceptual feature and the document class can be stable across domains. These two observations actually indicate the distinction and commonality across domains. Inspired by the above observations, we propose a generative statistical model, named Collaborative Dual-PLSA (CD-PLSA), to simultaneously capture both the domain distinction and commonality among multiple domains. Different from Probabilistic Latent Semantic Analysis (PLSA) with only one latent variable, the proposed model has two latent factors y and z, corresponding to word concept and document class, respectively. The shared commonality intertwines with the distinctions over multiple domains, and is also used as the bridge for knowledge transformation. An Expectation Maximization (EM) algorithm is developed to solve the CD-PLSA model, and further its distributed version is exploited to avoid uploading all the raw data to a centralized location and help to mitigate privacy concerns. After the training phase with all the data from multiple domains we propose to refine the immediate outputs using only the corresponding local data. In summary, we propose a two-phase method for cross-domain text classification, the first phase for collaborative training with all the data, and the second step for local refinement. Finally, we conduct extensive experiments over hundreds of classification tasks with multiple source domains and multiple target domains to validate the superiority of the proposed method over existing state-of-the-art methods of supervised and transfer learning. It is noted to mention that as shown by the experimental results CD-PLSA for the collaborative training is more tolerant of distribution differences, and the local refinement also gains significant improvement in terms of classification accuracy.},
keywords={computer based training;data mining;data privacy;expectation-maximisation algorithm;groupware;learning (artificial intelligence);pattern classification;statistical analysis;text analysis;mining distinction;multiple domains;cross-domain text categorization;data distribution difference;different index words;document class;commonality across domains;distinction across domains;generative statistical model;collaborative dual-PLSA;CD-PLSA;probabilistic latent semantic analysis;PLSA;word concept;commonality intertwines;knowledge transformation;expectation maximization algorithm;EM algorithm;privacy concern mitigation;two-phase method;collaborative training;local refinement;multiple source domains;multiple target domains;transfer learning;supervised learning;Data models;Training;Mathematical model;Collaboration;Joints;Graphical models;Companies;Statistical generative models;cross-domain learning;distinction and commonality;classification},
doi={10.1109/TKDE.2011.143},
ISSN={1041-4347},
month={Nov},}
@ARTICLE{8456507,
author={K. Wu and Z. Chen and W. Li},
journal={IEEE Access},
title={A Novel Intrusion Detection Model for a Massive Network Using Convolutional Neural Networks},
year={2018},
volume={6},
number={},
pages={50850-50859},
abstract={More and more network traffic data have brought great challenge to traditional intrusion detection system. The detection performance is tightly related to selected features and classifiers, but traditional feature selection algorithms and classification algorithms can't perform well in massive data environment. Also the raw traffic data are imbalanced, which has a serious impact on the classification results. In this paper, we propose a novel network intrusion detection model utilizing convolutional neural networks (CNNs). We use CNN to select traffic features from raw data set automatically, and we set the cost function weight coefficient of each class based on its numbers to solve the imbalanced data set problem. The model not only reduces the false alarm rate (FAR) but also improves the accuracy of the class with small numbers. To reduce the calculation cost further, we convert the raw traffic vector format into image format. We use the standard NSL-KDD data set to evaluate the performance of the proposed CNN model. The experimental results show that the accuracy, FAR, and calculation cost of the proposed model perform better than traditional standard algorithms. It is an effective and reliable solution for the intrusion detection of a massive network.},
keywords={computer network security;data mining;feature selection;feedforward neural nets;learning (artificial intelligence);pattern classification;telecommunication traffic;calculation cost;massive network;convolutional neural networks;network traffic data;detection performance;classifiers;massive data environment;raw traffic data;classification results;traffic features;cost function weight coefficient;imbalanced data;raw traffic vector;standard NSL-KDD data;CNN model;network intrusion detection model;Intrusion detection;Feature extraction;Training;Data models;Data preprocessing;Convolutional neural networks;Network intrusion detection;convolutional neural networks;image data format conversion;cost function weight;imbalanced dataset},
doi={10.1109/ACCESS.2018.2868993},
ISSN={2169-3536},
month={},}
@ARTICLE{4752829,
author={P. R. Rao and B. Moon},
journal={IEEE Transactions on Knowledge and Data Engineering},
title={Locating XML Documents in a Peer-to-Peer Network Using Distributed Hash Tables},
year={2009},
volume={21},
number={12},
pages={1737-1752},
abstract={One of the key challenges in a peer-to-peer (P2P) network is to efficiently locate relevant data sources across a large number of participating peers. With the increasing popularity of the extensible markup language (XML) as a standard for information interchange on the Internet, XML is commonly used as an underlying data model for P2P applications to deal with the heterogeneity of data and enhance the expressiveness of queries. In this paper, we address the problem of efficiently locating relevant XML documents in a P2P network, where a user poses queries in a language such as XPath. We have developed a new system called psiX that runs on top of an existing distributed hashing framework. Under the psiX system, each XML document is mapped into an algebraic signature that captures the structural summary of the document. An XML query pattern is also mapped into a signature. The query's signature is used to locate relevant document signatures. Our signature scheme supports holistic processing of query patterns without breaking them into multiple path queries and processing them individually. The participating peers in the network collectively maintain a collection of distributed hierarchical indexes for the document signatures. Value indexes are built to handle numeric and textual values in XML documents. These indexes are used to process queries with value predicates. Our experimental study on PlanetLab demonstrates that psiX provides an efficient location service in a P2P network for a wide variety of XML documents.},
keywords={data models;database indexing;digital signatures;document handling;Internet;peer-to-peer computing;query languages;query processing;XML document signature scheme;peer-to-peer network;distributed hash table;P2P network;extensible markup language;information interchange;Internet;data model;psiX system;distributed hashing framework;algebraic signature;XML query pattern;holistic processing;multiple path query;distributed hierarchical index;textual value index;numeric value;query language;XML;Peer to peer computing;Internet;Data models;Indexing;Query processing;Computer Society;Moon;Genomics;XML indexing;XPath;peer-to-peer computing;distributed hash tables.},
doi={10.1109/TKDE.2009.26},
ISSN={1041-4347},
month={Dec},}
@ARTICLE{7368881,
author={E. Fotopoulou and A. Zafeiropoulos and D. Papaspyros and P. Hasapis and G. Tsiolis and T. Bouras and S. Mouzakitis and N. Zanetti},
journal={IEEE Access},
title={Linked Data Analytics in Interdisciplinary Studies: The Health Impact of Air Pollution in Urban Areas},
year={2016},
volume={4},
number={},
pages={149-164},
abstract={The design of solutions that are able to exploit the available data collected in smart cities environments can lead to insights that can guide the implementation of approaches that have the potential to significantly improve the quality of life within a city. Such solutions include tools for the production of advanced analytics considering data fusion challenges. The preparation of qualitative input data sets, collected in many cases through heterogeneous sources and represented in various formats, constitute a very important step toward a meaningful analysis. Such input data sets, combined with approaches that reduce the data processing burden and support the easy and flexible-in terms of configuration-replication of an analysis, can lead to the next generation analytics tools. In this paper, a novel approach toward the production and consumption of linked data analytics in urban environments is presented. The approach is based on the exploitation of linked data principles, enhancing the ability of managing and processing of data, in ways not available before. In addition to the description of the overall technical approach, the application of the proposed solution into a real-life scenario for examining the health impact of outdoor air pollution in urban areas within an international, national, and regional perspective is detailed. A set of interesting results are produced along with their interpretation toward the provision of suggestions for policy making purposes.},
keywords={air pollution;data analysis;health care;sensor fusion;smart cities;Linked data analytics;interdisciplinary studies;health impact;air pollution;urban areas;smart cities environments;data fusion challenge;qualitative input data sets;heterogeneous sources;next generation analytics tools;data management;data processing;international perspective;national perspective;regional perspective;Open systems;Data analysis;Distributed databases;Air pollution;Smart cities;Data integration;Urban areas;Environmental factors;open data;linked data;linked data analytics;urban analytics;data mining;outdoor air pollution;particulate matter;mortality;burden of disease;clean air programme;Open data;linked data;linked data analytics;urban analytics;data mining;outdoor air pollution;particulate matter;mortality;burden of disease;clean air programme},
doi={10.1109/ACCESS.2015.2513439},
ISSN={2169-3536},
month={},}
@ARTICLE{6104043,
author={X. Shi and Q. Liu and W. Fan and P. S. Yu},
journal={IEEE Transactions on Knowledge and Data Engineering},
title={Transfer across Completely Different Feature Spaces via Spectral Embedding},
year={2013},
volume={25},
number={4},
pages={906-918},
abstract={In many applications, it is very expensive or time consuming to obtain a lot of labeled examples. One practically important problem is: can the labeled data from other related sources help predict the target task, even if they have 1) different feature spaces (e.g., image versus text data), 2) different data distributions, and 3) different output spaces? This paper proposes a solution and discusses the conditions where this is highly likely to produce better results. It first unifies the feature spaces of the target and source data sets by spectral embedding, even when they are with completely different feature spaces. The principle is to devise an optimization objective that preserves the original structure of the data, while at the same time, maximizes the similarity between the two. A linear projection model, as well as a nonlinear approach are derived on the basis of this principle with closed forms. Second, a judicious sample selection strategy is applied to select only those related source examples. At last, a Bayesian-based approach is applied to model the relationship between different output spaces. The three steps can bridge related heterogeneous sources in order to learn the target task. Among the 20 experiment data sets, for example, the images with wavelet-transformed-based features are used to predict another set of images whose features are constructed from color-histogram space; documents are used to help image classification, etc. By using these extracted examples from heterogeneous sources, the models can reduce the error rate by as much as 50 percent, compared with the methods using only the examples from the target task.},
keywords={Bayes methods;data analysis;document image processing;feature extraction;image classification;nonlinear programming;spectral analysis;wavelet transforms;feature space;data distribution;source data sets;spectral embedding;target data sets;optimization;linear projection model;nonlinear approach;sample selection strategy;Bayesian-based approach;heterogeneous sources;wavelet transformed-based feature;color histogram space;document processing;image classification;image feature construction;Optimization;Data models;Bridges;Vectors;Training data;Training;Bioinformatics;Feature generation;heterogeneous data;transfer learning},
doi={10.1109/TKDE.2011.252},
ISSN={1041-4347},
month={April},}
@ARTICLE{7520638,
author={C. Xu and Y. Zhang and R. Li and X. Wu},
journal={IEEE Transactions on Knowledge and Data Engineering},
title={On the Feasibility of Distributed Kernel Regression for Big Data},
year={2016},
volume={28},
number={11},
pages={3041-3052},
abstract={In Big Data applications, massive datasets with huge numbers of observations are frequently encountered. To deal with such massive datasets, a divide-and-conquer scheme (e.g., MapReduce) is often used for the analysis of Big Data. With such a strategy, a large dataset (e.g., a centralized real database or a virtual database with distributed data sources) is first divided into smaller manageable segments; the final output is then aggregated from the individual outputs of the segments. Despite its popularity in practice, it remains largely unknown whether such a distributive strategy provides valid theoretical inferences to the original data. In this paper, we address this fundamental issue for the distributed kernel regression (DKR) problem, where the algorithmic feasibility is measured by the generalization performance of the resulting estimator. To justify DKR, a uniform convergence rate is needed for bounding the generalization error over the individual outputs, which brings new and challenging issues in the Big Data setup. Using a sample dependent kernel dictionary, we show that, with proper data segmentation, DKR leads to an estimator that is generalization consistent to the unknown regression function. This result theoretically justifies DKR and sheds light on more advanced distributive algorithms for processing Big Data. The promising performance of the method is supported by both simulation and real data examples.},
keywords={Big Data;data analysis;divide and conquer methods;regression analysis;distributed kernel regression;Big Data analysis;massive datasets;divide-and-conquer scheme;distributive strategy;DKR problem;generalization performance;generalization error;sample dependent kernel dictionary;data segmentation;unknown regression function;advanced distributive algorithms;Big Data processing;Big data;Kernel;Distributed databases;Distributed algorithms;Estimation;Data models;Algorithm design and analysis;Big data;divide-and-conquer;distributed algorithm;generalization error;kernel regression},
doi={10.1109/TKDE.2016.2594060},
ISSN={1041-4347},
month={Nov},}
@ARTICLE{5342420,
author={F. Zhuang and P. Luo and H. Xiong and Y. Xiong and Q. He and Z. Shi},
journal={IEEE Transactions on Knowledge and Data Engineering},
title={Cross-Domain Learning from Multiple Sources: A Consensus Regularization Perspective},
year={2010},
volume={22},
number={12},
pages={1664-1678},
abstract={Classification across different domains studies how to adapt a learning model from one domain to another domain which shares similar data characteristics. While there are a number of existing works along this line, many of them are only focused on learning from a single source domain to a target domain. In particular, a remaining challenge is how to apply the knowledge learned from multiple source domains to a target domain. Indeed, data from multiple source domains can be semantically related, but have different data distributions. It is not clear how to exploit the distribution differences among multiple source domains to boost the learning performance in a target domain. To that end, in this paper, we propose a consensus regularization framework for learning from multiple source domains to a target domain. In this framework, a local classifier is trained by considering both local data available in one source domain and the prediction consensus with the classifiers learned from other source domains. Moreover, we provide a theoretical analysis as well as an empirical study of the proposed consensus regularization framework. The experimental results on text categorization and image classification problems show the effectiveness of this consensus regularization learning method. Finally, to deal with the situation that the multiple source domains are geographically distributed, we also develop the distributed version of the proposed algorithm, which avoids the need to upload all the data to a centralized location and helps to mitigate privacy concerns.},
keywords={distributed algorithms;image classification;learning (artificial intelligence);multimedia computing;text analysis;cross-domain learning performance;learning model;data characteristics;single source domain;multiple source domain;data distribution;local classifier;prediction consensus;text categorization;image classification;consensus regularization learning;distributed algorithm;privacy;Training data;Text categorization;Image classification;Learning systems;Data privacy;Laboratories;Information processing;Computers;Information management;Classification;multiple source domains;cross-domain learning;consensus regularization.},
doi={10.1109/TKDE.2009.205},
ISSN={1041-4347},
month={Dec},}
@ARTICLE{8302910,
author={L. Bai and Z. Jia and J. Liu},
journal={IEEE Access},
title={Reengineering Object-Oriented Fuzzy Spatiotemporal Data into XML},
year={2018},
volume={6},
number={},
pages={12686-12699},
abstract={With the rapid development of the Internet, XML has become the defacto standard for integrating and exchanging data. Since more and more business data are stored in object-oriented database, we study the methodology of modeling fuzzy spatiotemporal data and transforming fuzzy spatiotemporal data from object-oriented databases to XML as well. In order to allow for better and platform independent sharing of fuzzy spatiotemporal data stored in an object-oriented format, we devise a fuzzy spatiotemporal data model in object-oriented database to capture the semantics of spatiotemporal features. In particular, XML schema best describes the existing fuzzy object-oriented schema, and we investigate the transforming rules of spatiotemporal data from fuzzy object-oriented database to XML. Furthermore, an instance demonstrates the validation of our approach. Such approach of transformation can provide a significant consolidation of the interoperability of fuzzy spatiotemporal data from object-oriented databases to XML.},
keywords={data models;electronic data interchange;fuzzy set theory;Internet;object-oriented databases;XML;XML schema;object-oriented fuzzy spatiotemporal data;business data;fuzzy spatiotemporal data model;spatiotemporal features;data exchange;fuzzy object-oriented database;interoperability;Spatiotemporal phenomena;XML;Databases;Object oriented modeling;Data models;Fats;Object recognition;XML;object-oriented database;fuzzy spatiotemporal data;transformation},
doi={10.1109/ACCESS.2018.2809858},
ISSN={2169-3536},
month={},}
@ARTICLE{8453012,
author={W. Yuan and H. Wang and B. Hu and L. Wang and Q. Wang},
journal={IEEE Access},
title={Wide and Deep Model of Multi-Source Information-Aware Recommender System},
year={2018},
volume={6},
number={},
pages={49385-49398},
abstract={Collaborative filtering recommendation suffers from the problems of high data sparsity, poor expansibility, cold start, and the difficulty of modeling user preferences, among which data sparsity is the greatest issue. Although our previous work on matrix completion model, named low rank non-negative matrix factorization and completion algorithm (LR-NMFC) and stochastic sub-gradient based low rank matrix completion algorithm, could effectively alleviate the sparsity problem, they customarily model the linear feature interactions instead of the complex nonlinear structures between users and items when making recommendations. To better depict user preferences and item features, we deepen the linear model LR-NMFC to establish a wide and deep model, which we named Wide and Deep model of Multi-source information-Aware recommender system (WDMMA), based on multi-source information composed of user-item interaction matrix, attributes, and context. The wide part mainly handles the linear interactions between users and items, while the deep part portrays the high-order nonlinear interactions. We pre-train both the wide and the deep part using LR-NMFC in the embedding layer. In the pooling layer, we define a pooling operation, AC-pooling, which is used to model the various interactions among users, items, attributes, and context information. Upon the pooling layer, we stack some hidden layers to capture the high-order nonlinear feature interactions. Experiments on two public datasets show that WDMMA can learn complex nonlinear feature patterns successfully and effectively and is beneficial to improve the recommendation performance. Therefore, it is an effective way to consider both linear user-item interactions and multi-source information-aware nonlinear interactions in a deep learning framework when making recommendations.},
keywords={collaborative filtering;learning (artificial intelligence);matrix decomposition;recommender systems;completion algorithm;low rank nonnegative matrix factorization;multisource information-aware recommender system;collaborative filtering recommendation;high-order nonlinear interactions;deep part;linear interactions;wide part;user-item interaction matrix;deep model;wide model;linear model LR-NMFC;linear feature interactions;sparsity problem;stochastic sub-gradient based low rank matrix completion algorithm;matrix completion model;modeling user preferences;high data sparsity;making recommendations;linear user-item interactions;complex nonlinear feature patterns;high-order nonlinear feature interactions;pooling layer;Recommender systems;Feature extraction;Collaboration;Context modeling;Machine learning;Data models;Wavelength division multiplexing;Collaborative filtering recommendation;data sparsity;deep learning;high-order nonlinear interactions;multi-source information},
doi={10.1109/ACCESS.2018.2868083},
ISSN={2169-3536},
month={},}
@ARTICLE{7463492,
author={H. Soleimani and D. J. Miller},
journal={IEEE Transactions on Knowledge and Data Engineering},
title={ATD: Anomalous Topic Discovery in High Dimensional Discrete Data},
year={2016},
volume={28},
number={9},
pages={2267-2280},
abstract={We propose an algorithm for detecting patterns exhibited by anomalous clusters in high dimensional discrete data. Unlike most anomaly detection (AD) methods, which detect individual anomalies, our proposed method detects groups (clusters) of anomalies; i.e., sets of points which collectively exhibit abnormal patterns. In many applications, this can lead to a better understanding of the nature of the atypical behavior and to identifying the sources of the anomalies. Moreover, we consider the case where the atypical patterns exhibit on only a small (salient) subset of the very high dimensional feature space. Individual AD techniques and techniques that detect anomalies using all the features typically fail to detect such anomalies, but our method can detect such instances collectively, discover the shared anomalous patterns exhibited by them, and identify the subsets of salient features. In this paper, we focus on detecting anomalous topics in a batch of text documents, developing our algorithm based on topic models. Results of our experiments show that our method can accurately detect anomalous topics and salient features (words) under each such topic in a synthetic data set and two real-world text corpora and achieves better performance compared to both standard group AD and individual AD techniques. All required code to reproduce our experiments is available from https://github.com/hsoleimani/ATD.},
keywords={feature extraction;security of data;text analysis;ATD;anomalous topic discovery;high dimensional discrete data;pattern detection;anomalous clusters;anomaly source identification;feature space;anomaly detection;salient feature subset identification;anomalous topic detection;text documents;topic models;real-world text corpora;synthetic data set;Data models;Feature extraction;Training;Clustering algorithms;Computational modeling;Companies;Advertising;Anomaly detection;pattern detection;topic models;topic discovery},
doi={10.1109/TKDE.2016.2561288},
ISSN={1041-4347},
month={Sept},}
@ARTICLE{7056557,
author={U. Raza and A. Camerra and A. L. Murphy and T. Palpanas and G. P. Picco},
journal={IEEE Transactions on Knowledge and Data Engineering},
title={Practical Data Prediction for Real-World Wireless Sensor Networks},
year={2015},
volume={27},
number={8},
pages={2231-2244},
abstract={Data prediction is proposed in wireless sensor networks (WSNs) to extend the system lifetime by enabling the sink to determine the data sampled, within some accuracy bounds, with only minimal communication from source nodes. Several theoretical studies clearly demonstrate the tremendous potential of this approach, able to suppress the vast majority of data reports at the source nodes. Nevertheless, the techniques employed are relatively complex, and their feasibility on resource-scarce WSN devices is often not ascertained. More generally, the literature lacks reports from real-world deployments, quantifying the overall system-wide lifetime improvements determined by the interplay of data prediction with the underlying network. These two aspects, feasibility and system-wide gains, are key in determining the practical usefulness of data prediction in real-world WSN applications. In this paper, we describe derivative-based prediction (DBP), a novel data prediction technique much simpler than those found in the literature. Evaluation with real data sets from diverse WSN deployments shows that DBP often performs better than the competition, with data suppression rates up to 99 percent and good prediction accuracy. However, experiments with a real WSN in a road tunnel show that, when the network stack is taken into consideration, DBP only triples lifetime-a remarkable result per se, but a far cry from the data suppression rates above. To fully achieve the energy savings enabled by data prediction, the data and network layers must be jointly optimized. In our testbed experiments, a simple tuning of the MAC and routing stack, taking into account the operation of DBP, yields a remarkable seven-fold lifetime improvement w.r.t. the mainstream periodic reporting.},
keywords={data analysis;resource allocation;telecommunication computing;wireless sensor networks;real-world wireless sensor networks;data sampling;resource-scarce WSN devices;system-wide lifetime improvements;system-wide gains;real-world WSN applications;derivative-based prediction;DBP;data prediction technique;data suppression rates;network stack;energy savings;MAC;routing stack;Wireless sensor networks;Data models;Computational modeling;Predictive models;;Wireless sensor networks;data prediction;time series forecasting;energy efficiency;network protocols},
doi={10.1109/TKDE.2015.2411594},
ISSN={1041-4347},
month={Aug},}
@ARTICLE{6104047,
author={L. Wang and P. Wu and H. Chen},
journal={IEEE Transactions on Knowledge and Data Engineering},
title={Finding Probabilistic Prevalent Colocations in Spatially Uncertain Data Sets},
year={2013},
volume={25},
number={4},
pages={790-804},
abstract={A spatial colocation pattern is a group of spatial features whose instances are frequently located together in geographic space. Discovering colocations has many useful applications. For example, colocated plant species discovered from plant distribution data sets can contribute to the analysis of plant geography, phytosociology studies, and plant protection recommendations. In this paper, we study the colocation mining problem in the context of uncertain data, as the data generated from a wide range of data sources are inherently uncertain. One straightforward method to mine the prevalent colocations in a spatially uncertain data set is to simply compute the expected participation index of a candidate and decide if it exceeds a minimum prevalence threshold. Although this definition has been widely adopted, it misses important information about the confidence which can be associated with the participation index of a colocation. We propose another definition, probabilistic prevalent colocations, trying to find all the colocations that are likely to be prevalent in a randomly generated possible world. Finding probabilistic prevalent colocations (PPCs) turn out to be difficult. First, we propose pruning strategies for candidates to reduce the amount of computation of the probabilistic participation index values. Next, we design an improved dynamic programming algorithm for identifying candidates. This algorithm is suitable for parallel computation, and approximate computation. Finally, the effectiveness and efficiency of the methods proposed as well as the pruning strategies and the optimization techniques are verified by extensive experiments with “real + synthetic” spatially uncertain data sets.},
keywords={data mining;dynamic programming;parallel processing;probability;visual databases;probabilistic prevalent colocations;real-synthetic spatially uncertain data sets;spatial colocation pattern;spatial features;geographic space;colocation discovery;colocation mining problem;data sources;candidate participation index;minimum prevalence threshold;participation index colocation;PPC;pruning strategy;probabilistic participation index values;dynamic programming algorithm;parallel computation;approximate computation;optimization techniques;Indexes;Probabilistic logic;Dynamic programming;Data mining;Heuristic algorithms;Approximation algorithms;Data models;Spatial colocations;spatially uncertain data set;possible worlds;probabilistic prevalent colocations (PPCs);dynamic programming;approximate algorithms},
doi={10.1109/TKDE.2011.256},
ISSN={1041-4347},
month={April},}
@ARTICLE{8320377,
author={C. Kan and G. Ding and Q. Wu and R. Li and F. Song},
journal={IEEE Access},
title={Robust Relative Fingerprinting-Based Passive Source Localization via Data Cleansing},
year={2018},
volume={6},
number={},
pages={19255-19269},
abstract={Recently, source localization is becoming a major research focus. The majority of the existing studies focus on the design of received signal strength (RSS)-based localization methods. However, when in the face of complicated environments with severe fading, RSS-based localization methods achieve relatively inferior accuracy performance, compared with fingerprinting-based localization methods. Nevertheless, traditional fingerprinting-based localization methods are subject to the condition that the source transmit power is known, which cannot be directly used in passive localization cases where the sensing nodes do not have the prior information on the source. In addition, the received sensing data may contain errors and then affect the location precision due to various abnormal conditions, such as device failure and malicious cases. In this paper, we propose a novel robust relative fingerprinting-based passive localization algorithm via a data cleansing approach. First, we figure out the fingerprint correlations property and introduce a new relative fingerprint framework. The key idea is that by exploring the correlations between the source fingerprint and the reference fingerprint database, the correction factors can be achieved to apply the fingerprint idea into the passive localization case. Second, we formulate a generalized modeling of the abnormal data in localization problem and propose a data cleansing approach which utilizes the sparse property of the abnormal data. Based on this, the negative influence of abnormal data can be further eliminated. Third, considering the sparse property of the source position, we use the sparse Bayesian learning in the matching process for the purpose of achieving more precise estimated source position. Simulation results demonstrate that the proposed algorithm achieves higher accuracy performance in passive source localization in terms of eliminating the abnormal data impairment.},
keywords={Bayes methods;learning (artificial intelligence);radionavigation;telecommunication computing;sparse Bayesian learning;robust relative fingerprinting;precise estimated source position;sparse property;reference fingerprint database;source fingerprint;relative fingerprint framework;fingerprint correlations property;data cleansing approach;received sensing data;source transmit power;traditional fingerprinting-based localization methods;relatively inferior accuracy performance;RSS;received signal strength-based localization methods;passive source localization;Sensors;Databases;Robustness;Fingerprint recognition;Correlation;Data models;Estimation;Passive localization;robust localization;fingerprint;correlation;data cleansing},
doi={10.1109/ACCESS.2018.2817576},
ISSN={2169-3536},
month={},}
@ARTICLE{7809026,
author={B. Han and L. Luo and G. Sheng and G. Li and X. Jiang},
journal={IEEE Access},
title={Framework of Random Matrix Theory for Power System Data Mining in a Non-Gaussian Environment},
year={2016},
volume={4},
number={},
pages={9969-9977},
abstract={A novel empirical data analysis methodology based on the random matrix theory (RMT) and time series analysis is proposed for the power systems. Among the ongoing research studies of big data in the power system applications, there is a strong necessity for new mathematical tools that describe and analyze big data. This paper used RMT to model the empirical data which also treated as a time series. The proposed method extends traditional RMT for applications in a non-Gaussian distribution environment. Three case studies, i.e., power equipment condition monitoring, voltage stability analysis and low-frequency oscillation detection, illustrate the potential application value of our proposed method for multi-source heterogeneous data analysis, sensitive spot awareness and fast signal detection under an unknown noise pattern. The results showed that the empirical data from a power system modeled following RMT and in a time series have high sensitivity to dynamically characterized system states as well as observability and efficiency in system analysis compared with conventional equation-based methods.},
keywords={Big Data;condition monitoring;data analysis;matrix algebra;power apparatus;power engineering computing;power system reliability;random processes;signal detection;time series;random matrix theory;time series analysis;Big Data analysis;power system applications;mathematical tools;RMT;nonGaussian distribution;power equipment condition monitoring;voltage stability analysis;low-frequency oscillation detection;multisource heterogeneous data analysis;sensitive spot awareness;fast signal detection;unknown noise pattern;power system data mining;Power system stability;Time series analysis;Mathematical model;Big data;Data models;Stability analysis;Real-time systems;Data mining;Random matrix theory;data mining;time series analysis;non-Gaussian;condition monitoring;static voltage stability;low frequency oscillation},
doi={10.1109/ACCESS.2017.2649841},
ISSN={2169-3536},
month={},}
@ARTICLE{7358049,
author={A. Page and T. Soyata and J. Couderc and M. Aktas},
journal={IEEE Access},
title={An Open Source ECG Clock Generator for Visualization of Long-Term Cardiac Monitoring Data},
year={2015},
volume={3},
number={},
pages={2704-2714},
abstract={The collection of long-term health data is accelerating with the advent of portable/wearable medical devices including electrocardiograms (ECGs). This large corpus of data presents great opportunities to improve the quality of cardiac care. However, analyzing the data from these sensors is a challenge; the relevant information from ~120 000 heart beats per patient per day must be condensed into a human-readable form. Our goal is to facilitate the analysis of these unwieldy data sets. We have developed an open source tool for creating easy-to-interpret plots of cardiac information over long periods. We call these plots ECG clocks. The utility of our ECG clock library is demonstrated through multiple examples drawn from a database of 24-h Holter recordings. In these case studies, we focus on the visualization of heart rate and QT dynamics. The ECG clock concept is shown to be relevant for both physicians and researchers, for identifying healthy and abnormal values and patterns in ECG recordings. In this paper, we describe how to use the ECG clock library to analyze 24-h ECG recordings, and how to extend the source code for your own purposes. The tool is applicable to a wide range of cardiac monitoring tasks, such as heart rate variability or ST elevation. This library, which we have made freely available, can help provide new insights into circadian patterns of cardiac function in individuals and groups.},
keywords={circadian rhythms;data visualisation;electrocardiography;medical signal processing;patient monitoring;public domain software;cardiac function;circadian patterns;ST elevation;cardiac monitoring tasks;source code;heart rate variability;QT dynamics;Holter recordings;heart beats;cardiac care;wearable medical devices;portable medical devices;long-term health data;long-term cardiac monitoring data visualization;electrocardiograms;open source ECG clock generator;Electrocardiography;Clocks;Heart rate;Monitoring;Libraries;Databases;Medical services;Electrocardiogram;heart rate variability;long QT syndrome;open-source software;visualization;Electrocardiogram;heart rate variability;long QT syndrome;open-source software;visualization},
doi={10.1109/ACCESS.2015.2509426},
ISSN={2169-3536},
month={},}
@ARTICLE{6482563,
author={L. Meng and A. Tan and D. Xu},
journal={IEEE Transactions on Knowledge and Data Engineering},
title={Semi-Supervised Heterogeneous Fusion for Multimedia Data Co-Clustering},
year={2014},
volume={26},
number={9},
pages={2293-2306},
abstract={Co-clustering is a commonly used technique for tapping the rich meta-information of multimedia web documents, including category, annotation, and description, for associative discovery. However, most co-clustering methods proposed for heterogeneous data do not consider the representation problem of short and noisy text and their performance is limited by the empirical weighting of the multi-modal features. In this paper, we propose a generalized form of Heterogeneous Fusion Adaptive Resonance Theory, called GHF-ART, for co-clustering of large-scale web multimedia documents. By extending the two-channel Heterogeneous Fusion ART (HF-ART) to multiple channels, GHF-ART is designed to handle multimedia data with an arbitrarily rich level of meta-information. For handling short and noisy text, GHF-ART does not learn directly from the textual features. Instead, it identifies key tags by learning the probabilistic distribution of tag occurrences. More importantly, GHF-ART incorporates an adaptive method for effective fusion of multi-modal features, which weights the features of multiple data sources by incrementally measuring the importance of feature modalities through the intra-cluster scatters. Extensive experiments on two web image data sets and one text document set have shown that GHF-ART achieves significantly better clustering performance and is much faster than many existing state-of-the-art algorithms.},
keywords={ART neural nets;data mining;document handling;Internet;learning (artificial intelligence);multimedia computing;pattern clustering;statistical distributions;semisupervised heterogeneous fusion;multimedia data co-clustering method;rich meta-information;multimedia Web documents;associative discovery;heterogeneous data;multimodal features;heterogeneous fusion adaptive resonance theory;GHF-ART;large-scale Web multimedia documents;two-channel heterogeneous fusion ART;multimedia data handling;noisy text;textual features;probabilistic distribution;tag occurrences;adaptive method;multiple data sources;intra-cluster scatters;Web image data sets;text document set;semisupervised learning;multimedia data mining;Subspace constraints;Vectors;Clustering algorithms;Multimedia communication;Visualization;Feature extraction;Pattern matching;Semi-supervised learning;heterogeneous data co-clustering;multimedia data mining},
doi={10.1109/TKDE.2013.47},
ISSN={1041-4347},
month={Sept},}
@ARTICLE{8319952,
author={A. González-Vidal and P. Barnaghi and A. F. Skarmeta},
journal={IEEE Transactions on Knowledge and Data Engineering},
title={BEATS: Blocks of Eigenvalues Algorithm for Time Series Segmentation},
year={2018},
volume={30},
number={11},
pages={2051-2064},
abstract={The massive collection of data via emerging technologies like the Internet of Things (IoT) requires finding optimal ways to reduce the observations in the time series analysis domain. The IoT time series require aggregation methods that can preserve and represent the key characteristics of the data. In this paper, we propose a segmentation algorithm that adapts to unannounced mutations of the data (i.e., data drifts). The algorithm splits the data streams into blocks and groups them in square matrices, computes the Discrete Cosine Transform (DCT), and quantizes them. The key information is contained in the upper-left part of the resulting matrix. We extract this sub-matrix, compute the modulus of its eigenvalues, and remove duplicates. The algorithm, called BEATS, is designed to tackle dynamic IoT streams, whose distribution changes over time. We implement experiments with six datasets combining real, synthetic, real-world data, and data with drifts. Compared to other segmentation methods like Symbolic Aggregate approXimation (SAX), BEATS shows significant improvements. Trying it with classification and clustering algorithms it provides efficient results. BEATS is an effective mechanism to work with dynamic and multi-variate data, making it suitable for IoT data sources. The datasets, code of the algorithm and the analysis results can be accessed publicly at: https://github.com/auroragonzalez/BEATS.},
keywords={data aggregation;data analysis;data mining;discrete cosine transforms;eigenvalues and eigenfunctions;Internet of Things;matrix algebra;optimisation;pattern clustering;time series;segmentation algorithm;unannounced mutations;data drifts;data streams;square matrices;dynamic IoT streams;segmentation methods;classification;clustering algorithms;multivariate data;IoT data sources;optimal ways;time series analysis domain;IoT time series;aggregation methods;SAX;DCT;Internet of things;blocks of eigenvalues algorithm for time series segmentation;BEATS;symbolic aggregate approximation;discrete cosine transform;Time series analysis;Clustering algorithms;Approximation algorithms;Discrete cosine transforms;Signal processing algorithms;Machine learning algorithms;Standards;BEATS;SAX;data analytics;data aggregation;segmentation;DCT;smart cities},
doi={10.1109/TKDE.2018.2817229},
ISSN={1041-4347},
month={Nov},}
@ARTICLE{8166747,
author={L. Zhang and G. Qi and D. Zhang and J. Tang},
journal={IEEE Access},
title={Latent Dirichlet Truth Discovery: Separating Trustworthy and Untrustworthy Components in Data Sources},
year={2018},
volume={6},
number={},
pages={1741-1752},
abstract={The discovery of truth is a critical step toward effective information and knowledge utilization, especially in Web services, social media networks, and sensor networks. Typically, a set of sources with varying reliability claim observations about a set of objects and the goal is to jointly discover the true fact for each object and the trustworthy degree of each source. In this paper, we propose a latent Dirichlet truth (LDT) discovery model to approach this problem. It defines a random field over all the possible configurations of the trustworthy degrees of sources and facts, and the most probable configuration is inferred by a maximum a posteriori criterion over the observed claims. We note that a typical source is usually made of mixed trustworthy and untrustworthy components, since it can make true or false claims on different objects. While most of the existing algorithms do not attempt separate the untrustworthy component from the trustworthy one in each source, the proposed model explicitly identifies untrustworthy component in each source. This makes the LDT model more capable of separating the trustworthy and untrustworthy components, and in turn improves the accuracy of truth discovery. Experiments on real data sets show competitive results compared with existing algorithms.},
keywords={decision making;expectation-maximisation algorithm;learning (artificial intelligence);probability;social networking (online);Web services;facts;observed claims;typical source;untrustworthy component;data sets;data sources;knowledge utilization;social media networks;sensor networks;reliability claim observations;trustworthy degree;latent Dirichlet truth discovery model;information utilization;trustworthy component;LDT model;maximum a posteriori criterion;Random variables;Computer science;Social network services;Probabilistic logic;Web services;Web sites;Data mining;Truth discovery;trustworthy component;latent Dirichlet model},
doi={10.1109/ACCESS.2017.2780182},
ISSN={2169-3536},
month={},}
@ARTICLE{7875479,
author={N. Sabahat and A. A. Malik and F. Azam},
journal={IEEE Access},
title={A Size Estimation Model for Board-Based Desktop Games},
year={2017},
volume={5},
number={},
pages={4980-4990},
abstract={Software size estimation plays a key role in the planning of projects at the time of project inception. This paper describes the derivation, validation, and usage of a parametric model meant for estimating the size of board-based desktop games. This model is derived using forward stepwise multiple linear regression on a data set comprising over 60 open source board-based games collected from multiple open source repositories. A variety of prediction accuracy metrics (e.g., MMRE, PRED(x), MdMRE, and so on) are used to assess this model and K-fold cross-validation is used to validate this model. Model assessment and validation exercises yield promising results. The utility of this model is demonstrated by presenting a worked-out game size estimation example followed by some size-related what-if analyses.},
keywords={computer games;project management;public domain software;regression analysis;software cost estimation;software management;software metrics;software size estimation;project planning;project inception;parametric model;forward stepwise multiple linear regression;data set;multiple open source repositories;prediction accuracy metrics;K-fold cross-validation;model assessment;model validation;worked-out game size estimation;open source board-based desktop games;software cost estimation;Games;Estimation;Software;Object oriented modeling;Complexity theory;Predictive models;Size measurement;Linear regression;model fitting;model validation;open source software;software cost estimation;software games;software project management;software sizing},
doi={10.1109/ACCESS.2017.2678459},
ISSN={2169-3536},
month={},}
@ARTICLE{7976287,
author={C. Chiu and J. Zhan and F. Zhan},
journal={IEEE Access},
title={Uncovering Suspicious Activity From Partially Paired and Incomplete Multimodal Data},
year={2017},
volume={5},
number={},
pages={13689-13698},
abstract={Multimodal data can be used to gain additional perspective on a phenomenon. For applications, such as security and the detection of suspicious activity, the need to aggregate and analyze data from multiple modes is vital. Recent research in suspicious behavior detection has introduced methods for identifying and scoring dense blocks in multivariate tensors, which are consistent indicators of suspicious activity. None yet, however, have proposed a method for the merging and analysis of multiple modes of data for suspicious behavior, especially when the set of items described in each data set do not match-that is, the data is partially paired-which is common when data sets originate from different sources. Neither has a method been described for dealing with the similar case of incomplete data. This paper introduces a technique for multimodal data analysis for suspicious activity detection when the data are only partially paired and/or incomplete. The method is applied to synthetic and real data, demonstrating strong precision and recall even in poorly paired cases.},
keywords={data aggregation;data analysis;merging;tensors;partially paired data;incomplete multimodal data;security;suspicious activity detection;data aggregation;suspicious behavior detection;multivariate tensors;data merging;multimodal data analysis;Tensile stress;Measurement;Feature extraction;Twitter;Security;Data analysis;Suspicious activity;multimodal data;partially paired data;incomplete data},
doi={10.1109/ACCESS.2017.2726078},
ISSN={2169-3536},
month={},}
@ARTICLE{7911293,
author={F. Alam and R. Mehmood and I. Katib and N. N. Albogami and A. Albeshri},
journal={IEEE Access},
title={Data Fusion and IoT for Smart Ubiquitous Environments: A Survey},
year={2017},
volume={5},
number={},
pages={9533-9554},
abstract={The Internet of Things (IoT) is set to become one of the key technological developments of our times provided we are able to realize its full potential. The number of objects connected to IoT is expected to reach 50 billion by 2020 due to the massive influx of diverse objects emerging progressively. IoT, hence, is expected to be a major producer of big data. Sharing and collaboration of data and other resources would be the key for enabling sustainable ubiquitous environments, such as smart cities and societies. A timely fusion and analysis of big data, acquired from IoT and other sources, to enable highly efficient, reliable, and accurate decision making and management of ubiquitous environments would be a grand future challenge. Computational intelligence would play a key role in this challenge. A number of surveys exist on data fusion. However, these are mainly focused on specific application areas or classifications. The aim of this paper is to review literature on data fusion for IoT with a particular focus on mathematical methods (including probabilistic methods, artificial intelligence, and theory of belief) and specific IoT environments (distributed, heterogeneous, nonlinear, and object tracking environments). The opportunities and challenges for each of the mathematical methods and environments are given. Future developments, including emerging areas that would intrinsically benefit from data fusion and IoT, autonomous vehicles, deep learning for data fusion, and smart cities, are discussed.},
keywords={Big Data;data analysis;Internet of Things;learning (artificial intelligence);sensor fusion;data collaboration;big data analysis;mathematical methods;probabilistic methods;artificial intelligence;theory-of-belief;autonomous vehicles;deep learning;smart cities;data sharing;smart ubiquitous environments;Internet-of-Things;IoT;big data fusion;Data integration;Smart cities;Internet of Things;Big Data;Decision making;Computer science;Information technology;Internet of Things;big data;data fusion;computational and artificial intelligence;high performance computing;smart cities;smart societies;ubiquitous environments},
doi={10.1109/ACCESS.2017.2697839},
ISSN={2169-3536},
month={},}
@ARTICLE{7990488,
author={Y. Xu and Y. Sun and J. Wan and X. Liu and Z. Song},
journal={IEEE Access},
title={Industrial Big Data for Fault Diagnosis: Taxonomy, Review, and Applications},
year={2017},
volume={5},
number={},
pages={17368-17380},
abstract={Fault diagnosis is an important topic both in practice and research. There is intense pressure on industrial systems to continue reducing unscheduled downtime, performance degradation, and safety hazards, which requires detecting and recovering from potential faults as early as possible. From the historical perspective, this paper divides fault diagnosis into previous research and industrial big data era. According to primary drivers, this paper classifies fault diagnosis into knowledge-driven, data-driven, and value-driven methods. Among them, the former two approaches belong to the previous research on fault diagnosis. They mainly depend on expert experience and shallow models to detect and extract failures from relatively small size data. With the continuous exponential growth of data, it is insufficient to mine valuable fault information from massive multi-source heterogeneous data. The huge diagnostic value embodied in industrial big data has driven the emergence of the third category, which belongs to fault diagnosis based on big data. It consists of big data processing and analysis corresponding to high efficiency, cost effectiveness, and generality, which can deal well with problems that previous methods faced. We introduce the concept of a device electrocardiogram from the perspective of applicability to outline the present status of fault diagnosis for big data, and compare it with traditional diagnostic system. We also discuss issues and challenges that need to be further considered. It would be great valuable to integrate or explore more advanced diagnostic methods to handle collected industrial big data and put them into practice to mine the huge hidden diagnostic value.},
keywords={Big Data;condition monitoring;data analysis;fault diagnosis;production engineering computing;production management;fault diagnosis taxonomy;Big Data analysis;value-driven fault diagnosis methods;data-driven fault diagnosis;knowledge-driven fault diagnosis;industrial Big Data;Big Data processing;Fault diagnosis;Big Data;Data mining;Robustness;Signal processing;Feature extraction;Computational modeling;Fault diagnosis;industrial big data;value discovery;device electrocardiogram},
doi={10.1109/ACCESS.2017.2731945},
ISSN={2169-3536},
month={},}
@ARTICLE{8464661,
author={X. Zhang and S. Qu and J. Huang and B. Fang and P. Yu},
journal={IEEE Access},
title={Stock Market Prediction via Multi-Source Multiple Instance Learning},
year={2018},
volume={6},
number={},
pages={50720-50728},
abstract={Forecasting the stock market movements is an important and challenging task. As the Web information grows, researchers begin to extract effective indicators (e.g., the events and sentiments) from the Web to facilitate the prediction. However, the indicators obtained in previous studies are usually based on only one data source and thus may not fully cover the factors that can affect the stock market movements. In this paper, to improve the prediction for stock market composite index movements, we exploit the consistencies among different data sources, and develop a multi-source multiple instance model that can effectively combine events, sentiments, as well as the quantitative data into a comprehensive framework. To effectively capture the news events, we successfully apply a novel event extraction and representation method. Evaluations on the data from the year 2015 and 2016 demonstrate the effectiveness of our model. In addition, our approach is able to automatically determine the importance of each data source and identify the crucial input information that is considered to drive the movements, making the predictions interpretable.},
keywords={economic forecasting;financial data processing;Internet;learning (artificial intelligence);sentiment analysis;stock markets;stock market prediction;multisource multiple instance learning;Web information;sentiments;stock market composite index movements;multisource multiple instance model;quantitative data;news events;data sources;event extraction method;event representation method;Stock markets;Indexes;Predictive models;Social network services;Data models;Data mining;Forecasting;Stock prediction;multiple instance;event extraction;sentiment analysis},
doi={10.1109/ACCESS.2018.2869735},
ISSN={2169-3536},
month={},}
@ARTICLE{7350165,
author={T. Al-Khateeb and M. M. Masud and K. M. Al-Naami and S. E. Seker and A. M. Mustafa and L. Khan and Z. Trabelsi and C. Aggarwal and J. Han},
journal={IEEE Transactions on Knowledge and Data Engineering},
title={Recurring and Novel Class Detection Using Class-Based Ensemble for Evolving Data Stream},
year={2016},
volume={28},
number={10},
pages={2752-2764},
abstract={Streaming data is one of the attention receiving sources for concept-evolution studies. When a new class occurs in the data stream it can be considered as a new concept and so the concept-evolution. One attractive problem occurring in the concept-evolution studies is the recurring classes from our previous study. In data streams, a class can disappear and reappear after a while. Existing studies on data stream classification techniques either misclassify the recurring class or falsely identify the recurring classes as novel classes. Because of the misclassification or false novel classification, the error rates increases on those studies. In this paper we address the problem by defining a novel ensemble technique “class-based” ensemble which replaces the traditional “chunk-based” approach in order to detect the recurring classes. We discuss the details of two different approaches in class-based ensemble and explain and compare them in detail. Different than the previous studies in the field, we also prove the superiority of both “class-based” ensemble method over state-of-art techniques via empirical approach on a number of benchmark data sets including Web comments as text mining challenge.},
keywords={data mining;learning (artificial intelligence);pattern classification;pattern clustering;text analysis;evolving-data stream;concept-evolution;recurring class detection;data stream classification techniques;novel-class detection;error rates;novel-ensemble technique;class-based ensemble;empirical approach;benchmark data sets;Web comments;text mining;Data models;Electronic mail;Benchmark testing;Twitter;Market research;Decision trees;Error analysis;Database applications;clustering;classification;and association rules;data mining},
doi={10.1109/TKDE.2015.2507123},
ISSN={1041-4347},
month={Oct},}
@ARTICLE{7134793,
author={Y. Zhou and K. Kang},
journal={IEEE Transactions on Knowledge and Data Engineering},
title={Deadline Assignment and Feedback Control for Differentiated Real-Time Data Services},
year={2015},
volume={27},
number={12},
pages={3245-3257},
abstract={It is challenging to process real-time data service requests, such as online trade or traffic monitoring requests, within their deadlines, while providing differentiated real-time data services. To address the problem, we present new approaches to 1) assigning deadlines to real-time data service requests based on their data access needs and service classes to differentiate real-time data services when the system is busy and 2) closely supporting the specified target delay to deadline ratio (DDR)-the ratio of actual data service delays to deadlines-via feedback control even in the presence of dynamic workloads. Further, we have actually implemented our approaches by extending an open source database unlike most existing work on real-time databases. The experimental results show that our approach closely supports the target DDR bound and service differentiation requirements among the service classes unlike the tested baselines representing the current state of the art in real-time database research.},
keywords={data handling;database management systems;feedback;public domain software;deadline assignment;feedback control;differentiated real-time data services;real-time data service requests;target delay to deadline ratio;dynamic workloads;open source database;real-time databases;target DDR bound;Real-time systems;Telecommunication traffic;Databases;Feedback control;Control systems;Monitoring;Online services;Stock markets;Differentiated Real-Time Data Services;Deadline Assignment;Feedback Control;Differentiated real-time data services;deadline assignment;feedback control},
doi={10.1109/TKDE.2015.2441725},
ISSN={1041-4347},
month={Dec},}
@ARTICLE{6574839,
author={A. Hapfelmeier and B. Pfahringer and S. Kramer},
journal={IEEE Transactions on Knowledge and Data Engineering},
title={Pruning Incremental Linear Model Trees with Approximate Lookahead},
year={2014},
volume={26},
number={8},
pages={2072-2076},
abstract={Incremental linear model trees with approximate lookahead are fast, but produce overly large trees. This is due to non-optimal splitting decisions boosted by a possibly unlimited number of examples obtained from a data source. To keep the processing speed high and the tree complexity low, appropriate incremental pruning techniques are needed. In this paper, we introduce a pruning technique for the class of incremental linear model trees with approximate lookahead on stationary data sources. Experimental results show that the advantage of approximate lookahead in terms of processing speed can be further improved by producing much smaller and consequently more explanatory, less memory consuming trees on high-dimensional data. This is done at the expense of only a small increase in prediction error. Additionally, the pruning algorithm can be tuned to either produce less accurate model trees at a much higher processing speed or, alternatively, more accurate trees at the expense of higher processing times.},
keywords={learning (artificial intelligence);trees (mathematics);incremental linear model tree pruning;approximate lookahead;nonoptimal splitting decisions;data source;processing speed;tree complexity;incremental pruning techniques;prediction error;Adaptation models;Prediction algorithms;Data models;Predictive models;Runtime;Mathematical model;Complexity theory;Machine learning;Trees;Online computation;Real-time and embedded systems;Machine learning;trees;online computation;real-time and embedded systems},
doi={10.1109/TKDE.2013.132},
ISSN={1041-4347},
month={Aug},}
@ARTICLE{6880395,
author={M. Zhou and A. Mockus},
journal={IEEE Transactions on Software Engineering},
title={Who Will Stay in the FLOSS Community? Modeling Participant’s Initial Behavior},
year={2015},
volume={41},
number={1},
pages={82-99},
abstract={Motivation: To survive and succeed, FLOSS projects need contributors able to accomplish critical project tasks. However, such tasks require extensive project experience of long term contributors (LTCs). Aim: We measure, understand, and predict how the newcomers' involvement and environment in the issue tracking system (ITS) affect their odds of becoming an LTC. Method: ITS data of Mozilla and Gnome, literature, interviews, and online documents were used to design measures of involvement and environment. A logistic regression model was used to explain and predict contributor's odds of becoming an LTC. We also reproduced the results on new data provided by Mozilla. Results: We constructed nine measures of involvement and environment based on events recorded in an ITS. Macro-climate is the overall project environment while micro-climate is person-specific and varies among the participants. Newcomers who are able to get at least one issue reported in the first month to be fixed, doubled their odds of becoming an LTC. The macro-climate with high project popularity and the micro-climate with low attention from peers reduced the odds. The precision of LTC prediction was 38 times higher than for a random predictor. We were able to reproduce the results with new Mozilla data without losing the significance or predictive power of the previously published model. We encountered unexpected changes in some attributes and suggest ways to make analysis of ITS data more reproducible. Conclusions: The findings suggest the importance of initial behaviors and experiences of new participants and outline empirically-based approaches to help the communities with the recruitment of contributors for long-term participation and to help the participants contribute more effectively. To facilitate the reproduction of the study and of the proposed measures in other contexts, we provide the data we retrieved and the scripts we wrote at https://www.passion-lab.org/projects/developerfluency.html.},
keywords={behavioural sciences;project management;public domain software;Free-Libre and/or open source software projects;open source software;Mozilla data;microclimate;macroclimate;logistic regression model;Gnome;ITS data;issue tracking system;LTC;long term contributors;critical project;FLOSS community;Communities;Atmospheric measurements;Particle measurements;Predictive models;Data models;Data mining;Electronic mail;Long term contributor;open source software;issue tracking system;mining software repository;extent of involvement;interaction with environment;initial behavior},
doi={10.1109/TSE.2014.2349496},
ISSN={0098-5589},
month={Jan},}
@ARTICLE{7948800,
author={X. Wu and H. Chen and J. Liu and G. Wu and R. Lu and N. Zheng},
journal={IEEE Access},
title={Knowledge Engineering With Big Data (BigKE): A 54-Month, 45-Million RMB, 15-Institution National Grand Project},
year={2017},
volume={5},
number={},
pages={12696-12701},
abstract={Starting in July 2016, the Ministry of Science and Technology of China, along with several other national agencies, sponsors a 54-month 45-million RMB (Chinese Yuan) project on knowledge engineering with Big Data (www.bigke.org) for 15 top research and development institutions to study the fundamental theory and the applications of BigKE, a big-data knowledge engineering framework that handles fragmented knowledge modeling and online learning from multiple information sources, nonlinear fusion on fragmented knowledge, and automated demand-driven knowledge navigation. The project seeks to provide petabytescale data and knowledge services in identified application domains. In this paper, we discuss our BigKE framework, and present a novel application scenario for BigKE services.},
keywords={Big Data;data mining;knowledge engineering;learning (artificial intelligence);research and development;sensor fusion;BigKE services;knowledge services;petabyte-scale data;automated demand-driven knowledge navigation;nonlinear fusion;online learning;fragmented knowledge modeling;big-data knowledge engineering framework;research and development institutions;www.bigke.org;China;Ministry of Science and Technology;Knowledge engineering;Big Data;Data models;Navigation;Reliability;Electronic mail;Real-time systems;Knowledge engineering;data mining},
doi={10.1109/ACCESS.2017.2710298},
ISSN={2169-3536},
month={},}
@ARTICLE{8360430,
author={M. G. Kibria and K. Nguyen and G. P. Villardi and O. Zhao and K. Ishizu and F. Kojima},
journal={IEEE Access},
title={Big Data Analytics, Machine Learning, and Artificial Intelligence in Next-Generation Wireless Networks},
year={2018},
volume={6},
number={},
pages={32328-32338},
abstract={The next-generation wireless networks are evolving into very complex systems because of the very diversified service requirements, heterogeneity in applications, devices, and networks. The network operators need to make the best use of the available resources, for example, power, spectrum, as well as infrastructures. Traditional networking approaches, i.e., reactive, centrally-managed, one-size-fits-all approaches, and conventional data analysis tools that have limited capability (space and time) are not competent anymore and cannot satisfy and serve that future complex networks regarding operation and optimization cost effectively. A novel paradigm of proactive, self-aware, self-adaptive, and predictive networking is much needed. The network operators have access to large amounts of data, especially from the network and the subscribers. Systematic exploitation of the big data dramatically helps in making the system smart, intelligent, and facilitates efficient as well as cost-effective operation and optimization. We envision data-driven next-generation wireless networks, where the network operators employ advanced data analytics, machine learning (ML), and artificial intelligence. We discuss the data sources and strong drivers for the adoption of the data analytics, and the role of ML, artificial intelligence in making the system intelligent regarding being self-aware, self-adaptive, proactive and prescriptive. A set of network design and optimization schemes are presented concerning data analytics. This paper concludes with a discussion of challenges and the benefits of adopting big data analytics, ML, and artificial intelligence in the next-generation communication systems.},
keywords={Big Data;data analysis;learning (artificial intelligence);next generation networks;optimisation;radio networks;telecommunication computing;machine learning;artificial intelligence;complex systems;network operators;future complex networks;optimization cost;predictive networking;cost-effective operation;data sources;network design;optimization schemes;next-generation communication systems;Big Data analytics;proactive networking;self-aware networking;self-adaptive networking;data driven next-generation wireless networks;Big Data;Next generation networking;Optimization;Machine learning;Data analysis;Tools;Wireless communication;Big data analytics;machine learning;artificial intelligence;next-generation wireless},
doi={10.1109/ACCESS.2018.2837692},
ISSN={2169-3536},
month={},}
@ARTICLE{8362907,
author={R. Dautov and S. Distefano and D. Bruneo and F. Longo and G. Merlino and A. Puliafito},
journal={IEEE Access},
title={Data Processing in Cyber-Physical-Social Systems Through Edge Computing},
year={2018},
volume={6},
number={},
pages={29822-29835},
abstract={Cloud and Fog computing have established a convenient and widely adopted approach for computation offloading, where raw data generated by edge devices in the Internet of Things (IoT) context is collected and processed remotely. This vertical offloading pattern, however, typically does not take into account increasingly pressing time constraints of the emerging IoT scenarios, in which numerous data sources, including human agents (i.e., Social IoT), continuously generate large amounts of data to be processed in a timely manner. Big data solutions could be applied in this respect, provided that networking issues and limitations related to connectivity of edge devices are properly addressed. Although edge devices are traditionally considered to be resource-constrained, main limitations refer to energy, networking, and memory capacities, whereas their ever-growing processing capabilities are already sufficient to be effectively involved in actual (big data) processing. In this context, the role of human agents is no longer limited to passive data generation, but can also include their voluntary involvement in relatively complex computations. This way, users can share their personal computational resources (i.e., mobile phones) to support collaborative data processing, thereby turning the existing IoT into a global cyber-physical-social system (CPSS). To this extent, this paper proposes a novel IoT/CPSS data processing pattern based on the stream processing technology, aiming to distribute the workload among a cluster of edge devices, involving mobile nodes shared by contributors on a voluntary basis, and paving the way for cluster computing at the edge. Experiments on an intelligent surveillance system deployed on an edge device cluster demonstrate the feasibility of the proposed approach, illustrating how its distributed in-memory data processing architecture can be effective.},
keywords={Big Data;cloud computing;data analysis;Internet;Internet of Things;mobile computing;edge computing;computation offloading;raw data;vertical offloading pattern;human agents;Social IoT;big data solutions;processing capabilities;passive data generation;relatively complex computations;personal computational resources;collaborative data processing;global cyber-physical-social system;cluster computing;edge device cluster;in-memory data;Internet of Things context;data sources;CPSS data processing pattern;Streaming media;Big Data;Cloud computing;Servers;Cameras;Edge computing;Internet of Things;Internet of People;cyber-physical-social system;edge computing;big data;stream processing;horizontal and vertical offloading;Apache NiFi},
doi={10.1109/ACCESS.2018.2839915},
ISSN={2169-3536},
month={},}
@ARTICLE{6081868,
author={L. Cagliero},
journal={IEEE Transactions on Knowledge and Data Engineering},
title={Discovering Temporal Change Patterns in the Presence of Taxonomies},
year={2013},
volume={25},
number={3},
pages={541-555},
abstract={Frequent itemset mining is a widely exploratory technique that focuses on discovering recurrent correlations among data. The steadfast evolution of markets and business environments prompts the need of data mining algorithms to discover significant correlation changes in order to reactively suit product and service provision to customer needs. Change mining, in the context of frequent itemsets, focuses on detecting and reporting significant changes in the set of mined itemsets from one time period to another. The discovery of frequent generalized itemsets, i.e., itemsets that 1) frequently occur in the source data, and 2) provide a high-level abstraction of the mined knowledge, issues new challenges in the analysis of itemsets that become rare, and thus are no longer extracted, from a certain point. This paper proposes a novel kind of dynamic pattern, namely the History GENeralized Pattern (HIGEN), that represents the evolution of an itemset in consecutive time periods, by reporting the information about its frequent generalizations characterized by minimal redundancy (i.e., minimum level of abstraction) in case it becomes infrequent in a certain time period. To address HIGEN mining, it proposes HIGEN MINER, an algorithm that focuses on avoiding itemset mining followed by postprocessing by exploiting a support-driven itemset generalization approach. To focus the attention on the minimally redundant frequent generalizations and thus reduce the amount of the generated patterns, the discovery of a smart subset of HIGENs, namely the NONREDUNDANT HIGENs, is addressed as well. Experiments performed on both real and synthetic datasets show the efficiency and the effectiveness of the proposed approach as well as its usefulness in a real application context.},
keywords={business data processing;data mining;marketing data processing;pattern recognition;discovering temporal change patterns;taxonomies presence;frequent itemset mining;business environments;market environments;data mining algorithms;source data;history generalized pattern;HIGEN;Itemsets;Data mining;Taxonomy;Context awareness;Information retrieval;Search methods;Data mining;mining methods and algorithms},
doi={10.1109/TKDE.2011.233},
ISSN={1041-4347},
month={March},}
@ARTICLE{5680907,
author={H. Ma and I. King and M. R. Lyu},
journal={IEEE Transactions on Knowledge and Data Engineering},
title={Mining Web Graphs for Recommendations},
year={2012},
volume={24},
number={6},
pages={1051-1064},
abstract={As the exponential explosion of various contents generated on the Web, Recommendation techniques have become increasingly indispensable. Innumerable different kinds of recommendations are made on the Web every day, including movies, music, images, books recommendations, query suggestions, tags recommendations, etc. No matter what types of data sources are used for the recommendations, essentially these data sources can be modeled in the form of various types of graphs. In this paper, aiming at providing a general framework on mining Web graphs for recommendations, (1) we first propose a novel diffusion method which propagates similarities between different nodes and generates recommendations; (2) then we illustrate how to generalize different recommendation problems into our graph diffusion framework. The proposed framework can be utilized in many recommendation tasks on the World Wide Web, including query suggestions, tag recommendations, expert finding, image recommendations, image annotations, etc. The experimental analysis on large data sets shows the promising future of our work.},
keywords={data mining;graph theory;Internet;query processing;recommender systems;Web graph mining;exponential explosion;data source;diffusion method;recommendation problem;graph diffusion framework;World Wide Web;data sets;query suggestion;tag recommendation technique;expert finding;image recommendation task;image annotation;Heating;Collaboration;Heat engines;Algorithm design and analysis;Computational modeling;Data mining;Data models;Recommendation;diffusion;query suggestion;image recommendation.},
doi={10.1109/TKDE.2011.18},
ISSN={1041-4347},
month={June},}
@ARTICLE{8438866,
author={D. Hwang and S. Choi and J. Shin and G. Song and Y. Choi},
journal={IEEE Access},
title={Privacy-Preserving Compressed Reference-Oriented Alignment Map Using Decentralized Storage},
year={2018},
volume={6},
number={},
pages={45990-46001},
abstract={In bioinformatics, researchers have endeavored to resolve the following two issues: 1) how to increase the efficiency of storage through compression and 2) how to provide confidentiality for the genome sequence data. To resolve two issues, the sequence alignment map, the binary alignment map, the compressed reference-oriented alignment map (CRAM), and the selective retrieval on encrypted and CRAM formats were proposed. However, since these formats are stored in a centralized storage that is managed by the genome testing organizations, the privacy of sensitive genome sequence data is not guaranteed. In this paper, we propose a new compressed reference-oriented alignment map, called decentralized storage and compressed reference-oriented alignment map (D-RAM), which preserves the privacy of genome sequence data using a decentralized storage architecture. The proposed D-RAM format uses the reference-based compression and bzip2 compression to use storage space efficiently. In addition, to preserve the privacy of genome sequence data, the proposed decentralized storage architecture is designed to store the private genome sequence data and the public genome sequence data separately. From the experimental results under simulation and real genome sequence data, we show that the D-RAM format saves the size of the genome sequence data than other formats. By analyzing the computational complexity with which the attacker recovers the genome sequence data, we also show the theoretical analysis that explains why the D-RAM format is safer than the other formats.},
keywords={bioinformatics;computational complexity;cryptography;data compression;data privacy;DRAM chips;genomics;medical computing;decentralized storage architecture;D-RAM format;reference-based compression;bzip2 compression;private genome sequence data;public genome sequence data;genome testing organization;sensitive genome sequence data;binary alignment map;sequence alignment map;privacy-preserving compressed reference-oriented alignment map;Bioinformatics;Genomics;Data privacy;Cryptography;Computer architecture;DNA;Sequential analysis;Data compression;decentralized storage;genome sequence data;privacy-preserving},
doi={10.1109/ACCESS.2018.2865994},
ISSN={2169-3536},
month={},}
@ARTICLE{7562451,
author={W. Shi and Y. Zhu and P. S. Yu and T. Huang and C. Wang and Y. Mao and Y. Chen},
journal={IEEE Access},
title={Temporal Dynamic Matrix Factorization for Missing Data Prediction in Large Scale Coevolving Time Series},
year={2016},
volume={4},
number={},
pages={6719-6732},
abstract={Data missing in collections of time series occurs frequently in practical applications and turns out to be a major menace to precise data analysis. However, most of the existing methods either might be infeasible or could be inefficient to predict the missing values in large-scale coevolving time series. Also, the evolving of time series needs to be handled properly to adapt to the temporal characteristic. Furthermore, more massive volume of data is generated in many areas than ever before. In this paper, we have taken up the challenge of missing data prediction in coevolving time series by employing temporal dynamic matrix factorization techniques. First, our approaches are optimally designed to largely utilize both the interior patterns of each time series and the information of time series across multiple sources to build an initial model. Based on the idea, we have imposed hybrid regularization terms to constrain the objective functions of matrix factorization. Then, temporal dynamic matrix factorization is proposed to effectively update the initial already trained models. In the process of dynamic matrix factorization, batch updating and fine-tuning strategies are also employed to build an effective and efficient model. Extensive experiments on real-world data sets and synthetic data set demonstrate that the proposed approaches can effectively improve the performance of missing data prediction. Even when the missing ratio reaches as high as 90%, our proposed methods still show low prediction errors. Dynamic performance demonstrates that the methods can obtain satisfactory effectiveness and efficiency. Furthermore, we have also demonstrated how to take advantage of the high processing power of Apache Spark to perform missing data prediction in large-scale coevolving time series.},
keywords={data analysis;matrix decomposition;performance evaluation;time series;temporal dynamic matrix factorization;missing data prediction;large scale coevolving time series;data analysis;missing values prediction;temporal characteristics;hybrid regularization terms;batch updating strategies;fine-tuning strategies;performance improvement;improve prediction errors;Apache Spark;Time series analysis;Predictive models;Data models;Big data;Matrix factorization;Linear programming;Matrix factorization;missing data prediction;time series;Apache Spark},
doi={10.1109/ACCESS.2016.2606242},
ISSN={2169-3536},
month={},}
@ARTICLE{7738441,
author={M. S. Hossain and G. Muhammad},
journal={IEEE Access},
title={Healthcare Big Data Voice Pathology Assessment Framework},
year={2016},
volume={4},
number={},
pages={7806-7815},
abstract={The fast-growing healthcare big data plays an important role in healthcare service providing. Healthcare big data comprise data from different structured, semi-structured, and unstructured sources. These data sources vary in terms of heterogeneity, volume, variety, velocity, and value that traditional frameworks, algorithms, tools, and techniques are not fully capable of handling. Therefore, a framework is required that facilitates collection, extraction, storage, classification, processing, and modeling of this vast heterogeneous volume of data. This paper proposes a healthcare big data framework using voice pathology assessment (VPA) as a case study. In the proposed VPA system, two robust features, MPEG-7 low-level audio and the interlaced derivative pattern, are used for processing the voice or speech signals. The machine learning algorithms in the form of a support vector machine, an extreme learning machine, and a Gaussian mixture model are used as the classifier. In the experiments, the proposed VPA system shows its efficiency in terms of accuracy and time requirement.},
keywords={audio signal processing;Big Data;feature extraction;Gaussian processes;health care;learning (artificial intelligence);mixture models;pattern classification;speech processing;support vector machines;Gaussian mixture model;extreme learning machine;support vector machine;machine learning algorithm;interlaced derivative pattern;MPEG-7 low-level audio;VPA system;healthcare Big Data voice pathology assessment framework;Medical services;Big data;Feature extraction;Pathology;Data mining;Biomedical monitoring;Machine learning algorithms;Classification;Healthcare big data;voice pathology;classification;feature extraction},
doi={10.1109/ACCESS.2016.2626316},
ISSN={2169-3536},
month={},}
@ARTICLE{8101464,
author={Y. Li and Z. Huang and Y. Wang and B. Fang},
journal={IEEE Access},
title={Evaluating Data Filter on Cross-Project Defect Prediction: Comparison and Improvements},
year={2017},
volume={5},
number={},
pages={25646-25656},
abstract={Cross-project defect prediction (CPDP) is a field of study where a software project lacking enough local data can use data from other projects to build defect predictors. To support CPDP, the cross-project data must be carefully filtered before being applied locally. Researchers have devised and implemented a plethora of various data filters for the improvement of CPDP performance. However, it is still unclear what data filter strategy is most effective, both generally and specifically, in CPDP. The objective of this paper is to provide an extensive comparison of well-known data filters and a novel filter devised in this paper. We perform experiments on 44 releases of 14 open-source projects, and use Naive Bayes and a support vector machine as the underlying classifier. The results demonstrate that the data filter strategy improves the performance of cross-project defect prediction significantly, and the hierarchical select-based filter proposed performs significantly better. Moreover, when using appropriate data filter strategy, the defect predictor built from cross-project data can outperform the predictor learned by using within-project data.},
keywords={Bayes methods;learning (artificial intelligence);pattern classification;project management;software metrics;software quality;support vector machines;cross-project defect prediction;software project;local data;defect predictor;cross-project data;CPDP performance;within-project data;data filter strategy;hierarchical select-based filter;classifier;support vector machine;Naive Bayes method;open-source projects;Software;Training data;Data models;Predictive models;Filtering algorithms;Information filters;Software quality assurance;cross-project defect prediction;data filter;machine learning},
doi={10.1109/ACCESS.2017.2771460},
ISSN={2169-3536},
month={},}
@ARTICLE{6341764,
author={N. Ali and Y. Guéhéneuc and G. Antoniol},
journal={IEEE Transactions on Software Engineering},
title={Trustrace: Mining Software Repositories to Improve the Accuracy of Requirement Traceability Links},
year={2013},
volume={39},
number={5},
pages={725-741},
abstract={Traceability is the only means to ensure that the source code of a system is consistent with its requirements and that all and only the specified requirements have been implemented by developers. During software maintenance and evolution, requirement traceability links become obsolete because developers do not/cannot devote effort to updating them. Yet, recovering these traceability links later is a daunting and costly task for developers. Consequently, the literature has proposed methods, techniques, and tools to recover these traceability links semi-automatically or automatically. Among the proposed techniques, the literature showed that information retrieval (IR) techniques can automatically recover traceability links between free-text requirements and source code. However, IR techniques lack accuracy (precision and recall). In this paper, we show that mining software repositories and combining mined results with IR techniques can improve the accuracy (precision and recall) of IR techniques and we propose Trustrace, a trust--based traceability recovery approach. We apply Trustrace on four medium-size open-source systems to compare the accuracy of its traceability links with those recovered using state-of-the-art IR techniques from the literature, based on the Vector Space Model and Jensen-Shannon model. The results of Trustrace are up to 22.7 percent more precise and have 7.66 percent better recall values than those of the other techniques, on average. We thus show that mining software repositories and combining the mined data with existing results from IR techniques improves the precision and recall of requirement traceability links.},
keywords={data mining;data privacy;information retrieval;software maintenance;software repository mining;requirement traceability link;traceability method;software maintenance;software evolution;information retrieval technique;IR technique;precision accuracy;recall accuracy;Trustrace approach;trust-based traceability recovery approach;medium-size open-source system;vector space model;Jensen-Shannon model;Accuracy;Data mining;Software maintenance;Information retrieval;Open source software;Principal component analysis;Traceability;requirements;feature;source code;repositories;experts;trust-based model},
doi={10.1109/TSE.2012.71},
ISSN={0098-5589},
month={May},}
@ARTICLE{6572787,
author={T. Berger and S. She and R. Lotufo and A. Wasowski and K. Czarnecki},
journal={IEEE Transactions on Software Engineering},
title={A Study of Variability Models and Languages in the Systems Software Domain},
year={2013},
volume={39},
number={12},
pages={1611-1640},
abstract={Variability models represent the common and variable features of products in a product line. Since the introduction of FODA in 1990, several variability modeling languages have been proposed in academia and industry, followed by hundreds of research papers on variability models and modeling. However, little is known about the practical use of such languages. We study the constructs, semantics, usage, and associated tools of two variability modeling languages, Kconfig and CDL, which are independently developed outside academia and used in large and significant software projects. We analyze 128 variability models found in 12 open--source projects using these languages. Our study 1) supports variability modeling research with empirical data on the real-world use of its flagship concepts. However, we 2) also provide requirements for concepts and mechanisms that are not commonly considered in academic techniques, and 3) challenge assumptions about size and complexity of variability models made in academic papers. These results are of interest to researchers working on variability modeling and analysis techniques and to designers of tools, such as feature dependency checkers and interactive product configurators.},
keywords={public domain software;simulation languages;software engineering;interactive product configurators;feature dependency checkers;variability analysis techniques;open-source projects;software projects;associated language tools;language usage;language semantics;language constructs;CDL language;Kconfig language;variability modeling languages;FODA;systems software domain;variability models;Biological system modeling;Software products;Product line;Analytical models;Computational modeling;Semantics;Computer architecture;Empirical software engineering;software product lines;variability modeling;feature modeling;configuration;open source},
doi={10.1109/TSE.2013.34},
ISSN={0098-5589},
month={Dec},}
@ARTICLE{7109102,
author={K. Telegenov and Y. Tlegenov and A. Shintemirov},
journal={IEEE Access},
title={A Low-Cost Open-Source 3-D-Printed Three-Finger Gripper Platform for Research and Educational Purposes},
year={2015},
volume={3},
number={},
pages={638-647},
abstract={Robotics research and education have gained significant attention in recent years due to increased development and commercial deployment of industrial and service robots. A majority of researchers working on robot grasping and object manipulation tend to utilize commercially available robot-manipulators equipped with various end effectors for experimental studies. However, commercially available robotic grippers are often expensive and are not easy to modify for specific purposes. To extend the choice of robotic end effectors freely available to researchers and educators, we present an open-source low-cost three-finger robotic gripper platform for research and educational purposes. The 3-D design model of the gripper is presented and manufactured with a minimal number of 3-D-printed components and an off-the-shelf servo actuator. An underactuated finger and gear train mechanism, with an overall gripper assembly design, are described in detail, followed by illustrations and a discussion of the gripper grasping performance and possible gripper platform modifications. The presented open-source gripper platform computer-aided design model is released for downloading on the authors research lab website (&lt;;uri xlink:href="http://www.alaris.kz" xlink:type="simple"&gt;www.alaris.kz&lt;;/uri&gt;) and can be utilized by robotics researchers and educators as a design platform to build their own robotic end effector solutions for research and educational purposes.},
keywords={CAD;control engineering computing;design engineering;educational robots;end effectors;gears;grippers;public domain software;servomechanisms;three-dimensional printing;low-cost open-source 3-D-printed three-finger gripper platform;educational purposes;research purposes;industrial robots;service robots;robot grasping;object manipulation;commercially available robot-manipulators;commercially available robotic grippers;robotic end effectors;3-D design model;3-D-printed components;off-the-shelf servo actuator;underactuated finger;gear train mechanism;gripper assembly design;open-source gripper platform computer-aided design model;research lab Website;Robots;Three-dimensional displays;Printed circuits;Grippers;Education;End effectors;Open source software;Grasping;open-source robotic gripper;gear train mechanism;3D printing;underactuation;adaptive grasping;Open-source robotic gripper;gear train mechanism;3D printing;underactuation;adaptive grasping},
doi={10.1109/ACCESS.2015.2433937},
ISSN={2169-3536},
month={},}
@ARTICLE{7843666,
author={J. Fowkes and P. Chanthirasegaran and R. Ranca and M. Allamanis and M. Lapata and C. Sutton},
journal={IEEE Transactions on Software Engineering},
title={Autofolding for Source Code Summarization},
year={2017},
volume={43},
number={12},
pages={1095-1109},
abstract={Developers spend much of their time reading and browsing source code, raising new opportunities for summarization methods. Indeed, modern code editors provide code folding, which allows one to selectively hide blocks of code. However this is impractical to use as folding decisions must be made manually or based on simple rules. We introduce the autofolding problem, which is to automatically create a code summary by folding less informative code regions. We present a novel solution by formulating the problem as a sequence of AST folding decisions, leveraging a scoped topic model for code tokens. On an annotated set of popular open source projects, we show that our summarizer outperforms simpler baselines, yielding a 28 percent error reduction. Furthermore, we find through a case study that our summarizer is strongly preferred by experienced developers. More broadly, we hope this work will aid program comprehension by turning code folding into a usable and valuable tool.},
keywords={public domain software;source code (software);source code summarization;modern code editors;code folding;autofolding problem;informative code regions;AST folding decisions;code tokens;open source projects;Software development;Natural languages;Source coding;Feature extraction;Complexity theory;Source code summarization, program comprehension, topic modelling},
doi={10.1109/TSE.2017.2664836},
ISSN={0098-5589},
month={Dec},}
@ARTICLE{7539677,
author={F. Zhang and A. E. Hassan and S. McIntosh and Y. Zou},
journal={IEEE Transactions on Software Engineering},
title={The Use of Summation to Aggregate Software Metrics Hinders the Performance of Defect Prediction Models},
year={2017},
volume={43},
number={5},
pages={476-491},
abstract={Defect prediction models help software organizations to anticipate where defects will appear in the future. When training a defect prediction model, historical defect data is often mined from a Version Control System (VCS, e.g., Subversion), which records software changes at the file-level. Software metrics, on the other hand, are often calculated at the class- or method-level (e.g., McCabe's Cyclomatic Complexity). To address the disagreement in granularity, the class- and method-level software metrics are aggregated to file-level, often using summation (i.e., McCabe of a file is the sum of the McCabe of all methods within the file). A recent study shows that summation significantly inflates the correlation between lines of code (Sloc) and cyclomatic complexity (Cc) in Java projects. While there are many other aggregation schemes (e.g., central tendency, dispersion), they have remained unexplored in the scope of defect prediction. In this study, we set out to investigate how different aggregation schemes impact defect prediction models. Through an analysis of 11 aggregation schemes using data collected from 255 open source projects, we find that: (1) aggregation schemes can significantly alter correlations among metrics, as well as the correlations between metrics and the defect count; (2) when constructing models to predict defect proneness, applying only the summation scheme (i.e., the most commonly used aggregation scheme in the literature) only achieves the best performance (the best among the 12 studied configurations) in 11 percent of the studied projects, while applying all of the studied aggregation schemes achieves the best performance in 40 percent of the studied projects; (3) when constructing models to predict defect rank or count, either applying only the summation or applying all of the studied aggregation schemes achieves similar performance, with both achieving the closest to the best performance more often than the other studied aggregation schemes; and (4) when constructing models for effort-aware defect prediction, the mean or median aggregation schemes yield performance values that are significantly closer to the best performance than any of the other studied aggregation schemes. Broadly speaking, the performance of defect prediction models are often underestimated due to our community's tendency to only use the summation aggregation scheme. Given the potential benefit of applying additional aggregation schemes, we advise that future defect prediction models should explore a variety of aggregation schemes.},
keywords={data aggregation;data mining;Java;public domain software;software metrics;software metrics aggregation;defect prediction models;software organizations;historical defect data mining;version control system;software changes recording;McCabe cyclomatic complexity;granularity disagreement;class-level software metrics;method-level software metrics;lines of code;Sloc;Cc;Java projects;open source projects;effort-aware defect prediction;summation;Predictive models;Correlation;Software metrics;Indexes;Software;Data models;Defect prediction;aggregation scheme;software metrics},
doi={10.1109/TSE.2016.2599161},
ISSN={0098-5589},
month={May},}
@ARTICLE{8463489,
author={Y. Zhang and B. Chen and Y. Zhao and G. Pan},
journal={IEEE Access},
title={Wind Speed Prediction of IPSO-BP Neural Network Based on Lorenz Disturbance},
year={2018},
volume={6},
number={},
pages={53168-53179},
abstract={Accurate wind power prediction provides significant guarantee for power grid dispatching, and wind speed prediction, as the basic link of wind power forecasting, has crucial theoretical research significance and practical application value. In this paper, we present the wind speed prediction of IPSOBP neural network based on Lorenz disturbance. At first, the data is processed by principal component analysis (PCA) to select the key factors affecting wind speed, which can effectively reduce the complexity of model. Then, the improved particle swarm optimization (IPSO) algorithm is used to globally optimize the weights and thresholds of BP neural network, and overcome the problem of local minimum value. The initial prediction results can be obtained by the IPSO-BPNN model. Finally, Lorenz system is introduced to correct the initial prediction value and improve forecasting accuracy. According to the wind farm data of Spain and Chang Ma in China, we take an empirical research to analyze the optimization effect of IPSO algorithm and the promotion effect of Lorenz system on the precision of preliminary forecasting. The results are as follows: 1) IPSO algorithm accelerates the convergence rate of weights and thresholds of BP neural network and 2) Lorenz disturbance system obviously weakens the random volatility of wind speed, effectively modifies its preliminary prediction results, and upgrades its prediction accuracy.},
keywords={backpropagation;data analysis;neural nets;particle swarm optimisation;power engineering computing;power generation dispatch;power grids;principal component analysis;wind power plants;random volatility;wind power forecasting;power grid dispatching;accurate wind power prediction;IPSO-BP neural network;prediction accuracy;wind farm data;initial prediction value;Lorenz system;improved particle swarm optimization algorithm;Lorenz disturbance;IPSOBP neural network;wind speed prediction;Wind speed;Neural networks;Prediction algorithms;Wind power generation;Forecasting;Convergence;Principal component analysis;BP neural network;IPSO;Lorenz system;PCA;short-term wind prediction},
doi={10.1109/ACCESS.2018.2869981},
ISSN={2169-3536},
month={},}
@ARTICLE{6891380,
author={C. Lin and Y. Li},
journal={IEEE Transactions on Software Engineering},
title={Rate-Based Queueing Simulation Model of Open Source Software Debugging Activities},
year={2014},
volume={40},
number={11},
pages={1075-1099},
abstract={Open source software (OSS) approach has become increasingly prevalent for software development. As the widespread utilization of OSS, the reliability of OSS products becomes an important issue. By simulating the testing and debugging processes of software life cycle, the rate-based queueing simulation model has shown its feasibility for closed source software (CSS) reliability assessment. However, the debugging activities of OSS projects are different in many ways from those of CSS projects and thus the simulation approach needs to be calibrated for OSS projects. In this paper, we first characterize the debugging activities of OSS projects. Based on this, we propose a new rate-based queueing simulation framework for OSS reliability assessment including the model and the procedures. Then a decision model is developed to determine the optimal version-updating time with respect to two objectives: minimizing the time for version update, and maximizing OSS reliability. To illustrate the proposed framework, three real datasets from Apache and GNOME projects are used. The empirical results indicate that our framework is able to effectively approximate the real scenarios. Moreover, the influences of the core contributor staffing levels are analyzed and the optimal version-updating times are obtained.},
keywords={configuration management;program debugging;program testing;project management;public domain software;queueing theory;software reliability;open source software;OSS approach;software development;OSS products reliability;testing processes;debugging processes;software life cycle;rate-based queueing simulation model;closed source software;CSS reliability assessment;debugging activities;OSS projects;decision model;optimal version-updating time;Software reliability;Debugging;Software;Stochastic processes;Analytical models;Cascading style sheets;Queueing theory;rate-based simulation;open source software (OSS);bug reporting;report judgment;bug fixing;optimal version-updating time;non-homogeneous continuous time Markov chain (NHCTMC);multi-attribute utility theory (MAUT)},
doi={10.1109/TSE.2014.2354032},
ISSN={0098-5589},
month={Nov},}
@ARTICLE{7593335,
author={V. Huddar and B. K. Desiraju and V. Rajan and S. Bhattacharya and S. Roy and C. K. Reddy},
journal={IEEE Access},
title={Predicting Complications in Critical Care Using Heterogeneous Clinical Data},
year={2016},
volume={4},
number={},
pages={7988-8001},
abstract={Patients in hospitals, particularly in critical care, are susceptible to many complications affecting morbidity and mortality. Digitized clinical data in electronic medical records can be effectively used to develop machine learning models to identify patients at risk of complications early and provide prioritized care to prevent complications. However, clinical data from heterogeneous sources within hospitals pose significant modeling challenges. In particular, unstructured clinical notes are a valuable source of information containing regular assessments of the patient's condition but contain inconsistent abbreviations and lack the structure of formal documents. Our contributions in this paper are twofold. First, we present a new preprocessing technique for extracting features from informal clinical notes that can be used in a classification model to identify patients at risk of developing complications. Second, we explore the use of collective matrix factorization, a multi-view learning technique, to model heterogeneous clinical data-text-based features in combination with other measurements, such as clinical investigations, comorbidites, and demographic data. We present a detailed case study on postoperative respiratory failure using more than 700 patient records from the MIMIC II database. Our experiments demonstrate the efficacy of our preprocessing technique in extracting discriminatory features from clinical notes as well as the benefits of multi-view learning to combine clinical measurements with text data for predicting complications.},
keywords={electronic health records;learning (artificial intelligence);medical computing;patient care;MIMIC II database;postoperative respiratory failure;text-based features;multi-view learning technique;collective matrix factorization;machine learning models;electronic medical records;heterogeneous clinical data;Clinical diagnosis;Heterogeneous networks;Medical services;Feature extraction;Data models;Electronic medical records;Hospitals;MIMICs;Data mining;Respiratory failure;Clinical notes;topic models;heterogeneous data;multi–view learning;collective matrix factorization;postoperative respiratory failure},
doi={10.1109/ACCESS.2016.2618775},
ISSN={2169-3536},
month={},}
@ARTICLE{8246497,
author={A. Bai and P. S. Deshpande and M. Dhabu},
journal={IEEE Access},
title={Selective Database Projections Based Approach for Mining High-Utility Itemsets},
year={2018},
volume={6},
number={},
pages={14389-14409},
abstract={High-utility itemset mining (HilIM) is an emerging area of data mining and is widely used. HilIM differs from the frequent itemset mining (FIM), as the latter considers only the frequency factor, whereas the former has been designed to address both quantity and profit factors to reveal the most profitable products. The challenges of generating the HilI include exponential complexity in both time and space. Moreover, the pruning techniques of reducing the search space, which is available in FIM because of their monotonic and anti-monotonic properties, cannot be used in HilIM. In this paper, we propose a novel selective database projection-based HilI mining algorithm (SPHilI-Miner). We introduce an efficient data format, named HilI-RTPL, which is an optimum and compact representation of data requiring low memory. We also propose two novel data structures, viz, selective database projection utility list and Tail-Count list to prune the search space for HilI mining. Selective projections of the database reduce the scanning time of the database making our proposed approach more efficient. It creates unique data instances and new projections for data having less dimensions thereby resulting in faster HilI mining. We also prove upper bounds on the amount of memory consumed by these projections. Experimental comparisons on various benchmark data sets show that the SPHilI-Miner algorithm outperforms the state-of-the-art algorithms in terms of computation time, memory usage, scalability, and candidates generation.},
keywords={data mining;data structures;database management systems;high-utility itemset mining;data mining;frequent itemset mining;FIM;frequency factor;profit factors;profitable products;exponential complexity;anti-monotonic properties;novel selective database projection;HilI mining algorithm;named HilI-RTPL;data structures;selective database projection utility list;Tail-Count list;unique data instances;faster HilI mining;benchmark data sets;SPHilI-Miner algorithm;HilIM;selective database projection-based approach;Itemsets;Data mining;Algorithm design and analysis;Memory management;Upper bound;Data structures;Data mining;high-utility itemsets;projections;SPU-List;transaction weighted utility model},
doi={10.1109/ACCESS.2017.2788083},
ISSN={2169-3536},
month={},}
@ARTICLE{7676331,
author={X. Zhan and Y. Zheng and X. Yi and S. V. Ukkusuri},
journal={IEEE Transactions on Knowledge and Data Engineering},
title={Citywide Traffic Volume Estimation Using Trajectory Data},
year={2017},
volume={29},
number={2},
pages={272-285},
abstract={Traffic volume estimation at the city scale is an important problem useful to many transportation operations and urban applications. This paper proposes a hybrid framework that integrates both state-of-art machine learning techniques and well-established traffic flow theory to estimate citywide traffic volume. In addition to typical urban context features extracted from multiple sources, we extract a special set of features from GPS trajectories based on the implications of traffic flow theory, which provide extra information on the speed-flow relationship. Using the network-wide speed information estimated from a travel speed estimation model, a volume related high level feature is first learned using an unsupervised graphical model. A volume re-interpretation model is then introduced to map the volume related high level feature to the predicted volume using a small amount of ground truth data for training. The framework is evaluated using a GPS trajectory dataset from 33,000 Beijing taxis and volume ground truth data obtained from 4,980 video clips. The results demonstrate effectiveness and potential of the proposed framework in citywide traffic volume estimation.},
keywords={computer graphics;Global Positioning System;learning (artificial intelligence);traffic information systems;citywide traffic volume estimation;trajectory data;transportation operations;urban applications;machine learning techniques;traffic flow theory;urban context features;GPS trajectories;speed-flow relationship;network-wide speed information;travel speed estimation model;volume related high level feature;unsupervised graphical model;volume reinterpretation model;ground truth data;GPS trajectory dataset;Beijing taxis;volume ground truth data;Roads;Volume measurement;Feature extraction;Trajectory;Global Positioning System;Vehicles;Solid modeling;Urban computing;traffic volume estimation;trajectories;traffic flow theory},
doi={10.1109/TKDE.2016.2621104},
ISSN={1041-4347},
month={Feb},}
@ARTICLE{8240886,
author={A. M. Elmisery and M. Sertovic and B. B. Gupta},
journal={IEEE Access},
title={Cognitive Privacy Middleware for Deep Learning Mashup in Environmental IoT},
year={2018},
volume={6},
number={},
pages={8029-8041},
abstract={Data mashup is a Web technology that combines information from multiple sources into a single Web application. Mashup applications support new services, such as environmental monitoring. The different organizations utilize data mashup services to merge data sets from the different Internet of Multimedia Things (IoMT) context-based services in order to leverage the performance of their data analytics. However, mashup, different data sets from multiple sources, is a privacy hazard as it might reveal citizens specific behaviors in different regions. In this paper, we present our efforts to build a cognitive-based middleware for private data mashup (CMPM) to serve a centralized environmental monitoring service. The proposed middleware is equipped with concealment mechanisms to preserve the privacy of the merged data sets from multiple IoMT networks involved in the mashup application. In addition, we presented an IoT-enabled data mashup service, where the multimedia data are collected from the various IoMT platforms, and then fed into an environmental deep learning service in order to detect interesting patterns in hazardous areas. The viable features within each region were extracted using a multiresolution wavelet transform, and then fed into a discriminative classifier to extract various patterns. We also provide a scenario for IoMT-enabled data mashup service and experimentation results.},
keywords={cloud computing;data analysis;data mining;data privacy;environmental monitoring (geophysics);environmental science computing;Internet;Internet of Things;learning (artificial intelligence);middleware;mobile computing;wavelet transforms;Web services;cognitive privacy middleware;deep learning mashup;single Web application;mashup application;data mashup service;data analytics;cognitive-based middleware;private data mashup;centralized environmental monitoring service;multimedia data;environmental deep learning service;environmental IoT;Mashups;Fires;Privacy;Environmental monitoring;Data privacy;Multimedia communication;Sensors;IoT networks;cloud computing;environmental monitoring;smart cities;big data mashup;multimedia data},
doi={10.1109/ACCESS.2017.2787422},
ISSN={2169-3536},
month={},}
@ARTICLE{7883886,
author={Q. Wu and H. Wu and X. Zhou and M. Tan and Y. Xu and Y. Yan and T. Hao},
journal={IEEE Transactions on Knowledge and Data Engineering},
title={Online Transfer Learning with Multiple Homogeneous or Heterogeneous Sources},
year={2017},
volume={29},
number={7},
pages={1494-1507},
abstract={Transfer learning techniques have been broadly applied in applications where labeled data in a target domain are difficult to obtain while a lot of labeled data are available in related source domains. In practice, there can be multiple source domains that are related to the target domain, and how to combine them is still an open problem. In this paper, we seek to leverage labeled data from multiple source domains to enhance classification performance in a target domain where the target data are received in an online fashion. This problem is known as the online transfer learning problem. To achieve this, we propose novel online transfer learning paradigms in which the source and target domains are leveraged adaptively. We consider two different problem settings: homogeneous transfer learning and heterogeneous transfer learning. The proposed methods work in an online manner, where the weights of the source domains are adjusted dynamically. We provide the mistake bounds of the proposed methods and perform comprehensive experiments on real-world data sets to demonstrate the effectiveness of the proposed algorithms.},
keywords={learning (artificial intelligence);online transfer learning technique;homogeneous transfer learning;heterogeneous transfer learning;Training;Silicon;Kernel;Machine learning algorithms;Data mining;Training data;Computer vision;Online transfer learning;multiple source domains;heterogeneous transfer},
doi={10.1109/TKDE.2017.2685597},
ISSN={1041-4347},
month={July},}
@ARTICLE{7006747,
author={J. Camacho-Rodríguez and D. Colazzo and I. Manolescu},
journal={IEEE Transactions on Knowledge and Data Engineering},
title={PAXQuery: Efficient Parallel Processing of Complex XQuery},
year={2015},
volume={27},
number={7},
pages={1977-1991},
abstract={Increasing volumes of data are being produced and exchanged over the Web, in particular in tree-structured formats such as XML or JSON. This leads to a need of highly scalable algorithms and tools for processing such data, capable to take advantage of massively parallel processing platforms. This work considers the problem of efficiently parallelizing the execution of complex nested data processing, expressed in XQuery. We provide novel algorithms showing how to translate such queries into PACT, a recent framework generalizing MapReduce in particular by supporting many-input tasks. We present the first formal translation of complex XQuery algebraic expressions into PACT plans, and demonstrate experimentally the efficiency and scalability of our approach.},
keywords={Internet;parallel processing;query processing;trees (mathematics);complex XQuery algebraic expressions;many-input task;MapReduce;PACT;complex nested data processing;tree-structured formats;Web;parallel processing;PAXQuery;XML;Algebra;Data models;Navigation;Vegetation;Contracts;Optimization;XQuery processing;XQuery parallelization;XML data management;XQuery processing;XQuery parallelization;XML data management},
doi={10.1109/TKDE.2015.2391110},
ISSN={1041-4347},
month={July},}
@ARTICLE{5383377,
author={R. Shatnawi},
journal={IEEE Transactions on Software Engineering},
title={A Quantitative Investigation of the Acceptable Risk Levels of Object-Oriented Metrics in Open-Source Systems},
year={2010},
volume={36},
number={2},
pages={216-225},
abstract={Object-oriented metrics have been validated empirically as measures of design complexity. These metrics can be used to mitigate potential problems in the software complexity. However, there are few studies that were conducted to formulate the guidelines, represented as threshold values, to interpret the complexity of the software design using metrics. Classes can be clustered into low and high risk levels using threshold values. In this paper, we use a statistical model, derived from the logistic regression, to identify threshold values for the Chidamber and Kemerer (CK) metrics. The methodology is validated empirically on a large open-source system-the Eclipse project. The empirical results indicate that the CK metrics have threshold effects at various risk levels. We have validated the use of these thresholds on the next release of the Eclipse project-Version 2.1-using decision trees. In addition, the selected threshold values were more accurate than those were selected based on either intuitive perspectives or on data distribution parameters. Furthermore, the proposed model can be exploited to find the risk level for an arbitrary threshold value. These findings suggest that there is a relationship between risk levels and object-oriented metrics and that risk levels can be used to identify threshold effects.},
keywords={decision trees;object-oriented programming;public domain software;software fault tolerance;software maintenance;software metrics;statistical analysis;object-oriented metrics;open source systems;software complexity;software design;software metrics;statistical model;logistic regression;Chidamber and Kemerer metrics;Eclipse project version 2.1;decision trees;threshold values;data distribution parameters;Open source software;Object oriented modeling;Software metrics;Software quality;Software testing;Software design;Predictive models;Quality assurance;Probability;Fault diagnosis;Object-oriented programming;product metrics;CK metrics;threshold values;open-source software.},
doi={10.1109/TSE.2010.9},
ISSN={0098-5589},
month={March},}
@ARTICLE{6871430,
author={M. A. Bhuiyan and M. Al Hasan},
journal={IEEE Transactions on Knowledge and Data Engineering},
title={An Iterative MapReduce Based Frequent Subgraph Mining Algorithm},
year={2015},
volume={27},
number={3},
pages={608-620},
abstract={Frequent subgraph mining (FSM) is an important task for exploratory data analysis on graph data. Over the years, many algorithms have been proposed to solve this task. These algorithms assume that the data structure of the mining task is small enough to fit in the main memory of a computer. However, as the real-world graph data grows, both in size and quantity, such an assumption does not hold any longer. To overcome this, some graph database-centric methods have been proposed in recent years for solving FSM; however, a distributed solution using MapReduce paradigm has not been explored extensively. Since MapReduce is becoming the de-facto paradigm for computation on massive data, an efficient FSM algorithm on this paradigm is of huge demand. In this work, we propose a frequent subgraph mining algorithm called FSM-H which uses an iterative MapReduce based framework. FSM-H is complete as it returns all the frequent subgraphs for a given user-defined support, and it is efficient as it applies all the optimizations that the latest FSM algorithms adopt. Our experiments with real life and large synthetic datasets validate the effectiveness of FSM-H for mining frequent subgraphs from large graph datasets. The source code of FSM-H is available from www.cs.iupui.edu/~alhasan/ software/},
keywords={data handling;data mining;graph theory;iterative methods;parallel processing;iterative MapReduce based frequent subgraph mining algorithm;exploratory data analysis;graph data;data structure;graph database-centric methods;de-facto paradigm;optimizations;FSM-H;Data mining;Graphical models;Information analysis;Iterative methods;Algorithm design and analysis;Frequent sub-graph mining;iterative MapReduce},
doi={10.1109/TKDE.2014.2345408},
ISSN={1041-4347},
month={March},}
@ARTICLE{5416717,
author={E. W. Xiang and B. Cao and D. H. Hu and Q. Yang},
journal={IEEE Transactions on Knowledge and Data Engineering},
title={Bridging Domains Using World Wide Knowledge for Transfer Learning},
year={2010},
volume={22},
number={6},
pages={770-783},
abstract={A major problem of classification learning is the lack of ground-truth labeled data. It is usually expensive to label new data instances for training a model. To solve this problem, domain adaptation in transfer learning has been proposed to classify target domain data by using some other source domain data, even when the data may have different distributions. However, domain adaptation may not work well when the differences between the source and target domains are large. In this paper, we design a novel transfer learning approach, called BIG (Bridging Information Gap), to effectively extract useful knowledge in a worldwide knowledge base, which is then used to link the source and target domains for improving the classification performance. BIG works when the source and target domains share the same feature space but different underlying data distributions. Using the auxiliary source data, we can extract a ¿bridge¿ that allows cross-domain text classification problems to be solved using standard semisupervised learning algorithms. A major contribution of our work is that with BIG, a large amount of worldwide knowledge can be easily adapted and used for learning in the target domain. We conduct experiments on several real-world cross-domain text classification tasks and demonstrate that our proposed approach can outperform several existing domain adaptation approaches significantly.},
keywords={data mining;learning (artificial intelligence);text analysis;world wide knowledge;transfer learning;classification learning;ground-truth labeled data;bridging information gap;data distributions;cross-domain text classification problems;semisupervised learning algorithms;data mining;Data mining;Text categorization;Wikipedia;Information retrieval;Semisupervised learning;Content based retrieval;Web search;Supervised learning;Terminology;Knowledge transfer;Data mining;transfer learning;cross-domain;text classification;Wikipedia.},
doi={10.1109/TKDE.2010.31},
ISSN={1041-4347},
month={June},}
@ARTICLE{8038803,
author={C. Wang and X. Meng and Q. Guo and Z. Weng and C. Yang},
journal={IEEE Transactions on Knowledge and Data Engineering},
title={Automating Characterization Deployment in Distributed Data Stream Management Systems},
year={2017},
volume={29},
number={12},
pages={2669-2681},
abstract={Distributed data stream management systems (DDSMS) are usually composed of upper layer relational query systems (RQS) and lower layer stream processing systems (SPS). When users submit new queries to RQS, a query planner needs to be converted into a directed acyclic graph (DAG) consisting of tasks which are running on SPS. Based on different query requests and data stream properties, SPS need to configure different deployments strategies. However, how to dynamically predict deployment configurations of SPS to ensure the processing throughput and low resource usage is a great challenge. This article presents OrientStream, a framework for automating characterization deployment in DDSMS using incremental machine learning techniques. By introducing the data-level, query plan-level, operator-level, and cluster-level's four-level feature extraction mechanism, we first use the different query workloads as training sets to predict the resource usage by DDSMS, and select the optimal resource configuration from candidate settings based on the current query requests and stream properties, then migrate the operator state by introducing dynamic reconfiguration. Finally, we validate our approach on the open source SPS-Storm. In view of the application scenarios with long monitoring cycle and non-frequent data fluctuation, experiments show that OrientStream can reduce CPU usage of 8-15 percent and memory usage of 38-48 percent, respectively.},
keywords={directed graphs;distributed processing;feature extraction;learning (artificial intelligence);query processing;storage management;distributed data stream management systems;DDSMS;RQS;relational query systems;stream processing systems;operator-level;cluster-level;four-level feature extraction;SPS;directed acyclic graph;DAG;incremental machine learning;open source SPS-Storm;query plan-level;data-level;query planner;Predictive models;Dynamic scheduling;Media streaming;Delta-sigma modulation;Real-time systems;Query processing;Learning systems;Stream processing system;relational query system;incremental learning;modeling and prediction},
doi={10.1109/TKDE.2017.2751606},
ISSN={1041-4347},
month={Dec},}
@ARTICLE{8404026,
author={F. Ge and Y. Ju and Z. Qi and Y. Lin},
journal={IEEE Access},
title={Parameter Estimation of a Gaussian Mixture Model for Wind Power Forecast Error by Riemann L-BFGS Optimization},
year={2018},
volume={6},
number={},
pages={38892-38899},
abstract={With the increasing penetration of wind power into the electricity grid, wind power forecast error analysis plays an important role in operations scheduling. To better describe the characteristics of power forecast error, a probability density function should be established. Compared with the Kernel density estimation method, this paper adopts the Gaussian mixture model (GMM), which is flexible enough to capture different error distribution characteristics, such as bias, heavy tail, multi-peak, and so on. In addition, for GMM parameter estimation, when dealing with a large number of multi-dimensional data sets or unbalanced overlapping mixtures, the expectation maximization (EM) algorithm shows a slower convergence speed and requires a high number of iterations. In this paper, a new L-BFGS optimization method, based on the Riemannian manifold, is used for GMM parameter estimation. Based on actual wind power forecast error data, the suitability of the model and the new optimization algorithm was verified in large, multi-dimensional data sets. The new optimization algorithm has fewer iterations than the EM algorithm, with an improved convergence speed.},
keywords={data analysis;differential geometry;expectation-maximisation algorithm;Gaussian processes;optimisation;parameter estimation;power engineering computing;power generation scheduling;power grids;statistical distributions;wind power plants;error distribution characteristics;Riemannian manifold;wind power forecast error data;Riemann L-BFGS optimization;operations scheduling;wind power forecast error analysis;electricity grid;optimization algorithm;L-BFGS optimization method;expectation maximization algorithm;overlapping mixtures;multidimensional data sets;GMM parameter estimation;Gaussian mixture model;probability density function;Wind power generation;Wind forecasting;Optimization;Manifolds;Mathematical model;Probability density function;Predictive models;Wind power forecast error;probability density function;Gaussian mixture model;Kernel density estimation;Riemannian manifold optimization;L-BFGS},
doi={10.1109/ACCESS.2018.2852501},
ISSN={2169-3536},
month={},}
@ARTICLE{8360417,
author={J. Liu and W. Zheng and Z. Lin and N. Lin},
journal={IEEE Access},
title={Accurate Quantile Estimation for Skewed Data Streams Using Nonlinear Interpolation},
year={2018},
volume={6},
number={},
pages={28438-28446},
abstract={Quantile estimation is a fundamental method to generate the descriptions of the distribution of data for data management and analysis. Although the investigation and design of efficient quantile estimation algorithm has attracted much study, the problem of accurately finding quantiles in the case of skewed data streams, which are prevalent in many data sources like text data and IP traffic streams, is still not well addressed. In this paper, we specifically address the problem of estimating the quantiles of skewed data streams by designing and implementing an incremental quantile estimation with nonlinear-interpolation algorithm. The comprehensive experimental evaluation results demonstrate that the estimated quantiles of the proposed algorithm are more accurate than the existing methods in the literature on both synthetic and real-world datasets, especially on important extreme quantiles.},
keywords={data analysis;estimation theory;interpolation;accurate quantile estimation;skewed data streams;data management;efficient quantile estimation algorithm;data sources;incremental quantile estimation;nonlinear-interpolation algorithm;important extreme quantiles;Estimation;Approximation algorithms;Delays;Data models;Interpolation;IP networks;Distribution functions;Data streams;quantile estimation},
doi={10.1109/ACCESS.2018.2837906},
ISSN={2169-3536},
month={},}
@ARTICLE{6807734,
author={V. M. Megler and D. Maier},
journal={IEEE Transactions on Knowledge and Data Engineering},
title={Are Data Sets Like Documents?: Evaluating Similarity-Based Ranked Search over Scientific Data},
year={2015},
volume={27},
number={1},
pages={32-45},
abstract={The past decade has seen a dramatic increase in the amount of data captured and made available to scientists for research. This increase amplifies the difficulty scientists face in finding the data most relevant to their information needs. In prior work, we hypothesized that Information Retrieval-style ranked search can be applied to data sets to help a scientist discover the most relevant data amongst the thousands of data sets in many formats, much like text-based ranked search helps users make sense of the vast number of Internet documents. To test this hypothesis, we explored the use of ranked search for scientific data using an existing multi-terabyte observational archive as our test-bed. In this paper, we investigate whether the concept of varying relevance, and therefore ranked search, applies to numeric data-that is, are data sets are enough like documents for Information Retrieval techniques and evaluation measures to apply? We present a user study that demonstrates that data set similarity resonates with users as a basis for relevance and, therefore, for ranked search. We evaluate a prototype implementation of ranked search over data sets with a second user study and demonstrate that ranked search improves a scientist's ability to find needed data.},
keywords={information needs;information retrieval;Internet;natural sciences computing;text analysis;data sets;similarity-based ranked search evaluation;scientific data;information needs;information retrieval-style ranked search;text-based ranked search;Internet documents;multiterabyte observational archive;numeric data;Catalogs;Search problems;Sociology;Statistics;Temperature distribution;Geospatial analysis;Ocean temperature;Scientific databases;information retrieval and relevance;similarity search;Scientific databases;information retrieval and relevance;similarity search},
doi={10.1109/TKDE.2014.2320737},
ISSN={1041-4347},
month={Jan},}
@ARTICLE{8295185,
author={H. Chen and W. Guo and J. Shen and L. Wang and J. Song},
journal={IEEE Access},
title={Structural Principles Analysis of Host-Pathogen Protein-Protein Interactions: A Structural Bioinformatics Survey},
year={2018},
volume={6},
number={},
pages={11760-11771},
abstract={Computational-intelligence methods in bioinformatics and systems biology show promising potential for leveraging abundant, large-scale molecular data. These methods can facilitate analysis and prediction of the principles of biological systems through the construction of statistical and visualized models. Specifically, structural data from exogenous and endogenous protein-protein interactions are of vital significance in this context, encompassing primarily 3-D structural information for a cohort of macromolecules underpinning the biological system. In this paper, we surveyed the main methodologies and algorithms for the reconstruction and modeling of the structural-interaction networks (SINs) of host- pathogen protein-protein interactions (HPPPIs), regarding how the protein domains interact with each other to constitute a SIN. Surveying the pattern and the organization of the SIN delivers a state-of-theart view of HPPPIs and illustrates prospective future research directions. In addition to the binary PPI network, we distilled the relevant data sources into several branching research areas and further expanded the discussions into computational-intelligence methods according to the algorithms applied, including machine learning statistical models, to shed light on effective method design. In particular, atomic resolution level investigations can reveal novel insights into the underlying principles of the organization and the complexity of HPPPIs networks. Combining data analytics and machine-learning technologies, we anticipate that our systematic overview will serve as a useful guide for interested researchers to carry out related studies on this exciting and challenging research topic in system biology.},
keywords={biology computing;data analysis;learning (artificial intelligence);macromolecules;molecular biophysics;proteins;structural principles analysis;host-pathogen protein-protein interactions;structural bioinformatics survey;computational-intelligence methods;large-scale molecular data;biological system;statistical models;visualized models;structural data;exogenous protein-protein interactions;endogenous protein-protein interactions;3-D structural information;SIN;relevant data sources;effective method design;data analytics;machine-learning technologies;system biology;systems biology;Proteins;Amino acids;Bioinformatics;Computational modeling;Proteomics;Biological system modeling;Three-dimensional displays;Host-pathogen interactions;structural-interaction network;bioinformatics;machine learning;data analytics},
doi={10.1109/ACCESS.2018.2807881},
ISSN={2169-3536},
month={},}
@ARTICLE{8169664,
author={Y. Tai and W. Hu and D. Mu and B. Mao and L. Zhang},
journal={IEEE Access},
title={Towards Quantified Data Analysis of Information Flow Tracking for Secure System Design},
year={2018},
volume={6},
number={},
pages={1822-1831},
abstract={Computing hardware has become an attractive attack surface due to the globalization of semi-conductor design and supply chain, and the wide integration of third-party intellectual property cores. Recently, gate-level information flow tracking (GLIFT) has been proposed to monitor the flow of information in secure hardware design by associating data objects with sensitivity labels and tracking the flow of labeled data. GLIFT can be used to model and verify security-related properties, such as confidentiality and integrity. However, existing work in this realm only considers binary labels. These are inadequate for understanding simultaneous information flow behaviors and the root source information flows. In this paper, we propose a precise multi-bit GLIFT method to perform simultaneous multi-bit flow tracking for understanding exactly which bits are affecting a data object at the same time. The proposed method provides more detailed insights into simultaneous information flow behaviors and thus allows proof of quantitative information flow data properties. We compare the complexity and verification performance for different information flow models using primitive gates, IWLS benchmarks, several cryptographic cores, and trustHUB benchmarks. Experimental results have demonstrated that our method can reason about multi-bit information flow behaviors and identify potential security flaws.},
keywords={cryptography;data analysis;industrial property;invasive software;quantitative information flow data properties;multibit information flow behaviors;quantified data analysis;secure system design;computing hardware;attractive attack surface;supply chain;third-party intellectual property cores;gate-level information flow tracking;secure hardware design;data object;sensitivity labels;security-related properties;binary labels;simultaneous information flow behaviors;root source information;multibit GLIFT method;simultaneous multibit flow;information flow models;security flaws;Security;Hardware;Lattices;Complexity theory;Logic gates;Data models;Timing;Hardware security;information flow security;information flow tracking;security verification;multi-flow},
doi={10.1109/ACCESS.2017.2780254},
ISSN={2169-3536},
month={},}
@ARTICLE{6412667,
author={T. Tran and L. Zhang},
journal={IEEE Transactions on Knowledge and Data Engineering},
title={Keyword Query Routing},
year={2014},
volume={26},
number={2},
pages={363-375},
abstract={Keyword search is an intuitive paradigm for searching linked data sources on the web. We propose to route keywords only to relevant sources to reduce the high cost of processing keyword search queries over all sources. We propose a novel method for computing top-k routing plans based on their potentials to contain results for a given keyword query. We employ a keyword-element relationship summary that compactly represents relationships between keywords and the data elements mentioning them. A multilevel scoring mechanism is proposed for computing the relevance of routing plans based on scores at the level of keywords, data elements, element sets, and subgraphs that connect these elements. Experiments carried out using 150 publicly available sources on the web showed that valid plans (precision@1 of 0.92) that are highly relevant (mean reciprocal rank of 0.89) can be computed in 1 second on average on a single PC. Further, we show routing greatly helps to improve the performance of keyword search, without compromising its result quality.},
keywords={Internet;query processing;keyword query routing;linked data sources;keyword search queries;top-k routing plans;keyword-element relationship summary;multilevel scoring mechanism;publicly available sources;PC;Keyword search;Resource description framework;Query processing;Data models;Awards activities;Keyword search;keyword query;keyword query routing;graph-structured data;RDF},
doi={10.1109/TKDE.2013.13},
ISSN={1041-4347},
month={Feb},}
@ARTICLE{8288619,
author={A. Akbar and G. Kousiouris and H. Pervaiz and J. Sancho and P. Ta-Shma and F. Carrez and K. Moessner},
journal={IEEE Access},
title={Real-Time Probabilistic Data Fusion for Large-Scale IoT Applications},
year={2018},
volume={6},
number={},
pages={10015-10027},
abstract={Internet of Things (IoT) data analytics is underpinning numerous applications, however, the task is still challenging predominantly due to heterogeneous IoT data streams, unreliable networks, and ever increasing size of the data. In this context, we propose a two-layer architecture for analyzing IoT data. The first layer provides a generic interface using a service oriented gateway to ingest data from multiple interfaces and IoT systems, store it in a scalable manner and analyze it in real-time to extract high-level events; whereas second layer is responsible for probabilistic fusion of these high-level events. In the second layer, we extend state-of-the-art event processing using Bayesian networks in order to take uncertainty into account while detecting complex events. We implement our proposed solution using open source components optimized for large-scale applications. We demonstrate our solution on real-world use-case in the domain of intelligent transportation system where we analyzed traffic, weather, and social media data streams from Madrid city in order to predict probability of congestion in real-time. The performance of the system is evaluated qualitatively using a web-interface where traffic administrators can provide the feedback about the quality of predictions and quantitatively using F-measure with an accuracy of over 80%.},
keywords={belief networks;data analysis;intelligent transportation systems;Internet of Things;probability;sensor fusion;Web-interface;real-time probabilistic data fusion;Internet of Things data analytics;Madrid city;service oriented gateway;generic interface;two-layer architecture;heterogeneous IoT data streams;large-scale IoT applications;social media data streams;intelligent transportation system;complex events;Bayesian networks;high-level events;IoT systems;Real-time systems;Probabilistic logic;Uncertainty;Data mining;Meteorology;Bayes methods;Data analysis;Complex event processing;data analysis;internet of things;real-time systems;intelligent transportation systems},
doi={10.1109/ACCESS.2018.2804623},
ISSN={2169-3536},
month={},}
@ARTICLE{6295617,
author={R. Jiang and H. Fei and J. Huan},
journal={IEEE Transactions on Knowledge and Data Engineering},
title={A Family of Joint Sparse PCA Algorithms for Anomaly Localization in Network Data Streams},
year={2013},
volume={25},
number={11},
pages={2421-2433},
abstract={Determining anomalies in data streams that are collected and transformed from various types of networks has recently attracted significant research interest. Principal component analysis (PCA) has been extensively applied to detecting anomalies in network data streams. However, none of existing PCA-based approaches addresses the problem of identifying the sources that contribute most to the observed anomaly, or anomaly localization. In this paper, we propose novel sparse PCA methods to perform anomaly detection and localization for network data streams. Our key observation is that we can localize anomalies by identifying a sparse low-dimensional space that captures the abnormal events in data streams. To better capture the sources of anomalies, we incorporate the structure information of the network stream data in our anomaly localization framework. Furthermore, we extend our joint sparse PCA framework with multidimensional Karhunen Loève Expansion that considers both spatial and temporal domains of data streams to stabilize localization performance. We have performed comprehensive experimental studies of the proposed methods and have compared our methods with the state-of-the-art using three real-world data sets from different application domains. Our experimental studies demonstrate the utility of the proposed methods.},
keywords={data analysis;network theory (graphs);principal component analysis;joint sparse PCA algorithms;anomaly localization;network data streams;principal component analysis;PCA-based approaches;sparse low-dimensional space;multidimensional Karhunen Lώve expansion;spatial domains;temporal domains;Principal component analysis;Joints;Vectors;Equations;Correlation;Sparse matrices;Time series analysis;Anomaly detection;anomaly localization;PCA;network data stream;joint sparsity;optimization},
doi={10.1109/TKDE.2012.176},
ISSN={1041-4347},
month={Nov},}
@ARTICLE{7572203,
author={P. Yang and H. Davulcu and Y. Zhu and J. He},
journal={IEEE Transactions on Knowledge and Data Engineering},
title={A Generalized Hierarchical Multi-Latent Space Model for Heterogeneous Learning},
year={2016},
volume={28},
number={12},
pages={3154-3168},
abstract={In many real world applications such as image annotation, gene function prediction, and insider threat detection, the data collected from heterogeneous sources often exhibit multiple types of heterogeneity, such as task heterogeneity, view heterogeneity, and label heterogeneity. To address this problem, we propose a Hierarchical Multi-Latent Space (HiMLS) learning framework to jointly model the triple types of heterogeneity. The basic idea is to learn a hierarchical multi-latent space by which we can simultaneously leverage the task relatedness, view consistency and the label correlations to improve the learning performance. We first propose a multi-latent space approach to model the complex heterogeneity, which is then used as a building block to stack up a multi-layer structure in order to learn the hierarchical multi-latent space. In such a way, we can gradually learn the more abstract concepts in the higher level. We present two instantiated models of the generalized framework using different divergence measures. The two-phase learning algorithms are used to train the multi-layer models. We drive the multiplicative update rules for pre-training and fine-tuning in each model, and prove the convergence and correctness of the update methods. The effectiveness of the proposed approach is verified on various data sets.},
keywords={learning (artificial intelligence);matrix decomposition;generalized hierarchical multilatent space model;heterogeneous learning;hierarchical multilatent space learning framework;HiMLS learning framework;task relatedness;multilatent space approach;multilayer structure;two-phase learning algorithms;multiplicative update rules;Correlation;Data models;Learning systems;Data mining;Feature extraction;Matrix decomposition;Encoding;Labeling;Heterogeneous learning;multi-task learning;multi-view learning;multi-label learning;matrix tri-factorization},
doi={10.1109/TKDE.2016.2611514},
ISSN={1041-4347},
month={Dec},}
@ARTICLE{7452659,
author={M. Long and J. Wang and Y. Cao and J. Sun and P. S. Yu},
journal={IEEE Transactions on Knowledge and Data Engineering},
title={Deep Learning of Transferable Representation for Scalable Domain Adaptation},
year={2016},
volume={28},
number={8},
pages={2027-2040},
abstract={Domain adaptation generalizes a learning model across source domain and target domain that are sampled from different distributions. It is widely applied to cross-domain data mining for reusing labeled information and mitigating labeling consumption. Recent studies reveal that deep neural networks can learn abstract feature representation, which can reduce, but not remove, the cross-domain discrepancy. To enhance the invariance of deep representation and make it more transferable across domains, we propose a unified deep adaptation framework for jointly learning transferable representation and classifier to enable scalable domain adaptation, by taking the advantages of both deep learning and optimal two-sample matching. The framework constitutes two inter-dependent paradigms, unsupervised pre-training for effective training of deep models using deep denoising autoencoders, and supervised fine-tuning for effective exploitation of discriminative information using deep neural networks, both learned by embedding the deep representations to reproducing kernel Hilbert spaces (RKHSs) and optimally matching different domain distributions. To enable scalable learning, we develop a linear-time algorithm using unbiased estimate that scales linearly to large samples. Extensive empirical results show that the proposed framework significantly outperforms state of the art methods on diverse adaptation tasks: sentiment polarity prediction, email spam filtering, newsgroup content categorization, and visual object recognition.},
keywords={data mining;learning (artificial intelligence);neural nets;deep learning;transferable representation learning;scalable domain adaptation;cross-domain data mining;labeled information reuse;labeling consumption mitigation;deep neural networks;abstract feature representation learning;cross-domain discrepancy;optimal two-sample matching;deep denoising autoencoders;reproducing kernel Hilbert spaces;RKHS;scalable learning;linear-time algorithm;sentiment polarity prediction task;email spam filtering task;newsgroup content categorization task;visual object recognition task;Kernel;Machine learning;Adaptation models;Neural networks;Noise reduction;Labeling;Object recognition;Domain adaptation;deep learning;denoising autoencoder;neural network;two-sample test;multiple kernel learning},
doi={10.1109/TKDE.2016.2554549},
ISSN={1041-4347},
month={Aug},}
@ARTICLE{8082527,
author={X. Zhu and S. Zhang and R. Hu and Y. Zhu and j. song},
journal={IEEE Transactions on Knowledge and Data Engineering},
title={Local and Global Structure Preservation for Robust Unsupervised Spectral Feature Selection},
year={2018},
volume={30},
number={3},
pages={517-529},
abstract={This paper proposes a new unsupervised spectral feature selection method to preserve both the local and global structure of the features as well as the samples. Specifically, our method uses the self-expressiveness of the features to represent each feature by other features for preserving the local structure of features, and a low-rank constraint on the weight matrix to preserve the global structure among samples as well as features. Our method also proposes to learn the graph matrix measuring the similarity of samples for preserving the local structure among samples. Furthermore, we propose a new optimization algorithm to the resulting objective function, which iteratively updates the graph matrix and the intrinsic space so that collaboratively improving each of them. Experimental analysis on 12 benchmark datasets showed that the proposed method outperformed the state-of-the-art feature selection methods in terms of classification performance.},
keywords={feature extraction;feature selection;graph theory;iterative methods;matrix algebra;optimisation;pattern classification;unsupervised learning;local structure preservation;global structure preservation;optimization algorithm;graph matrix;weight matrix;unsupervised spectral feature selection method;robust unsupervised spectral feature selection;Feature extraction;Correlation;Robustness;Training;Redundancy;Kernel;Feature selection;graph matrix;dimensionality reduction;subspace learning},
doi={10.1109/TKDE.2017.2763618},
ISSN={1041-4347},
month={March},}
@ARTICLE{8078180,
author={I. García-Magariño and R. Lacuesta and J. Lloret},
journal={IEEE Access},
title={Agent-Based Simulation of Smart Beds With Internet-of-Things for Exploring Big Data Analytics},
year={2018},
volume={6},
number={},
pages={366-379},
abstract={Internet-of-Things (IoT) can allow healthcare professionals to remotely monitor patients by analyzing the sensors outputs with big data analytics. Sleeping conditions are one of the most influential factors on health. However, the literature lacks of the appropriate simulation tools to widely support the research on the recognition of sleeping postures. This paper proposes an agent-based simulation framework to simulate sleeper movements on a simulated smart bed with load sensors. This framework allows one to define sleeping posture recognition algorithms and compare their outcomes with the poses adopted by the sleeper. This novel presented ABS-BedIoT simulator allows users to graphically explore the results with starplots, evolution charts, and final visual representations of the states of the bed sensors. This simulator can also generate logs text files with big data for applying offline big data techniques on them. The source code of ABS-BedIoT and some examples of logs are freely available from a public research repository. The current approach is illustrated with an algorithm that properly recognized the simulated sleeping postures with an average accuracy of 98%. This accuracy is higher than the one reported by an existing alternative work in this area.},
keywords={Big Data;cloud computing;data analysis;health care;Internet of Things;medical computing;patient monitoring;power system simulation;sleep;smart power grids;internet-of-things;healthcare professionals;load sensors;sleeping posture recognition algorithms;ABS-BedIoT simulator;bed sensors;offline big data techniques;smart beds;Big data analytics;remotely monitor patients;agent-based simulation framework;Intelligent sensors;Big Data;Sleep apnea;Monitoring;Biomedical monitoring;Agent-based-simulation;big data;Internet-of-Things;multi-agent systems;smart bed},
doi={10.1109/ACCESS.2017.2764467},
ISSN={2169-3536},
month={},}
@ARTICLE{4906994,
author={T. Wong and W. Lam},
journal={IEEE Transactions on Knowledge and Data Engineering},
title={Learning to Adapt Web Information Extraction Knowledge and Discovering New Attributes via a Bayesian Approach},
year={2010},
volume={22},
number={4},
pages={523-536},
abstract={This paper presents a Bayesian learning framework for adapting information extraction wrappers with new attribute discovery, reducing human effort in extracting precise information from unseen Web sites. Our approach aims at automatically adapting the information extraction knowledge previously learned from a source Web site to a new unseen site, at the same time, discovering previously unseen attributes. Two kinds of text-related clues from the source Web site are considered. The first kind of clue is obtained from the extraction pattern contained in the previously learned wrapper. The second kind of clue is derived from the previously extracted or collected items. A generative model for the generation of the site-independent content information and the site-dependent layout format of the text fragments related to attribute values contained in a Web page is designed to harness the uncertainty involved. Bayesian learning and expectation-maximization (EM) techniques are developed under the proposed generative model for identifying new training data for learning the new wrapper for new unseen sites. Previously unseen attributes together with their semantic labels can also be discovered via another EM-based Bayesian learning based on the generative model. We have conducted extensive experiments from more than 30 real-world Web sites in three different domains to demonstrate the effectiveness of our framework.},
keywords={Bayes methods;data mining;expectation-maximisation algorithm;information retrieval systems;learning (artificial intelligence);Web sites;Web information knowledge extraction;Bayesian learning framework;information extraction wrappers;Web sites;text-related clues;pattern extraction;site-independent content information;site-dependent layout format;text fragments;Web page;expectation-maximization techniques;training;EM-based Bayesian learning;Data mining;Bayesian methods;Humans;Web pages;Learning systems;Books;Web page design;Uncertainty;Training data;Web mining;Wrapper adaptation;Web mining;text mining;machine learning.},
doi={10.1109/TKDE.2009.111},
ISSN={1041-4347},
month={April},}
@ARTICLE{7299607,
author={I. Model and L. Shamir},
journal={IEEE Access},
title={Comparison of Data Set Bias in Object Recognition Benchmarks},
year={2015},
volume={3},
number={},
pages={1953-1962},
abstract={Current research in the area of automatic visual object recognition heavily relies on testing the performance of new algorithms by using benchmark data sets. Such data sets can be based on standardized data sets collected systematically in a controlled environment (e.g., COIL-20), as well as benchmarks compiled by collecting images from various sources, normally via the World Wide Web (e.g., Caltech 101). Here, we test bias in benchmark data sets by separating a small area from each image such that the area is seemingly blank, and too small to allow manual recognition of the object. The method can be used to detect the existence of data set bias in a single-object recognition data set, and compare the bias to other data sets. The results show that all the tested data sets allowed classification accuracy higher than mere chance by using the small images, although the sub-images did not contain any visually interpretable information. That shows that the consistency of the images within the different classes of object recognition data sets can allow classifying the images even by algorithms that do not recognize objects. Among the tested data sets, PASCAL is the data set with the lowest observed bias, while data sets acquired in a controlled environment, such as COIL-20, COIL-100, and NEC Animals, are more vulnerable to bias, and can be classified by the sub-images with accuracy far higher than mere chance.},
keywords={image classification;Internet;object recognition;data set bias;object recognition benchmarks;automatic visual object recognition;benchmark data sets;standardized data sets;controlled environment;World Wide Web;manual object recognition;single-object recognition data set;classification accuracy;PASCAL;COIL-20;COIL-100;NEC Animals;Object recognition;Performance evaluation;Benchmark testing;Pattern recognition;Validation;Object recognition;performance evaluation;benchmarks;validation;computer vision;pattern recognition;Object recognition;performance evaluation;benchmarks;validation;computer vision;pattern recognition},
doi={10.1109/ACCESS.2015.2491921},
ISSN={2169-3536},
month={},}
@ARTICLE{8070349,
author={L. Wu and Q. Liu and R. Hong and E. Chen and Y. Ge and X. Xie and M. Wang},
journal={IEEE Transactions on Knowledge and Data Engineering},
title={Product Adoption Rate Prediction in a Competitive Market},
year={2018},
volume={30},
number={2},
pages={325-338},
abstract={As the worlds of commerce and the Internet technology become more inextricably linked, a large number of user consumption series become available for online market intelligence analysis. A critical demand along this line is to predict the future product adoption state of each user, which enables a wide range of applications such as targeted marketing. Nevertheless, previous works only aimed at predicting if a user would adopt a particular product or not with a binary buy-or-not representation. The problem of tracking and predicting users' adoption rates, i.e., the frequency and regularity of using each product over time, is still under-explored. To this end, we present a comprehensive study of product adoption rate prediction in a competitive market. This task is nontrivial as there are three major challenges in modeling users' complex adoption states: the heterogeneous data sources around users, the unique user preference and the competitive product selection. To deal with these challenges, we first introduce a flexible factor-based decision function to capture the change of users' product adoption rate over time, where various factors that may influence users' decisions from heterogeneous data sources can be leveraged. Using this factor-based decision function, we then provide two corresponding models to learn the parameters of the decision function with both generalized and personalized assumptions of users' preferences. We further study how to leverage the competition among different products and simultaneously learn product competition and users' preferences with both generalized and personalized assumptions. Finally, extensive experiments on two real-world datasets show the superiority of our proposed models.},
keywords={electronic commerce;Internet;product adoption rate prediction;competitive market;user consumption series;online market intelligence analysis;future product adoption state;targeted marketing;heterogeneous data sources;unique user preference;competitive product selection;flexible factor-based decision function;product competition;Smart devices;Social network services;Recommender systems;Predictive models;Electronic mail;Time-frequency analysis;User modeling;product adoption;user interest modeling;product competition},
doi={10.1109/TKDE.2017.2763944},
ISSN={1041-4347},
month={Feb},}
@ARTICLE{8063396,
author={W. H. Walters},
journal={IEEE Access},
title={Citation-Based Journal Rankings: Key Questions, Metrics, and Data Sources},
year={2017},
volume={5},
number={},
pages={22036-22053},
abstract={This guide presents nine key questions that can help researchers make good use of citation-based journal rankings (metrics) in the natural and social sciences. The nine questions address the characteristics that distinguish one metric from another: the source documents, the citation-counting window, the document types counted, the cited-document window, the impact of highly cited documents, the treatment of self-citations, the distinction between size-dependent and size-independent metrics, the use of normalization to account for disciplinary differences in impact, and the use of weighting to account for the impact or centrality of each citing journal. Next, the guide reviews 19 standard citation metrics, including the h index, g index, impact factor, source normalized impact per paper, eigenfactor, article influence score, and SCImago journal rank. Three underlying data sources (Web of Science, Scopus, and Google Scholar) are described, along with six major data download sites: Journal Citation Reports, Eigenfactor, CWTS Journal Indicators, SCImago, Scopus Journal Metrics, Cabell's International, and Google Scholar Metrics. The paper summarizes the main criticisms of citation metrics and concludes with suggestions for their further development, dissemination, and use.},
keywords={citation analysis;document handling;Internet;Journal Citation Reports;CWTS Journal Indicators;Scopus Journal Metrics;Google Scholar Metrics;citation-based journal rankings;natural sciences;social sciences;source documents;citation-counting window;document types;cited-document window;highly cited documents;self-citations;size-independent metrics;citing journal;impact factor;source normalized impact;SCImago journal rank;data download sites;data sources;standard citation metrics;Measurement;Indexes;Bibliometrics;Economics;Standards;Google;Continuous wavelet transforms;Impact;indicator;metric;ranking;rating},
doi={10.1109/ACCESS.2017.2761400},
ISSN={2169-3536},
month={},}
@ARTICLE{7875079,
author={A. Abbas and I. F. Siddiqui and S. U. Lee and A. K. Bashir},
journal={IEEE Access},
title={Binary Pattern for Nested Cardinality Constraints for Software Product Line of IoT-Based Feature Models},
year={2017},
volume={5},
number={},
pages={3971-3980},
abstract={Software product line (SPL) is extensively used for reusability of resources in family of products. Feature modeling is an important technique used to manage common and variable features of SPL in applications, such as Internet of Things (IoT). In order to adopt SPL for application development, organizations require information, such as cost, scope, complexity, number of features, total number of products, and combination of features for each product to start the application development. Application development of IoT is varied in different contexts, such as heat sensor indoor and outdoor environment. Variability management of IoT applications enables to find the cost, scope, and complexity. All possible combinations of features make it easy to find the cost of individual application. However, exact number of all possible products and features combination for each product is more valuable information for an organization to adopt product line. In this paper, we have proposed binary pattern for nested cardinality constraints (BPNCC), which is simple and effective approach to calculate the exact number of products with complex relationships between application's feature models. Furthermore, BPNCC approach identifies the feasible features combinations of each IoT application by tracing the constraint relationship from top-to-bottom. BPNCC is an open source and tool-independent approach that does not hide the internal information of selected and non-selected IoT features. The proposed method is validated by implementing it on small and large IoT application feature models with “n” number of constraints, and it is found that the total number of products and all features combinations in each product without any constraint violation.},
keywords={Internet of Things;public domain software;software cost estimation;software product lines;cost estimation models;open source approach;tool-independent approach;binary pattern for nested cardinality constraints;BPNCC;variability management;Internet of Things;SPL;IoT-based feature models;software product line;Biological system modeling;Software;Task analysis;Organizations;Estimation;Adaptation models;Analytical models;Software product line;feature model;internet of things (IoT);cost estimation models},
doi={10.1109/ACCESS.2017.2680470},
ISSN={2169-3536},
month={},}
@ARTICLE{7006742,
author={H. Mashayekhi and J. Habibi and T. Khalafbeigi and S. Voulgaris and M. van Steen},
journal={IEEE Transactions on Knowledge and Data Engineering},
title={GDCluster: A General Decentralized Clustering Algorithm},
year={2015},
volume={27},
number={7},
pages={1892-1905},
abstract={In many popular applications like peer-to-peer systems, large amounts of data are distributed among multiple sources. Analysis of this data and identifying clusters is challenging due to processing, storage, and transmission costs. In this paper, we propose GDCluster, a general fully decentralized clustering method, which is capable of clustering dynamic and distributed data sets. Nodes continuously cooperate through decentralized gossip-based communication to maintain summarized views of the data set. We customize GDCluster for execution of the partition-based and density-based clustering methods on the summarized views, and also offer enhancements to the basic algorithm. Coping with dynamic data is made possible by gradually adapting the clustering model. Our experimental evaluations show that GDCluster can discover the clusters efficiently with scalable transmission cost, and also expose its supremacy in comparison to the popular method LSP2P.},
keywords={costing;data analysis;distributed processing;pattern clustering;LSP2P;density-based clustering method;partition-based clustering method;decentralized gossip-based communication;distributed data set clustering;dynamic data set clustering;transmission cost;storage cost;processing cost;cluster identification;data analysis;general decentralized clustering algorithm;GDCluster;Peer-to-peer computing;Clustering algorithms;Partitioning algorithms;Distributed databases;Data models;Approximation algorithms;Vectors;Distributed Systems;Clustering;Partition-based Clustering;Density-based Clustering;Dynamic System;Distributed systems;clustering;partition-based clustering;density-based clustering;dynamic system},
doi={10.1109/TKDE.2015.2391123},
ISSN={1041-4347},
month={July},}
@ARTICLE{7931592,
author={G. Valkanas and T. Lappas and D. Gunopulos},
journal={IEEE Transactions on Knowledge and Data Engineering},
title={Mining Competitors from Large Unstructured Datasets},
year={2017},
volume={29},
number={9},
pages={1971-1984},
abstract={In any competitive business, success is based on the ability to make an item more appealing to customers than the competition. A number of questions arise in the context of this task: how do we formalize and quantify the competitiveness between two items? Who are the main competitors of a given item? What are the features of an item that most affect its competitiveness? Despite the impact and relevance of this problem to many domains, only a limited amount of work has been devoted toward an effective solution. In this paper, we present a formal definition of the competitiveness between two items, based on the market segments that they can both cover. Our evaluation of competitiveness utilizes customer reviews, an abundant source of information that is available in a wide range of domains. We present efficient methods for evaluating competitiveness in large review datasets and address the natural problem of finding the top-k competitors of a given item. Finally, we evaluate the quality of our results and the scalability of our approach using multiple datasets from different domains.},
keywords={data mining;electronic commerce;information retrieval;marketing data processing;data mining;unstructured datasets;competitive business;market segments;competitiveness;customer reviews;top-k competitors;electronic commerce;information retrieval;Bars;Business;Indexes;Electronic mail;Standards;Data mining;Context;Data mining;web mining;information search and retrieval;electronic commerce},
doi={10.1109/TKDE.2017.2705101},
ISSN={1041-4347},
month={Sept},}
@ARTICLE{5740892,
author={M. Mazuran and E. Quintarelli and L. Tanca},
journal={IEEE Transactions on Knowledge and Data Engineering},
title={Data Mining for XML Query-Answering Support},
year={2012},
volume={24},
number={8},
pages={1393-1407},
abstract={Extracting information from semistructured documents is a very hard task, and is going to become more and more critical as the amount of digital information available on the Internet grows. Indeed, documents are often so large that the data set returned as answer to a query may be too big to convey interpretable knowledge. In this paper, we describe an approach based on Tree-Based Association Rules (TARs): mined rules, which provide approximate, intensional information on both the structure and the contents of Extensible Markup Language (XML) documents, and can be stored in XML format as well. This mined knowledge is later used to provide: 1) a concise idea-the gist-of both the structure and the content of the XML document and 2) quick, approximate answers to queries. In this paper, we focus on the second feature. A prototype system and experimental results demonstrate the effectiveness of the approach.},
keywords={data mining;document handling;Internet;query processing;trees (mathematics);XML;data mining;XML query-answering support;information extraction;semistructured documents;digital information;Internet;interpretable knowledge;tree-based association rules;TAR;extensible markup language documents;XML;Association rules;Metals;Indexes;Proposals;Semantics;Context;XML;approximate query-answering;data mining;intensional information;succinct answers.},
doi={10.1109/TKDE.2011.80},
ISSN={1041-4347},
month={Aug},}
@ARTICLE{7898457,
author={S. McIntosh and Y. Kamei},
journal={IEEE Transactions on Software Engineering},
title={Are Fix-Inducing Changes a Moving Target? A Longitudinal Case Study of Just-In-Time Defect Prediction},
year={2018},
volume={44},
number={5},
pages={412-428},
abstract={Just-In-Time (JIT) models identify fix-inducing code changes. JIT models are trained using techniques that assume that past fix-inducing changes are similar to future ones. However, this assumption may not hold, e.g., as system complexity tends to accrue, expertise may become more important as systems age. In this paper, we study JIT models as systems evolve. Through a longitudinal case study of 37,524 changes from the rapidly evolving Qt and OpenStack systems, we find that fluctuations in the properties of fix-inducing changes can impact the performance and interpretation of JIT models. More specifically: (a) the discriminatory power (AUC) and calibration (Brier) scores of JIT models drop considerably one year after being trained; (b) the role that code change properties (e.g., Size, Experience) play within JIT models fluctuates over time; and (c) those fluctuations yield over- and underestimates of the future impact of code change properties on the likelihood of inducing fixes. To avoid erroneous or misleading predictions, JIT models should be retrained using recently recorded data (within three months). Moreover, quality improvement plans should be informed by JIT models that are trained using six months (or more) of historical data, since they are more resilient to period-specific fluctuations in the importance of code change properties.},
keywords={data mining;just-in-time;learning (artificial intelligence);public domain software;software management;software quality;source code (software);Just-In-Time models;fix-inducing code changes;code change properties;target moving;just-in-time defect prediction;fix-inducing changes;JIT models;OpenStack systems;Qt systems;mining software repositories;Predictive models;Data models;Software;Complexity theory;Market research;Context modeling;Calibration;Just-In-Time prediction;defect prediction;mining software repositories},
doi={10.1109/TSE.2017.2693980},
ISSN={0098-5589},
month={May},}
@ARTICLE{7820211,
author={E. d. S. Maldonado and E. Shihab and N. Tsantalis},
journal={IEEE Transactions on Software Engineering},
title={Using Natural Language Processing to Automatically Detect Self-Admitted Technical Debt},
year={2017},
volume={43},
number={11},
pages={1044-1062},
abstract={The metaphor of technical debt was introduced to express the trade off between productivity and quality, i.e., when developers take shortcuts or perform quick hacks. More recently, our work has shown that it is possible to detect technical debt using source code comments (i.e., self-admitted technical debt), and that the most common types of self-admitted technical debt are design and requirement debt. However, all approaches thus far heavily depend on the manual classification of source code comments. In this paper, we present an approach to automatically identify design and requirement self-admitted technical debt using Natural Language Processing (NLP). We study 10 open source projects: Ant, ArgoUML, Columba, EMF, Hibernate, JEdit, JFreeChart, JMeter, JRuby and SQuirrel SQL and find that 1) we are able to accurately identify self-admitted technical debt, significantly outperforming the current state-of-the-art based on fixed keywords and phrases; 2) words related to sloppy code or mediocre source code quality are the best indicators of design debt, whereas words related to the need to complete a partially implemented requirement in the future are the best indicators of requirement debt; and 3) we can achieve 90 percent of the best classification performance, using as little as 23 percent of the comments for both design and requirement self-admitted technical debt, and 80 percent of the best performance, using as little as 9 and 5 percent of the comments for design and requirement self-admitted technical debt, respectively. The last finding shows that the proposed approach can achieve a good accuracy even with a relatively small training dataset.},
keywords={computer crime;Java;natural language processing;project management;public domain software;software maintenance;software management;software quality;SQL;Natural Language Processing;requirement debt;self-admitted technical debt detection;open source projects;source code quality;source code comment classification;NLP;design debt;Software;Natural language processing;Manuals;Entropy;Unified modeling language;Java;Structured Query Language;Technical debt;source code comments;natural language processing;empirical study},
doi={10.1109/TSE.2017.2654244},
ISSN={0098-5589},
month={Nov},}
@ARTICLE{4569847,
author={J. Ontrup and H. Ritter and S. W. Scholz and R. Wagner},
journal={IEEE Transactions on Knowledge and Data Engineering},
title={Detecting, Assessing and Monitoring Relevant Topics in Virtual Information Environments},
year={2009},
volume={21},
number={3},
pages={415-427},
abstract={The ability to assess the relevance of topics and related sources in information-rich environments is a key to success when scanning business environments. This paper introduces a hybrid system to support managerial information gathering. The system is made up of three components: 1) a hierarchical hyperbolic SOM for structuring the information environment and visualizing the intensity of news activity with respect to identified topics, 2) a spreading activation network for the selection of the most relevant information sources with respect to an already existing knowledge infrastructure, and 3) measures of interestingness for association rules as well as statistical testing facilitates the monitoring of already identified topics. Embedding the system by a framework describing three modes of human information seeking behavior endorses an active organization, exploration and selection of information that matches the needs of decision makers in all stages of the information gathering process. By applying our system in the domain of the hotel industry we demonstrate how typical information gathering tasks are supported. Moreover, we present an empirical study investigating the effectiveness and efficiency of the visualization framework of our system.},
keywords={data mining;data visualisation;decision making;hotel industry;information management;information needs;knowledge management;monitoring;self-organising feature maps;statistical testing;virtual reality;virtual information environment scanning;topic monitoring;topic assessment;topic detection;business environment;information gathering management;hierarchical hyperbolic SOM;activation network;knowledge infrastructure;association rule;statistical testing;human information seeking behavior;decision making;hotel industry;Business;Environmental management;Cognitive science;Visualization;Monitoring;Humans;World Wide Web;Knowledge management;Information management;Association rules;Human information processing;Data and knowledge visualization;Clustering;Graphical user interfaces;Search process;Information Search and Retrieval;Information Storage and Retrieval;Information Technology;Text mining;Database Applications;Database Management;Information Technology and Systems;Human information processing;Data and knowledge visualization;Clustering;Graphical user interfaces;Search process;Information Search and Retrieval;Information Storage and Retrieval;Information Technology;Text mining;Database Applications;Database Management;Information Technology and Systems},
doi={10.1109/TKDE.2008.149},
ISSN={1041-4347},
month={March},}
@ARTICLE{5740889,
author={X. Ding and H. Jin},
journal={IEEE Transactions on Knowledge and Data Engineering},
title={Efficient and Progressive Algorithms for Distributed Skyline Queries over Uncertain Data},
year={2012},
volume={24},
number={8},
pages={1448-1462},
abstract={The skyline operator has received considerable attention from the database community, due to its importance in many applications including multicriteria decision making, preference answering, and so forth. In many applications where uncertain data are inherently exist, i.e., data collected from different sources in distributed locations are usually with imprecise measurements, and thus exhibit kind of uncertainty. Taking into account the network delay and economic cost associated with sharing and communicating large amounts of distributed data over an internet, an important problem in this scenario is to retrieve the global skyline tuples from all the distributed local sites with minimum communication cost. Based on the well-known notation of the probabilistic skyline query over centralized uncertain data, in this paper, we propose the notation of distributed skyline queries over uncertain data. Furthermore, two communication- and computation-efficient algorithms are proposed to retrieve the qualified skylines from distributed local sites. Extensive experiments have been conducted to verify the efficiency, the effectiveness and the progressiveness of our algorithms with both the synthetic and real data sets.},
keywords={decision making;distributed databases;Internet;probability;query processing;distributed skyline queries;skyline operator;multicriteria decision making;preference answering;network delay;economic cost;distributed data sharing;Internet;global skyline tuples;distributed local sites;minimum communication cost;probabilistic skyline query;centralized uncertain data;computation-efficient algorithm;communication-efficient algorithm;Distributed databases;Algorithm design and analysis;Servers;Bandwidth;Uncertainty;Probabilistic logic;Skyline;distributed database;uncertain data.},
doi={10.1109/TKDE.2011.77},
ISSN={1041-4347},
month={Aug},}
@ARTICLE{7029106,
author={J. Sroka and A. Panasiuk and K. Stencel and J. Tyszkiewicz},
journal={IEEE Transactions on Knowledge and Data Engineering},
title={Translating Relational Queries into Spreadsheets},
year={2015},
volume={27},
number={8},
pages={2291-2303},
abstract={Spreadsheets are among the most commonly used applications for data management and analysis. They combine data processing with very diverse supplementary features: statistics, visualization, reporting, linear programming solvers, Web queries periodically downloading data from external sources, etc. However, the spreadsheet paradigm of computation still lacks sufficient analysis. In this article, we demonstrate that a spreadsheet can implement all data transformations definable in SQL, merely by utilizing spreadsheet formulas. We provide a query compiler, which translates any given SQL query into a worksheet of the same semantics, including NULL values. Thereby, database operations become available to the users who do not want to migrate to a database. They can define their queries using a high-level language and then get their execution plans in a plain vanilla spreadsheet. The functions available in spreadsheets impose limitations on the algorithms one can implement. In this paper, we offer O(n log<sup>2</sup> n) sorting spreadsheet, using a non-constant number of rows, and, surprisingly, Depth-First-Search and Breadth-First-Search on graphs.},
keywords={computational complexity;data analysis;Internet;program compilers;program interpreters;query languages;query processing;relational databases;spreadsheet programs;SQL;relational query translation;data management;data analysis;data processing;linear programming solvers;Web query;statistics;data visualization;data reporting;SQL query;NULL values;database operations;high-level language;plain vanilla spreadsheet;O(n log2 n) sorting spreadsheet;depth-first-search;breadth-first-search;query compiler;Databases;Algebra;Semantics;Data visualization;Sorting;High level languages;Standards;Relational databases;physical database design prototypes;spreadsheets;query languages;Relational databases;physical database design prototypes;spreadsheets;query languages},
doi={10.1109/TKDE.2015.2397440},
ISSN={1041-4347},
month={Aug},}
@ARTICLE{6778767,
author={G. Qi and C. C. Aggarwal and T. S. Huang},
journal={IEEE Transactions on Knowledge and Data Engineering},
title={Breaking the Barrier to Transferring Link Information across Networks},
year={2015},
volume={27},
number={7},
pages={1741-1753},
abstract={Link prediction is one of the most fundamental problems in graph modeling and mining. It has been studied in a wide range of scenarios, from uncovering missing links between different entities in databases, to recommending relations between people in social networks. In this problem, we wish to predict unseen links in a growing target network by exploiting existing structures in source networks. Most of the existing methods often assume that abundant links are available in the target network to build a model for link prediction. However, in many scenarios, the target network may be too sparse to enable robust inference process, which makes link prediction challenging with the paucity of link data. On the other hand, in many cases, other (more densely linked) auxiliary networks can be available that contains similar link structure relevant to that in the target network. The linkage information in the existing networks can be used in conjunction with the node attribute information in both networks in order to make more accurate link recommendations. Thus, this paper proposes the use of learning methods to perform link inference by transferring the link information from the source network to the target network. We also note that the source network may contain the link information irrelevant to the target network. This leads to cross-network bias between the networks, which makes the link model built upon the source network misaligned with the link structure of the target network. Therefore, we re-sample the source network to rectify such cross-network bias by maximizing the cross-network relevance measured by the node attributes, as well as preserving as rich link information as possible to avoid the loss of source link structure caused by the re-sampling algorithm. The link model based on the re-sampled source network can make more accurate link predictions on the target network with aligned link structures across the networks. We present experimental results illustrating the effectiveness of the approach.},
keywords={inference mechanisms;learning (artificial intelligence);network theory (graphs);sampling methods;re-sampled source network;re-sampling algorithm;link model;cross-network bias;link information;link inference;learning methods;link recommendations;node attribute information;linkage information;link structure;densely-linked auxiliary networks;robust inference process;abundant links;target network;social networks;link prediction;link information transfer;Predictive models;Couplings;Social network services;Vectors;Training;Data mining;Prediction algorithms;Link prediction;link transfer;cross-network bias;node attribution;link richness},
doi={10.1109/TKDE.2014.2313871},
ISSN={1041-4347},
month={July},}
@ARTICLE{8374422,
author={N. Moustafa and E. Adi and B. Turnbull and J. Hu},
journal={IEEE Access},
title={A New Threat Intelligence Scheme for Safeguarding Industry 4.0 Systems},
year={2018},
volume={6},
number={},
pages={32910-32924},
abstract={Industry 4.0 represents the fourth phase of industry and manufacturing revolution, unique in that it provides Internet-connected smart systems, including automated factories, organizations, development on demand, and `just-in-time' development. Industry 4.0 includes the integration of cyber-physical systems (CPSs), Internet of Things (IoT), cloud and fog computing paradigms for developing smart systems, smart homes, and smart cities. Given Industry 4.0 is comprised sensor fields, actuators, fog and cloud processing paradigms, and network systems, designing a secure architecture faces two major challenges: handling heterogeneous sources at scale and maintaining security over a large, disparate, data-driven system that interacts with the physical environment. This paper addresses these challenges by proposing a new threat intelligence scheme that models the dynamic interactions of industry 4.0 components including physical and network systems. The scheme consists of two components: a smart management module and a threat intelligence module. The smart data management module handles heterogeneous data sources, one of the foundational requirements for interacting with an Industry 4.0 system. This includes data to and from sensors, actuators, in addition to other forms of network traffic. The proposed threat intelligence technique is designed based on beta mixture-hidden Markov models (MHMMs) for discovering anomalous activities against both physical and network systems. The scheme is evaluated on two well-known datasets: the CPS dataset of sensors and actuators and the UNSW-NB15 dataset of network traffic. The results reveal that the proposed technique outperforms five peer mechanisms, suggesting its effectiveness as a viable deployment methodology in real-Industry 4.0 systems.},
keywords={cloud computing;factory automation;hidden Markov models;intelligent manufacturing systems;Internet;Internet of Things;just-in-time;production engineering computing;security of data;telecommunication traffic;cloud processing paradigms;network systems;disparate data-driven system;physical environment;new threat intelligence scheme;smart management module;threat intelligence module;smart data management module;heterogeneous data sources;actuators;network traffic;threat intelligence technique;safeguarding Industry 4.0 systems;manufacturing revolution;Internet-connected smart systems;just-in-time development;cyber-physical systems;smart homes;smart cities;security;fog computing;Industries;Manufacturing;Internet of Things;Cloud computing;Actuators;Security;Cyber-physical systems;Industry 4.0;threat intelligence;cyber-attacks;cyber-physical systems (CPS);Internet of Things (IoT);cloud;fog;beta mixture-hidden Markov models (MHMM)},
doi={10.1109/ACCESS.2018.2844794},
ISSN={2169-3536},
month={},}
@ARTICLE{6544187,
author={M. Long and J. Wang and G. Ding and D. Shen and Q. Yang},
journal={IEEE Transactions on Knowledge and Data Engineering},
title={Transfer Learning with Graph Co-Regularization},
year={2014},
volume={26},
number={7},
pages={1805-1818},
abstract={Transfer learning is established as an effective technology to leverage rich labeled data from some source domain to build an accurate classifier for the target domain. The basic assumption is that the input domains may share certain knowledge structure, which can be encoded into common latent factors and extracted by preserving important property of original data, e.g., statistical property and geometric structure. In this paper, we show that different properties of input data can be complementary to each other and exploring them simultaneously can make the learning model robust to the domain difference. We propose a general framework, referred to as Graph Co-Regularized Transfer Learning (GTL), where various matrix factorization models can be incorporated. Specifically, GTL aims to extract common latent factors for knowledge transfer by preserving the statistical property across domains, and simultaneously, refine the latent factors to alleviate negative transfer by preserving the geometric structure in each domain. Based on the framework, we propose two novel methods using NMF and NMTF, respectively. Extensive experiments verify that GTL can significantly outperform state-of-the-art learning methods on several public text and image datasets.},
keywords={graph theory;image classification;knowledge management;learning (artificial intelligence);matrix decomposition;text analysis;labeled data;source domain;knowledge structure;input data;domain difference;graph coregularized transfer learning;matrix factorization models;GTL;latent factors;knowledge transfer;statistical property;geometric structure;NMF;NMTF;public text datasets;image datasets;classifier;Feature extraction;Knowledge transfer;Optimization;Matrix decomposition;Data mining;Robustness;Bridges;Computing Methodologies;Artificial Intelligence;Learning;Knowledge acquisition;Information Technology and Systems;Database Management;Database Applications;Feature extraction or construction;Transfer learning;negative transfer;graph regularization;matrix factorization;text mining;image classification},
doi={10.1109/TKDE.2013.97},
ISSN={1041-4347},
month={July},}
@ARTICLE{7079484,
author={T. Kuhn and M. Dumontier},
journal={IEEE Transactions on Knowledge and Data Engineering},
title={Making Digital Artifacts on the Web Verifiable and Reliable},
year={2015},
volume={27},
number={9},
pages={2390-2400},
abstract={The current Web has no general mechanisms to make digital artifacts-such as datasets, code, texts, and images-verifiable and permanent. For digital artifacts that are supposed to be immutable, there is moreover no commonly accepted method to enforce this immutability. These shortcomings have a serious negative impact on the ability to reproduce the results of processes that rely on Web resources, which in turn heavily impacts areas such as science where reproducibility is important. To solve this problem, we propose trusty URIs containing cryptographic hash values. We show how trusty URIs can be used for the verification of digital artifacts, in a manner that is independent of the serialization format in the case of structured data files such as nanopublications. We demonstrate how the contents of these files become immutable, including dependencies to external digital artifacts and thereby extending the range of verifiability to the entire reference tree. Our approach sticks to the core principles of the Web, namely openness and decentralized architecture, and is fully compatible with existing standards and protocols. Evaluation of our reference implementations shows that these design goals are indeed accomplished by our approach, and that it remains practical even for very large files.},
keywords={cryptography;data structures;Internet;trusted computing;digital artifacts;Web resources;trusty URI;cryptographic hash values;structured data files;nanopublications;reference tree;design goals;Resource description framework;Cryptography;Standards;Semantics;Protocols;Concrete;Abstracts;;Decentralized systems;data publishing;Semantic Web;linked data;resource description framework;nanopublications},
doi={10.1109/TKDE.2015.2419657},
ISSN={1041-4347},
month={Sept},}
@ARTICLE{7061528,
author={M. Fang and J. Yin and X. Zhu and C. Zhang},
journal={IEEE Transactions on Knowledge and Data Engineering},
title={TrGraph: Cross-Network Transfer Learning via Common Signature Subgraphs},
year={2015},
volume={27},
number={9},
pages={2536-2549},
abstract={In this paper, we present a novel transfer learning framework for network node classification. Our objective is to accurately predict the labels of nodes in a target network by leveraging information from an auxiliary source network. Such a transfer learning framework is potentially useful for broader areas of network classification, where emerging new networks might not have sufficient labeled information because node labels are either costly to obtain or simply not available, whereas many established networks from related domains are available to benefit the learning. In reality, the source and the target networks may not share common nodes or connections, so the major challenge of cross-network transfer learning is to identify knowledge/patterns transferable between networks and potentially useful to support cross-network learning. In this work, we propose to learn common signature subgraphs between networks, and use them to construct new structure features for the target network. By combining the original node content features and the new structure features, we develop an iterative classification algorithm, TrGraph, that utilizes label dependency to jointly classify nodes in the target network. Experiments on real-world networks demonstrate that TrGraph achieves the superior performance compared to the state-of-the-art baseline methods, and transferring generalizable structure information can indeed improve the node classification accuracy.},
keywords={computer networks;learning (artificial intelligence);TrGraph;cross-network transfer learning;common signature subgraphs;transfer learning framework;network node classification;information leveraging;auxiliary source network;network classification;cross-network learning;iterative classification algorithm;baseline methods;Knowledge engineering;Accuracy;Knowledge transfer;Social network services;Data mining;Prediction algorithms;Classification algorithms;Transfer learning;node classification;networked data;Transfer learning;node classification;networked data},
doi={10.1109/TKDE.2015.2413789},
ISSN={1041-4347},
month={Sept},}
@ARTICLE{7976300,
author={N. M. F. Qureshi and D. R. Shin and I. F. Siddiqui and B. S. Chowdhry},
journal={IEEE Access},
title={Storage-Tag-Aware Scheduler for Hadoop Cluster},
year={2017},
volume={5},
number={},
pages={13742-13755},
abstract={Big data analytics has simplified the processing complexity of extremely large data sets through ecosystems, such as Hadoop, MapR, and Cloudera. Apache Hadoop is an open-source ecosystem that manages large data sets in a distributed environment. MapReduce is a programming model that processes massive amount of unstructured data sets over Hadoop cluster. Recently, Hadoop enhances its homogeneous storage function to heterogeneous storage and stores data sets into multiple storage media, i.e., SSD, RAM, and DISK. This development increases the performance of data block placement strategy and allows a client to store large data sets into multiple storage media efficiently than homogeneous storage. However, this evolution increases the consumption of computing capacity and memory usage over MapReduce job scheduling. The scheduler processes MapReduce job into homogeneous container having configuration of CPU, memory, DISK volume, and network I/O, and accesses, processes, and stores data sets over heterogeneous storage media. This produces a processing latency of locating and pairing source data set to MapReduce tasks and results an abnormal high consumption of computing capacity and memory usage in a Datanode. Similarly, when scheduler assigns MapReduce jobs to multiple Datanodes, the same processing latency can severely affect the performance of whole cluster. In this paper, we present Storage-Tag-Aware Scheduler (STAS) that reduces processing latency by scheduling MapReduce jobs into heterogeneous storage containers, i.e., SSD, DISK, and RAM container. STAS endorses job with a tag of storage media, such as JobSSD, JobDISK, and JobRAM and parses them into heterogeneous shared-queues, which assign processing configuration to enlist jobs. STAS manager then schedules shared-queue jobs into heterogeneous MapReduce containers and generates an output into storage media of the cluster. The experimental evaluation shows that STAS optimizes the consumption of computing capacity and memory usage efficiently than available schedulers in a Hadoop cluster.},
keywords={Big Data;data analysis;parallel processing;public domain software;scheduling;storage management;storage-tag-aware scheduler;STAS;Hadoop cluster;Big Data analytics;Apache Hadoop;open-source ecosystem;distributed environment;programming model;unstructured data set processesing;data block placement strategy;data set storage;multiple storage media;MapReduce job scheduling;CPU;memory;DISK volume;network I/O;heterogeneous storage media;Datanode;heterogeneous shared-queues;processing configuration;shared-queue jobs;heterogeneous MapReduce containers;Media;Containers;Random access memory;Schedules;Ecosystems;Memory management;Programming;Hadoop;HDFS;scheduler;MapReduce;storage-tier},
doi={10.1109/ACCESS.2017.2725318},
ISSN={2169-3536},
month={},}
@ARTICLE{5288524,
author={L. V. Subramaniam and A. A. Nanavati and S. Mukherjea},
journal={IEEE Transactions on Knowledge and Data Engineering},
title={Enriching One Taxonomy Using Another},
year={2010},
volume={22},
number={10},
pages={1415-1427},
abstract={Taxonomies, representing hierarchical data, are a key knowledge source in multiple disciplines. Information processing across taxonomies is not possible unless they are appropriately merged for commonalities and differences. For taxonomy merging, the first task is to identify common concepts between the taxonomies. Then, these common concepts along with their associated concepts in the two taxonomies need to be integrated. Doing this in a conflict-free manner is a challenging task and generally requires human intervention. In this paper, we explore the possibility of asymmetrically merging one taxonomy into another automatically. Given one or more source taxonomies and a destination taxonomy, modeled as directed acyclic graphs, we present intuitive algorithms that merge relevant portions of the source taxonomies into the destination taxonomy. We prove that our algorithms are conflict-free, information lossless, and scalable. We also define precision and recall measures for evaluating enriched taxonomies, such as T<sub>A</sub>, the result of merging two taxonomies, with T<sub>I</sub>, the ideal merger. Our experiments indicate the effectiveness of our approach.},
keywords={classification;data analysis;directed graphs;merging;information processing;taxonomy merging;directed acyclic graphs;hierarchical data;Taxonomy;Ontologies;Merging;Humans;Unified modeling language;Biology computing;Information processing;Object oriented modeling;Corporate acquisitions;Software algorithms;Taxonomy merging;graph merging algorithms.},
doi={10.1109/TKDE.2009.189},
ISSN={1041-4347},
month={Oct},}
@ARTICLE{8263608,
author={X. Chen and J. Wang and J. Zhou},
journal={IEEE Access},
title={Process Monitoring Based on Multivariate Causality Analysis and Probability Inference},
year={2018},
volume={6},
number={},
pages={6360-6369},
abstract={System security is one of the key challenges of the cyber-physical systems. Bayesian approach can estimate and predict the potentially harmful factors of the general system, but it has many limitations that can lead to undesirable effects in the complex systems. This paper presents a new modeling and monitoring framework to avoid the traditional Bayesian network disadvantage. A multivariate causal analysis method is proposed to establish a compact system structure. Combined with network parameter learning, we constructed a corresponding multivariate alarm predict graph model, in which the qualitative and quantitative relationships among the process variables are revealed distinctly. Then this model is used to accurately predict the future possible alarm events via the probability inference. Similarly, it also can be used to detect faults and find the source of the fault. The effectiveness of the proposed method is verified in public data sets and the Tenessee Eastman process. Simulation results show that the established causal relationship is completely consistent with the actual mechanism, and the alarm state of the critical variable is accurately predicted.},
keywords={alarm systems;Bayes methods;belief networks;causality;control engineering computing;data analysis;fault diagnosis;Gaussian processes;graph theory;learning (artificial intelligence);probability;process monitoring;production engineering computing;process monitoring;multivariate causality analysis;probability inference;system security;cyber-physical systems;Bayesian approach;complex systems;monitoring framework;multivariate causal analysis method;compact system structure;network parameter learning;graph model;qualitative relationships;quantitative relationships;process variables;future possible alarm events;Tenessee Eastman process;Bayesian network disadvantage;multivariate alarm;Data models;Bayes methods;Monitoring;Predictive models;Adaptation models;Complex systems;Computational modeling;Alarm prediction;multivariate causality analysis;process monitoring modeling;parameter learning},
doi={10.1109/ACCESS.2018.2795535},
ISSN={2169-3536},
month={},}
@ARTICLE{7448405,
author={A. Bitarafan and M. S. Baghshah and M. Gheisari},
journal={IEEE Transactions on Knowledge and Data Engineering},
title={Incremental Evolving Domain Adaptation},
year={2016},
volume={28},
number={8},
pages={2128-2141},
abstract={Almost all of the existing domain adaptation methods assume that all test data belong to a single stationary target distribution. However, in many real world applications, data arrive sequentially and the data distribution is continuously evolving. In this paper, we tackle the problem of adaptation to a continuously evolving target domain that has been recently introduced. We assume that the available data for the source domain are labeled but the examples of the target domain can be unlabeled and arrive sequentially. Moreover, the distribution of the target domain can evolve continuously over time. We propose the Evolving Domain Adaptation (EDA) method that first finds a new feature space in which the source domain and the current target domain are approximately indistinguishable. Therefore, source and target domain data are similarly distributed in the new feature space and we use a semi-supervised classification method to utilize both the unlabeled data of the target domain and the labeled data of the source domain. Since test data arrives sequentially, we propose an incremental approach both for finding the new feature space and for semi-supervised classification. Experiments on several real datasets demonstrate the superiority of our proposed method in comparison to the other recent methods.},
keywords={data handling;learning (artificial intelligence);pattern classification;incremental EDA method;data distribution;feature space;semisupervised classification;online learning;Distributed databases;Kernel;Manifolds;Face recognition;Training data;Lighting;Object recognition;Domain adaptation;evolving domains;semi-supervised learning;online learning},
doi={10.1109/TKDE.2016.2551241},
ISSN={1041-4347},
month={Aug},}
@ARTICLE{6363444,
author={T. Menzies and A. Butcher and D. Cok and A. Marcus and L. Layman and F. Shull and B. Turhan and T. Zimmermann},
journal={IEEE Transactions on Software Engineering},
title={Local versus Global Lessons for Defect Prediction and Effort Estimation},
year={2013},
volume={39},
number={6},
pages={822-834},
abstract={Existing research is unclear on how to generate lessons learned for defect prediction and effort estimation. Should we seek lessons that are global to multiple projects or just local to particular projects? This paper aims to comparatively evaluate local versus global lessons learned for effort estimation and defect prediction. We applied automated clustering tools to effort and defect datasets from the PROMISE repository. Rule learners generated lessons learned from all the data, from local projects, or just from each cluster. The results indicate that the lessons learned after combining small parts of different data sources (i.e., the clusters) were superior to either generalizations formed over all the data or local lessons formed from particular projects. We conclude that when researchers attempt to draw lessons from some historical data source, they should 1) ignore any existing local divisions into multiple sources, 2) cluster across all available data, then 3) restrict the learning of lessons to the clusters from other sources that are nearest to the test data.},
keywords={automatic test pattern generation;data mining;pattern clustering;defect prediction;effort estimation;global lessons;local lessons;automated clustering tools;PROMISE repository;data source;learned lesson generated rule;defect dataset;Estimation;Data models;Context;Java;Telecommunications;Measurement;Software;Data mining;clustering;defect prediction;effort estimation},
doi={10.1109/TSE.2012.83},
ISSN={0098-5589},
month={June},}
@ARTICLE{8458104,
author={C. Liu and D. Yang and X. Zhang and B. Ray and M. M. Rahman},
journal={IEEE Access},
title={Recommending GitHub Projects for Developer Onboarding},
year={2018},
volume={6},
number={},
pages={52082-52094},
abstract={Open-source platform (e.g., GitHub) creates a tremendous opportunity for developers to learn and build experience. Contribution to open source can be rewarding for developers and advocates the evolutionary progress of the open-source software. However, finding a suitable project to contribute can be intimidating for developers because of the enormous possible choices. Due to various social and technical barriers, developers might fail to contribute successfully. Frequent unsuccessful onboarding hampers not only developers' individual advancement but also the evolutionary progress of open-source projects. To mitigate developers' costly efforts for onboarding, we propose a learning-to-rank model, neural network for list-wise ranking (NNLRank), to recommend projects that developers are likely to contribute. NNLRank leverages project features and developers' experience to recommend projects for onboarding. We develop an efficient approach to optimize the neural network where we leverage a list-wise loss function which intends to minimize the difference between the predicted projects list and the ground-truth list preferred by developers. We evaluate NNLRank with 2044 successful onboarding decisions from GitHub and compare it with three standard learning-to-rank models and a prior onboarding tool. Experimental results show that NNLRank can provide effective and efficient onboarding recommendation to developers, substantially outperforming the previous models.},
keywords={Internet;learning (artificial intelligence);neural nets;project management;public domain software;recommender systems;GitHub projects;developer onboarding;open-source platform;open-source software;open-source projects;learning-to-rank model;onboarding recommendation;neural network for list-wise ranking;NNLRank;Artificial neural networks;Tools;Feature extraction;Standards;Recommender systems;Ecosystems;Developer onboarding;recommender system;learning to rank},
doi={10.1109/ACCESS.2018.2869207},
ISSN={2169-3536},
month={},}
@ARTICLE{8239661,
author={H. Shuai and C. Shen and D. Yang and Y. C. Lan and W. Lee and P. S. Yu and M. Chen},
journal={IEEE Transactions on Knowledge and Data Engineering},
title={A Comprehensive Study on Social Network Mental Disorders Detection via Online Social Media Mining},
year={2018},
volume={30},
number={7},
pages={1212-1225},
abstract={The explosive growth in popularity of social networking leads to the problematic usage. An increasing number of social network mental disorders (SNMDs), such as Cyber-Relationship Addiction, Information Overload, and Net Compulsion, have been recently noted. Symptoms of these mental disorders are usually observed passively today, resulting in delayed clinical intervention. In this paper, we argue that mining online social behavior provides an opportunity to actively identify SNMDs at an early stage. It is challenging to detect SNMDs because the mental status cannot be directly observed from online social activity logs. Our approach, new and innovative to the practice of SNMD detection, does not rely on self-revealing of those mental factors via questionnaires in Psychology. Instead, we propose a machine learning framework, namely, Social Network Mental Disorder Detection (SNMDD), that exploits features extracted from social network data to accurately identify potential cases of SNMDs. We also exploit multi-source learning in SNMDD and propose a new SNMD-based Tensor Model (STM) to improve the accuracy. To increase the scalability of STM, we further improve the efficiency with performance guarantee. Our framework is evaluated via a user study with 3,126 online social network users. We conduct a feature analysis, and also apply SNMDD on large-scale datasets and analyze the characteristics of the three SNMD types. The results manifest that SNMDD is promising for identifying online social network users with potential SNMDs.},
keywords={data mining;feature extraction;learning (artificial intelligence);psychology;social networking (online);social networking;mental status;online social activity;mental factors;social network data;online social network users;social network mental disorders detection;online social media mining;social network mental disorder detection;cyber-relationship addiction;information overload;net compulsion;delayed clinical intervention;online social behavior mining;online social activity logs;machine learning framework;SNMDD detection;SNMD-based tensor model;STM;feature analysis;Social network services;Feature extraction;Mental disorders;Psychology;Data mining;Tensile stress;Internet;Tensor factorization acceleration;online social network;mental disorder detection;feature extraction},
doi={10.1109/TKDE.2017.2786695},
ISSN={1041-4347},
month={July},}
@ARTICLE{8304571,
author={Z. Liao and D. He and Z. Chen and X. Fan and Y. Zhang and S. Liu},
journal={IEEE Access},
title={Exploring the Characteristics of Issue-Related Behaviors in GitHub Using Visualization Techniques},
year={2018},
volume={6},
number={},
pages={24003-24015},
abstract={Feedback from software users, such as bug reports, is vital in the management of software projects. In GitHub, the feedback is typically expressed as new issues. Through filing issue reports, users may help identify and fix bugs, document software code, and enhance software quality via feature requests. In this paper, we aim at investigating some characteristics of issues to facilitate issue management and software management. We investigate the important degrees of behaviors that are related to issues in popular projects to assess the importance of issues in GitHub and analyze the effectiveness of issue labeling for issue handling. Then, we explore the patterns of issue commits over time in popular projects based on visual analysis and obtain the following results: we find that the behaviors that are related to issues play important roles in the GitHub. We also find that the time distribution of issue commits follows a three-period development model, which approximately corresponds to the project life cycle. These results may provide a new knowledge about issues that can help managers manage and allocate project resources more effectively and even reduce software failures.},
keywords={data visualisation;program debugging;project management;public domain software;software development management;software maintenance;software management;software quality;visualization techniques;software failures;project resources;project life cycle;visual analysis;issue commits;issue handling;issue labeling;software management;issue management;software quality;document software code;bugs;software projects;bug reports;software users;GitHub;issue-related behaviors;Software;Data visualization;Analytical models;Visualization;Labeling;Computer bugs;Sentiment analysis;Open-source software community;project development model;visual analysis;issue commit;software management},
doi={10.1109/ACCESS.2018.2810295},
ISSN={2169-3536},
month={},}
@ARTICLE{7140733,
author={A. Akusok and K. Björk and Y. Miche and A. Lendasse},
journal={IEEE Access},
title={High-Performance Extreme Learning Machines: A Complete Toolbox for Big Data Applications},
year={2015},
volume={3},
number={},
pages={1011-1025},
abstract={This paper presents a complete approach to a successful utilization of a high-performance extreme learning machines (ELMs) Toolbox for Big Data. It summarizes recent advantages in algorithmic performance; gives a fresh view on the ELM solution in relation to the traditional linear algebraic performance; and reaps the latest software and hardware performance achievements. The results are applicable to a wide range of machine learning problems and thus provide a solid ground for tackling numerous Big Data challenges. The included toolbox is targeted at enabling the full potential of ELMs to the widest range of users.},
keywords={Big Data;learning (artificial intelligence);linear algebra;extreme learning machines;Big Data;linear algebraic performance;ELM Toolbox;feedforward neural networks;Learning systems;Performance evaluation;Machine learning;Learning systems;Supervised learning;Machine learning;Prediction methods;Predictive models;Neural networks;Artificial neural networks;Feedforward neural networks;Radial basis function networks;Computer applications;Scientific computing;Performance analysis;High performance computing Software;Open source software;Utility programs},
doi={10.1109/ACCESS.2015.2450498},
ISSN={2169-3536},
month={},}
@ARTICLE{8119784,
author={F. Gao and F. Ma and J. Wang and J. Sun and E. Yang and H. Zhou},
journal={IEEE Access},
title={Visual Saliency Modeling for River Detection in High-Resolution SAR Imagery},
year={2018},
volume={6},
number={},
pages={1000-1014},
abstract={Accurate detection of rivers plays a significant role in water conservancy construction and ecological protection, where airborne synthetic aperture radar (SAR) data have already become one of the main sources. However, extracting river information from radar data efficiently and accurately still remains an open problem. The existing methods for detecting rivers are typically based on rivers' edges, which are easily mixed with those of artificial buildings or farmland. In addition, pixel-based image processing approaches cannot meet the requirement of real-time processing. Inspired by the feature integration and target recognition capabilities of biological vision systems, in this paper, we present a hierarchical method for automated detection of river networks in the high-resolution SAR data using biologically visual saliency modeling. For effective saliency detection, the original image is first over-segmented into a set of primitive superpixels. A visual feature set is designed to extract a regional feature histogram, which is then quantized based on the optimal parameters learned from the labeled SAR images. Afterward, three saliency measurements based on the specificity of the rivers in the SAR images are proposed to generate a single layer saliency map, i.e., local region contrast, boundary connectivity, and edge density. Finally, by exploiting belief propagation, we propose a multi-layer saliency fusion approach to derive a high-quality saliency map. Extensive experimental results on three airborne SAR image data sets with the ground truth demonstrate that the proposed saliency model consistently outperforms the existing saliency target detection models.},
keywords={airborne radar;feature extraction;geophysical image processing;geophysics computing;hydrological techniques;image colour analysis;image fusion;image resolution;image segmentation;object detection;radar imaging;rivers;synthetic aperture radar;river networks;biologically visual saliency modeling;visual feature set;regional feature histogram;labeled SAR images;saliency measurements;single layer saliency map;local region contrast;edge density;multilayer saliency fusion approach;high-quality saliency map;airborne SAR image data sets;river detection;high-resolution SAR imagery;water conservancy construction;ecological protection;airborne synthetic aperture radar data;pixel-based image processing approaches;biological vision systems;hierarchical method;automated detection;saliency detection;saliency target detection models;belief propagation;Feature extraction;Synthetic aperture radar;Rivers;Merging;Visualization;Filtering algorithms;Dogs;Synthetic aperture radar (SAR);remote sensing;rivers;object detection;biological system modeling},
doi={10.1109/ACCESS.2017.2777444},
ISSN={2169-3536},
month={},}
@ARTICLE{5551133,
author={D. Dey and V. Mookerjee and D. Liu},
journal={IEEE Transactions on Knowledge and Data Engineering},
title={Efficient Techniques for Online Record Linkage},
year={2011},
volume={23},
number={3},
pages={373-387},
abstract={The need to consolidate the information contained in heterogeneous data sources has been widely documented in recent years. In order to accomplish this goal, an organization must resolve several types of heterogeneity problems, especially the entity heterogeneity problem that arises when the same real-world entity type is represented using different identifiers in different data sources. Statistical record linkage techniques could be used for resolving this problem. However, the use of such techniques for online record linkage could pose a tremendous communication bottleneck in a distributed environment (where entity heterogeneity problems are often encountered). In order to resolve this issue, we develop a matching tree, similar to a decision tree, and use it to propose techniques that reduce the communication overhead significantly, while providing matching decisions that are guaranteed to be the same as those obtained using the conventional linkage technique. These techniques have been implemented, and experiments with real-world and synthetic databases show significant reduction in communication overhead.},
keywords={distributed databases;Internet;online record linkage;data sources;statistical record linkage techniques;communication bottleneck;distributed environment;synthetic databases;Couplings;Companies;Insurance;Standards organizations;Distributed databases;Record linkage;entity matching;sequential decision making;decision tree;data heterogeneity.},
doi={10.1109/TKDE.2010.134},
ISSN={1041-4347},
month={March},}
@ARTICLE{7530818,
author={M. Tsytsarau and T. Palpanas},
journal={IEEE Transactions on Knowledge and Data Engineering},
title={Managing Diverse Sentiments at Large Scale},
year={2016},
volume={28},
number={11},
pages={3028-3040},
abstract={The large-scale aggregation and analysis of user opinions is becoming increasingly relevant to a variety of applications, from detecting social mood on some political topics to tracking their sentiment changes related to events. The analysis of diverse sentiments is another important application, which becomes possible based on the ability of modern methods to capture sentiment polarity on various topics with high precision and on the ever-growing scale. Therefore, there is a need for a scalable way of sentiment aggregation with respect to the time dimension, which stores enough information to preserve diversity, and which allows statistically accurate analysis of sentiment trends and opinion shifts. In this paper, we are focusing on the novel problem of aggregating diverse sentiments at a large scale, based on data sources that are continuously updated. First, we develop a theoretical framework that models sentiment diversity (contradiction) and defines two types of contradictions, depending on the distribution of sentiments over time. Second, we introduce novel measures that capture sentiment diversity from aggregated sentiment statistics. Third, we develop robust and scalable indexing and storage methods for diverse sentiments. Finally, we propose an adaptive approach for identifying contradictions at different time scales. The experimental evaluation demonstrates the effectiveness of the proposed method of capturing contradictions and its superiority over relational databases in real-world scenarios.},
keywords={data aggregation;indexing;Internet;sentiment analysis;storage management;sentiment aggregation;sentiment analysis;sentiment polarity;sentiment diversity;indexing method;storage method;online platform;Sentiment analysis;Adaptation models;Context;Semantics;Blogs;Pragmatics;Biological system modeling;Sentiment aggregation;opinion mining;contradiction analysis},
doi={10.1109/TKDE.2016.2597848},
ISSN={1041-4347},
month={Nov},}
@ARTICLE{8478282,
author={Z. Yang and O. I. Raymond and C. Zhang and Y. Wan and J. Long},
journal={IEEE Access},
title={DFTerNet: Towards 2-bit Dynamic Fusion Networks for Accurate Human Activity Recognition},
year={2018},
volume={6},
number={},
pages={56750-56764},
abstract={Deep convolutional neural networks (DCNNs) are currently popular in human activity recognition (HAR) applications. However, in the face of modern artificial intelligence sensor-based games, many research achievements cannot be practically applied on portable devices (i.e., smart phone, VR/AR). DCNNs are typically resource-intensive and too large to be deployed on portable devices, and thus, this limits the practical application of complex activity detection. In addition, since portable devices do not possess high-performance graphic processing units, there is hardly any improvement in Action Game (ACT) experience. Besides, in order to deal with multi-sensor collaboration, all previous HAR models typically treated the representations from different sensor signal sources equally. However, distinct types of activities should adopt different fusion strategies. In this paper, a novel scheme is proposed. This scheme is used to train 2-bit CNNs with weights and activations constrained to {-0.5, 0, 0.5}. It takes into account the correlation between different sensor signal sources and the activity types. This model, which we refer to as DFTerNet, aims at producing a more reliable inference and better trade-offs for practical applications. It is known that quantization of weights and activations can substantially reduce memory size and use more efficient bitwise operations to replace floating or matrix operations to achieve much faster calculation and lower power consumption. Our basic idea is to exploit quantization of weights and activations directly in pre-trained filter banks and adopt dynamic fusion strategies for different activity types. Experiments demonstrate that by using a dynamic fusion strategy, it is possible to exceed the baseline model performance by up to ~5% on activity recognition data sets, such as the OPPORTUNITY and PAMAP2 data sets. Using the quantization method proposed, we were able to achieve performances closer to that of the full-precision counterpart. These results were also verified using the UniMiB-SHAR data set. In addition, the proposed method can achieve ~9x acceleration on CPUs and ~11x memory saving.},
keywords={computer games;convolution;feedforward neural nets;image recognition;intelligent sensors;learning (artificial intelligence);accurate human activity recognition;DCNNs;modern artificial intelligence sensor-based games;portable devices;high-performance graphic processing units;multisensor collaboration;2-bit CNNs;activations;DFTerNet;baseline model performance;activity recognition data sets;UniMiB-SHAR data set;quantization method;OPPORTUNITY data sets;PAMAP2 data sets;pre-trained filter banks;weights;ACT experience;sensor signal sources;action game experience;2-bit dynamic fusion networks;HAR models;deep convolutional neural networks;Quantization (signal);Activity recognition;Convolutional neural networks;Computational modeling;Games;Memory management;Human activity recognition;2-bit neural networks;dynamic fusion strategy},
doi={10.1109/ACCESS.2018.2873315},
ISSN={2169-3536},
month={},}
@ARTICLE{7829353,
author={J. Shen and E. Zheng and Z. Cheng and C. Deng},
journal={IEEE Access},
title={Assisting Attraction Classification by Harvesting Web Data},
year={2017},
volume={5},
number={},
pages={1600-1608},
abstract={Intelligent travel guide systems have grown increasingly popular in recent years. They also benefit a lot from the development of social media, resulting in a large amount of attractions uploaded by users. To tackle this, attractions should be real time classified by user-generated photos automatically to gain better user experience. However, in practice, the given label of photos and text ratings may be incomplete or missing. Moreover, recently, domain adaptation has been applied to deal with few labeled data. Thus, in this paper, we propose a novel framework for automatically attraction classification in leveraging web-harvesting data from search engine and the photos of attractions uploaded by users. Specifically, we assume that top-k web-harvesting images from search engines have correct labels. The classification problem is formulated as a regularized domain adaptation approach. Experiments conducted on the collected real-world data set demonstrated that the promising performance is gained over state-of-the art classification methods.},
keywords={information retrieval;Internet;pattern classification;search engines;social networking (online);travel industry;attraction classification;intelligent travel guide systems;social media;user-generated photos;user experience;text ratings;domain adaptation;Web-harvesting data;search engine;top-k Web-harvesting images;regularized domain adaptation;Search engines;Noise measurement;Visualization;Feature extraction;Adaptation models;Training data;Testing;Attraction classification;domain adaptation;web source},
doi={10.1109/ACCESS.2017.2656878},
ISSN={2169-3536},
month={},}
@ARTICLE{7887704,
author={V. Cosentino and J. L. Cánovas Izquierdo and J. Cabot},
journal={IEEE Access},
title={A Systematic Mapping Study of Software Development With GitHub},
year={2017},
volume={5},
number={},
pages={7173-7192},
abstract={Context: GitHub, nowadays the most popular social coding platform, has become the reference for mining Open Source repositories, a growing research trend aiming at learning from previous software projects to improve the development of new ones. In the last years, a considerable amount of research papers have been published reporting findings based on data mined from GitHub. As the community continues to deepen in its understanding of software engineering thanks to the analysis performed on this platform, we believe that it is worthwhile to reflect on how research papers have addressed the task of mining GitHub and what findings they have reported. Objective: The main objective of this paper is to identify the quantity, topic, and empirical methods of research works, targeting the analysis of how software development practices are influenced by the use of a distributed social coding platform like GitHub. Method: A systematic mapping study was conducted with four research questions and assessed 80 publications from 2009 to 2016. Results: Most works focused on the interaction around coding-related tasks and project communities. We also identified some concerns about how reliable were these results based on the fact that, overall, papers used small data sets and poor sampling techniques, employed a scarce variety of methodologies and/or were hard to replicate. Conclusions: This paper attested the high activity of research work around the field of Open Source collaboration, especially in the software domain, revealed a set of shortcomings and proposed some actions to mitigate them. We hope that this paper can also create the basis for additional studies on other collaborative activities (like book writing for instance) that are also moving to GitHub.},
keywords={data mining;public domain software;social networking (online);software engineering;coding-related tasks;project communities;sampling techniques;open source collaboration;software domain;distributed social coding platform;software engineering;open source repositories mining;systematic mapping study;GitHub;software development;Software;Conferences;Software engineering;Libraries;Systematics;Data mining;Collaboration;GitHub;open source software;systematic mapping study},
doi={10.1109/ACCESS.2017.2682323},
ISSN={2169-3536},
month={},}
@ARTICLE{8278215,
author={C. Vaduva and F. A. Georgescu and M. Datcu},
journal={IEEE Access},
title={Understanding Heterogeneous EO Datasets: A Framework for Semantic Representations},
year={2018},
volume={6},
number={},
pages={11184-11202},
abstract={Earth observation (EO) has become a valuable source of comprehensive, reliable, and persistent information for a wide number of applications. However, dealing with the complexity of land cover is sometimes difficult, as the variety of EO sensors reflects in the multitude of details recorded in several types of image data. Their properties dictate the category and nature of the perceptible land structures. The data heterogeneity hampers proper understanding, preventing the definition of universal procedures for content exploitation. The main shortcomings are due to the different human and sensor perception on objects, as well as to the lack of coincidence between visual elements and similarities obtained by computation. In order to bridge these sensory and semantic gaps, the paper presents a compound framework for EO image information extraction. The proposed approach acts like a common ground between the user's understanding, who is visually shortsighted to the visible domain, and the machines numerical interpretation of a much wider information. A hierarchical data representation is considered. At first, basic elements are automatically computed. Then, users can enforce their judgement on the data processing results until semantic structures are revealed. This procedure completes a user-machine knowledge transfer. The interaction is formalized as a dialogue, where communication is determined by a set of parameters guiding the computational process at each level of representation. The purpose is to maintain the data-driven observable connected to the level of semantics and to human awareness. The proposed concept offers flexibility and interoperability to users, allowing them to generate those results that best fit their application scenario. The experiments performed on different satellite images demonstrate the ability to increase the performances in case of semantic annotation by adjusting a set of parameters to the particularities of the analyzed data.},
keywords={data visualisation;feature extraction;geophysical image processing;land cover;remote sensing;semantic networks;heterogeneous EO datasets;semantic representations;earth observation;land cover;EO sensors;image data;data heterogeneity;sensor perception;visual elements;sensory gaps;semantic gaps;EO image information extraction;semantic structures;user-machine knowledge transfer;computational process;human awareness;semantic annotation;land structures;data processing;satellite images;Semantics;Sensors;Earth;Remote sensing;Bridges;Data mining;Feature extraction;Earth observation data understanding;human machine communication;image information mining;semantic gap;sensory gap;semantic representation},
doi={10.1109/ACCESS.2018.2801032},
ISSN={2169-3536},
month={},}
@ARTICLE{5557874,
author={C. Kim and K. Shim},
journal={IEEE Transactions on Knowledge and Data Engineering},
title={TEXT: Automatic Template Extraction from Heterogeneous Web Pages},
year={2011},
volume={23},
number={4},
pages={612-626},
abstract={World Wide Web is the most useful source of information. In order to achieve high productivity of publishing, the webpages in many websites are automatically populated by using the common templates with contents. The templates provide readers easy access to the contents guided by consistent structures. However, for machines, the templates are considered harmful since they degrade the accuracy and performance of web applications due to irrelevant terms in templates. Thus, template detection techniques have received a lot of attention recently to improve the performance of search engines, clustering, and classification of web documents. In this paper, we present novel algorithms for extracting templates from a large number of web documents which are generated from heterogeneous templates. We cluster the web documents based on the similarity of underlying template structures in the documents so that the template for each cluster is extracted simultaneously. We develop a novel goodness measure with its fast approximation for clustering and provide comprehensive analysis of our algorithm. Our experimental results with real-life data sets confirm the effectiveness and robustness of our algorithm compared to the state of the art for template detection algorithms.},
keywords={feature extraction;Internet;TEXT;automatic template extraction;heterogeneous web pages;World Wide Web;web applications;search engines;web documents;HTML;Clustering algorithms;Data mining;Data models;Merging;XML;Web pages;Template extraction;clustering;minimum description length principle;MinHash.},
doi={10.1109/TKDE.2010.140},
ISSN={1041-4347},
month={April},}
@ARTICLE{6860243,
author={R. Scandariato and J. Walden and A. Hovsepyan and W. Joosen},
journal={IEEE Transactions on Software Engineering},
title={Predicting Vulnerable Software Components via Text Mining},
year={2014},
volume={40},
number={10},
pages={993-1006},
abstract={This paper presents an approach based on machine learning to predict which components of a software application contain security vulnerabilities. The approach is based on text mining the source code of the components. Namely, each component is characterized as a series of terms contained in its source code, with the associated frequencies. These features are used to forecast whether each component is likely to contain vulnerabilities. In an exploratory validation with 20 Android applications, we discovered that a dependable prediction model can be built. Such model could be useful to prioritize the validation activities, e.g., to identify the components needing special scrutiny.},
keywords={data mining;learning (artificial intelligence);program verification;security of data;vulnerable software component;text mining;machine learning;security vulnerability;source code;Android application;Software;Predictive models;Measurement;Security;Androids;Humanoid robots;Text mining;Vulnerabilities;prediction model;machine learning},
doi={10.1109/TSE.2014.2340398},
ISSN={0098-5589},
month={Oct},}
@ARTICLE{8397161,
author={R. B. Melton and K. P. Schneider and E. Lightner and T. E. Mcdermott and P. Sharma and Y. Zhang and F. Ding and S. Vadari and R. Podmore and A. Dubey and R. W. Wies and E. G. Stephan},
journal={IEEE Access},
title={Leveraging Standards to Create an Open Platform for the Development of Advanced Distribution Applications},
year={2018},
volume={6},
number={},
pages={37361-37370},
abstract={Modern electric power distribution systems are data rich and include growing numbers of distributed energy resources and distribution automation. To take advantage of distribution automation and manage growing penetrations of distributed energy resources, distribution utilities need applications for planning and operations that use all available data and may incorporate distributed approaches to operate and control. The industry would benefit from distribution management applications based on a common platform that makes systems of each type interchangeable. This paper describes an approach to enabling cost-effective development and deployment of advanced applications for distribution system planning and operations based on development of an open-source, standards-based platform for application development called GridAPPS-D, which leverages data abstractions for application development based on standards, such as the distribution system common information model.},
keywords={power distribution planning;power engineering computing;power system management;public domain software;distribution system common information model;open platform;advanced distribution applications;modern electric power distribution systems;distributed energy resources;distribution automation;distribution utilities;distributed approaches;distribution management applications;cost-effective development;distribution system planning;standards-based platform;application development;open-source standards-based platform;GridAPPS-D;Unified modeling language;Data models;Planning;Standards;Distributed databases;Energy resources;Distribution system analysis;dynamic simulation;power modeling;power simulation;smart grid},
doi={10.1109/ACCESS.2018.2851186},
ISSN={2169-3536},
month={},}
@ARTICLE{5645620,
author={C. Chen and K. Yang and C. Chen and J. Ho},
journal={IEEE Transactions on Knowledge and Data Engineering},
title={BibPro: A Citation Parser Based on Sequence Alignment},
year={2012},
volume={24},
number={2},
pages={236-250},
abstract={Dramatic increase in the number of academic publications has led to growing demand for efficient organization of the resources to meet researchers' needs. As a result, a number of network services have compiled databases from the public resources scattered over the Internet. However, publications by different conferences and journals adopt different citation styles. It is an interesting problem to accurately extract metadata from a citation string which is formatted in one of thousands of different styles. It has attracted a great deal of attention in research in recent years. In this paper, based on the notion of sequence alignment, we present a citation parser called BibPro that extracts components of a citation string. To demonstrate the efficacy of BibPro, we conducted experiments on three benchmark data sets. The results show that BibPro achieved over 90 percent accuracy on each benchmark. Even with citations and associated metadata retrieved from the web as training data, our experiments show that BibPro still achieves a reasonable performance.},
keywords={citation analysis;Internet;meta data;BibPro;citation parser;sequence alignment;public resources;Internet;metadata extraction;citation string;World Wide Web;academic publications;Decision support systems;Information retrieval;Sequential analysis;Benchmark testing;Internet;Resource management;Web and internet services;Metadata;Data integration;digital libraries;information extraction;sequence alignment.},
doi={10.1109/TKDE.2010.231},
ISSN={1041-4347},
month={Feb},}
@ARTICLE{7801138,
author={S. Charalampidou and A. Ampatzoglou and A. Chatzigeorgiou and A. Gkortzis and P. Avgeriou},
journal={IEEE Transactions on Software Engineering},
title={Identifying Extract Method Refactoring Opportunities Based on Functional Relevance},
year={2017},
volume={43},
number={10},
pages={954-974},
abstract={`Extract Method' is considered one of the most frequently applied and beneficial refactorings, since the corresponding Long Method smell is among the most common and persistent ones. Although Long Method is conceptually related to the implementation of diverse functionalities within a method, until now, this relationship has not been utilized while identifying refactoring opportunities. In this paper we introduce an approach (accompanied by a tool) that aims at identifying source code chunks that collaborate to provide a specific functionality, and propose their extraction as separate methods. The accuracy of the proposed approach has been empirically validated both in an industrial and an open-source setting. In the former case, the approach was capable of identifying functionally related statements within two industrial long methods (approx. 500 LoC each), with a recall rate of 93 percent. In the latter case, based on a comparative study on open-source data, our approach ranks better compared to two well-known techniques of the literature. To assist software engineers in the prioritization of the suggested refactoring opportunities the approach ranks them based on an estimate of their fitness for extraction. The provided ranking has been validated in both settings and proved to be strongly correlated with experts' opinion.},
keywords={public domain software;software maintenance;open-source data;functional relevance;beneficial refactorings;diverse functionalities;specific functionality;open-source setting;functionally related statements;industrial long methods;refactoring opportunities;Extract Method;Long Method smell;source code chunks;Measurement;Open source software;Mathematics;Data mining;Computer science;Syntactics;Design tools and techniques;object-oriented programming;metrics/measurement},
doi={10.1109/TSE.2016.2645572},
ISSN={0098-5589},
month={Oct},}
@ARTICLE{8438448,
author={S. Guo and R. Chen and M. Wei and H. Li and Y. Liu},
journal={IEEE Access},
title={Ensemble Data Reduction Techniques and Multi-RSMOTE via Fuzzy Integral for Bug Report Classification},
year={2018},
volume={6},
number={},
pages={45934-45950},
abstract={Due to the unavoidable bugs appearing in the most of the software systems, bug resolution has become one of the most important activities in software maintenance. To decrease the time cost in manual work, text classification techniques are applied to automatically identify severity of bug reports. In this paper, we address the problem of low-quality and class imbalance for identifying the severity of bug reports. First, we combine feature selection with instance selection to simultaneously reduce the bug report dimension and the word dimension, which could get small-scale and high-quality reduced data set. Then, an improve random oversampling technique, named, RSMOTE, which is presented to weaken the imbalancedness degree of class distribution. Finally, to avoid the random over-sampling uncertainty of RSMOTE, we develop an ensemble learning algorithm, which is based on Choquet fuzzy integral, to combine multiple RSMOTE. We empirically investigate the performance of data reduction on ten data sets of three large open source projects, namely, Eclipse, Mozilla, and GNOME. The results show that our approach can effectively reduce the data scale and improve the performance of identifying the severity of bug reports.},
keywords={data reduction;feature selection;fuzzy set theory;learning (artificial intelligence);pattern classification;program debugging;public domain software;software maintenance;text analysis;ensemble data reduction techniques;multiRSMOTE;bug report classification;software systems;bug resolution;software maintenance;text classification techniques;class imbalance;random oversampling technique;RSMOTE;Choquet fuzzy integral;feature selection;instance selection;ensemble learning algorithm;Eclipse;Mozilla;GNOME;Computer bugs;Software;Training;Feature extraction;Classification algorithms;Uncertainty;Radio frequency;Mining software repositories;data reduction;imbalance distribution;fuzzy integral},
doi={10.1109/ACCESS.2018.2865780},
ISSN={2169-3536},
month={},}
@ARTICLE{6127835,
author={U. Raja and M. J. Tretter},
journal={IEEE Transactions on Software Engineering},
title={Defining and Evaluating a Measure of Open Source Project Survivability},
year={2012},
volume={38},
number={1},
pages={163-174},
abstract={In this paper, we define and validate a new multidimensional measure of Open Source Software (OSS) project survivability, called Project Viability. Project viability has three dimensions: vigor, resilience, and organization. We define each of these dimensions and formulate an index called the Viability Index (VI) to combine all three dimensions. Archival data of projects hosted at SourceForge.net are used for the empirical validation of the measure. An Analysis Sample (n=136) is used to assign weights to each dimension of project viability and to determine a suitable cut-off point for VI. Cross-validation of the measure is performed on a hold-out Validation Sample (n=96). We demonstrate that project viability is a robust and valid measure of OSS project survivability that can be used to predict the failure or survival of an OSS project accurately. It is a tangible measure that can be used by organizations to compare various OSS projects and to make informed decisions regarding investment in the OSS domain.},
keywords={project management;public domain software;software metrics;open source project survivability;multidimensional measure;open source software project survivability;project viability;vigor;resilience;organization;viability index;Software measurement;Indexes;Maintenance engineering;Evaluation framework;external validity;open source software;project evaluation;software measurement;software survivability.},
doi={10.1109/TSE.2011.39},
ISSN={0098-5589},
month={Jan},}
@ARTICLE{5936060,
author={A. Makanju and A. N. Zincir-Heywood and E. E. Milios},
journal={IEEE Transactions on Knowledge and Data Engineering},
title={A Lightweight Algorithm for Message Type Extraction in System Application Logs},
year={2012},
volume={24},
number={11},
pages={1921-1936},
abstract={Message type or message cluster extraction is an important task in the analysis of system logs in computer networks. Defining these message types automatically facilitates the automatic analysis of system logs. When the message types that exist in a log file are represented explicitly, they can form the basis for carrying out other automatic application log analysis tasks. In this paper, we introduce a novel algorithm for carrying out message type extraction from event log files. IPLoM, which stands for Iterative Partitioning Log Mining, works through a 4-step process. The first three steps hierarchically partition the event log into groups of event log messages or event clusters. In its fourth and final stage, IPLoM produces a message type description or line format for each of the message clusters. IPLoM is able to find clusters in data irrespective of the frequency of its instances in the data, it scales gracefully in the case of long message type patterns and produces message type descriptions at a level of abstraction, which is preferred by a human observer. Evaluations show that IPLoM outperforms similar algorithms statistically significantly.},
keywords={data mining;fault tolerant computing;iterative methods;pattern clustering;lightweight algorithm;message type extraction;system application logs;message cluster extraction;computer networks;automatic application log analysis tasks;IPLoM;iterative partitioning log mining;message type patterns;human observer;autonomic computing;Kernel;Data mining;Humans;Clustering algorithms;Buildings;Observers;Partitioning algorithms;Algorithms;experimentation;event log mining;fault management;clustering},
doi={10.1109/TKDE.2011.138},
ISSN={1041-4347},
month={Nov},}
@ARTICLE{6384533,
author={U. Kang and B. Meeder and E. E. Papalexakis and C. Faloutsos},
journal={IEEE Transactions on Knowledge and Data Engineering},
title={HEigen: Spectral Analysis for Billion-Scale Graphs},
year={2014},
volume={26},
number={2},
pages={350-362},
abstract={Given a graph with billions of nodes and edges, how can we find patterns and anomalies? Are there nodes that participate in too many or too few triangles? Are there close-knit near-cliques? These questions are expensive to answer unless we have the first several eigenvalues and eigenvectors of the graph adjacency matrix. However, eigensolvers suffer from subtle problems (e.g., convergence) for large sparse matrices, let alone for billion-scale ones. We address this problem with the proposed HEIGEN algorithm, which we carefully design to be accurate, efficient, and able to run on the highly scalable MAPREDUCE (HADOOP) environment. This enables HEIGEN to handle matrices more than 1;000 × larger than those which can be analyzed by existing algorithms. We implement HEIGEN and run it on the M45 cluster, one of the top 50 supercomputers in the world. We report important discoveries about nearcliques and triangles on several real-world graphs, including a snapshot of the Twitter social network (56 Gb, 2 billion edges) and the “YahooWeb” data set, one of the largest publicly available graphs (120 Gb, 1.4 billion nodes, 6.6 billion edges).},
keywords={data mining;graph theory;parallel machines;pattern recognition;spectral analysis;HEigen;spectral analysis;billion-scale graphs;matrices;M45 cluster;supercomputers;real-world graphs;Twitter social network;YahooWeb data set;publicly available graphs;graph mining;pattern discovery;eigensolver;open-source MapReduce framework;HADOOP platform;Eigenvalues and eigenfunctions;Twitter;Algorithm design and analysis;Sparse matrices;Symmetric matrices;LinkedIn;Web pages;Spectral analysis;MapReduce;Hadoop;HEigen;graph mining},
doi={10.1109/TKDE.2012.244},
ISSN={1041-4347},
month={Feb},}
@ARTICLE{6926828,
author={A. Sakti and G. Pesant and Y. Guéhéneuc},
journal={IEEE Transactions on Software Engineering},
title={Instance Generator and Problem Representation to Improve Object Oriented Code Coverage},
year={2015},
volume={41},
number={3},
pages={294-313},
abstract={Search-based approaches have been extensively applied to solve the problem of software test-data generation. Yet, test-data generation for object-oriented programming (OOP) is challenging due to the features of OOP, e.g., abstraction, encapsulation, and visibility that prevent direct access to some parts of the source code. To address this problem we present a new automated search-based software test-data generation approach that achieves high code coverage for unit-class testing. We first describe how we structure the test-data generation problem for unit-class testing to generate relevant sequences of method calls. Through a static analysis, we consider only methods or constructors changing the state of the class-under-test or that may reach a test target. Then we introduce a generator of instances of classes that is based on a family of means-of-instantiation including subclasses and external factory methods. It also uses a seeding strategy and a diversification strategy to increase the likelihood to reach a test target. Using a search heuristic to reach all test targets at the same time, we implement our approach in a tool, JTExpert, that we evaluate on more than a hundred Java classes from different open-source libraries. JTExpert gives better results in terms of search time and code coverage than the state of the art, EvoSuite, which uses traditional techniques.},
keywords={Java;object-oriented programming;program diagnostics;program testing;public domain software;instance generator;problem representation;object oriented code coverage;search-based approach;object-oriented programming;OOP;abstraction;encapsulation;visibility;source code;automated search-based software test-data generation approach;unit-class testing;method call sequences;static analysis;class-under-test;means-of-instantiation;seeding strategy;diversification strategy;search heuristic;JTExpert;Java class evaluation;open-source libraries;search time;EvoSuite;Testing;Complexity theory;Generators;Search problems;Java;Production facilities;Libraries;Automatic Test Data Generation;Search Based Software Testing;Unit Class Testing;Seeding Strategy;Diversification Strategy;Java Testing;Automatic test data generation;search based software testing;unit class testing;seeding strategy;diversification strategy;Java testing},
doi={10.1109/TSE.2014.2363479},
ISSN={0098-5589},
month={March},}
@ARTICLE{8344797,
author={K. Al-Sabahi and Z. Zuping and M. Nadher},
journal={IEEE Access},
title={A Hierarchical Structured Self-Attentive Model for Extractive Document Summarization (HSSAS)},
year={2018},
volume={6},
number={},
pages={24205-24212},
abstract={The recent advance in neural network architecture and training algorithms has shown the effectiveness of representation learning. The neural-network-based models generate better representation than the traditional ones. They have the ability to automatically learn the distributed representation for sentences and documents. To this end, we proposed a novel model that addresses several issues that are not adequately modeled by the previously proposed models, such as the memory problem and incorporating the knowledge of document structure. Our model uses a hierarchical structured self-attention mechanism to create the sentence and document embeddings. This architecture mirrors the hierarchical structure of the document and in turn enables us to obtain better feature representation. The attention mechanism provides extra source of information to guide the summary extraction. The new model treated the summarization task as a classification problem in which the model computes the respective probabilities of sentence-summary membership. The model predictions are broken up by several features such as information content, salience, novelty, and positional representation. The proposed model was evaluated on two well-known datasets, the CNN/Daily Mail and DUC 2002. The experimental results show that our model outperforms the current extractive state of the art by a considerable margin.},
keywords={learning (artificial intelligence);neural net architecture;probability;text analysis;document structure;hierarchical structured self-attention mechanism;feature representation;summary extraction;classification problem;sentence-summary membership;model predictions;positional representation;current extractive state;extractive document summarization;neural network architecture;training algorithms;representation learning;neural-network-based models;distributed representation;memory problem;Computational modeling;Computer architecture;Task analysis;Data mining;Recurrent neural networks;Feature extraction;Semantics;Long short-term memory;hierarchical structured self-attention;document summarization;abstract features;sentence embedding;document embedding},
doi={10.1109/ACCESS.2018.2829199},
ISSN={2169-3536},
month={},}
@ARTICLE{5989837,
author={C. Zhang and H. Jacobsen},
journal={IEEE Transactions on Software Engineering},
title={Mining Crosscutting Concerns through Random Walks},
year={2012},
volume={38},
number={5},
pages={1123-1137},
abstract={Inspired by our past manual aspect mining experiences, this paper describes a probabilistic random walk model to approximate the process of discovering crosscutting concerns (CCs) in the absence of the domain knowledge about the investigated application. The random walks are performed on the concept graphs extracted from the program sources to calculate metrics of “utilization” and “aggregation” for each of the program elements. We rank all the program elements based on these metrics and use a threshold to produce a set of candidates that represent crosscutting concerns. We implemented the algorithm as the Prism CC miner (PCM) and evaluated PCM on Java applications ranging from a small-scale drawing application to a medium-sized middleware application and to a large-scale enterprise application server. Our quantification shows that PCM is able to produce comparable results (95 percent accuracy for the top 125 candidates) with respect to the manual mining effort. PCM is also significantly more effective as compared to the conventional approach.},
keywords={aspect-oriented programming;data mining;graph theory;Java;middleware;probability;small-to-medium enterprises;crosscutting concerns mining;probabilistic random walk model;concept graphs;utilization metric;aggregation metric;program sources;program elements;Prism CC miner;Java applications;small-scale drawing application;medium-sized middleware application;large-scale enterprise application server;aspect mining;Phase change materials;Radiation detectors;Data mining;Manuals;Mathematical model;Computational modeling;Algorithm design and analysis;Aspect mining;mining crosscutting concerns},
doi={10.1109/TSE.2011.83},
ISSN={0098-5589},
month={Sept},}
@ARTICLE{8194845,
author={X. Liu and Z. Liu and G. Wang and Z. Cai and H. Zhang},
journal={IEEE Access},
title={Ensemble Transfer Learning Algorithm},
year={2018},
volume={6},
number={},
pages={2389-2396},
abstract={Transfer learning and ensemble learning are the new trends for solving the problem that training data and test data have different distributions. In this paper, we design an ensemble transfer learning framework to improve the classification accuracy when the training data are insufficient. First, a weightedresampling method for transfer learning is proposed, which is named TrResampling. In each iteration, the data with heavy weights in the source domain are resampled, and the TrAdaBoost algorithm is used to adjust the weights of the source data and target data. Second, three classic machine learning algorithms, namely, naive Bayes, decision tree, and SVM, are used as the base learners of TrResampling, where the base learner with the best performance is chosen for transfer learning. To illustrate the performance of TrResampling, the TrAdaBoost and decision tree are used for evaluation and comparison on 15 UCI data sets, TrAdaBoost, ARTL, and SVM are used for evaluation and comparison on five text data sets. According to the experimental results, our proposed TrResampling is superior to the state-of-the-art learning methods on UCI data sets and text data sets. In addition, TrResampling, bagging-based transfer learning algorithm, and MultiBoosting-based transfer learning algorithm (TrMultiBoosting) are assembled in the framework, and we compare the three ensemble transfer learning algorithms with TrAdaBoost to illustrate the framework's effective transfer ability.},
keywords={Bayes methods;decision trees;learning (artificial intelligence);pattern classification;support vector machines;ensemble transfer learning algorithm;training data;test data;ensemble transfer learning framework;TrResampling;TrAdaBoost algorithm;source data;target data;decision tree;base learner;text data sets;learning methods;bagging-based transfer learning algorithm;MultiBoosting-based transfer learning algorithm;effective transfer ability;UCI data sets;TrMultiBoosting;naive Bayes;SVM;Training data;Boosting;Machine learning algorithms;Bagging;Support vector machines;Training;Transfer learning;bagging;boosting;ensemble learning},
doi={10.1109/ACCESS.2017.2782884},
ISSN={2169-3536},
month={},}
@ARTICLE{8325299,
author={M. Kulin and T. Kazaz and I. Moerman and E. De Poorter},
journal={IEEE Access},
title={End-to-End Learning From Spectrum Data: A Deep Learning Approach for Wireless Signal Identification in Spectrum Monitoring Applications},
year={2018},
volume={6},
number={},
pages={18484-18501},
abstract={This paper presents end-to-end learning from spectrum data-an umbrella term for new sophisticated wireless signal identification approaches in spectrum monitoring applications based on deep neural networks. End-to-end learning allows to: 1) automatically learn features directly from simple wireless signal representations, without requiring design of hand-crafted expert features like higher order cyclic moments and 2) train wireless signal classifiers in one end-to-end step which eliminates the need for complex multi-stage machine learning processing pipelines. The purpose of this paper is to present the conceptual framework of end-to-end learning for spectrum monitoring and systematically introduce a generic methodology to easily design and implement wireless signal classifiers. Furthermore, we investigate the importance of the choice of wireless data representation to various spectrum monitoring tasks. In particular, two case studies are elaborated: 1) modulation recognition and 2) wireless technology interference detection. For each case study three convolutional neural networks are evaluated for the following wireless signal representations: temporal IQ data, the amplitude/phase representation, and the frequency domain representation. From our analysis, we prove that the wireless data representation impacts the accuracy depending on the specifics and similarities of the wireless signals that need to be differentiated, with different data representations resulting in accuracy variations of up to 29%. Experimental results show that using the amplitude/phase representation for recognizing modulation formats can lead to performance improvements up to 2% and 12% for medium to high SNR compared to IQ and frequency domain data, respectively. For the task of detecting interference, frequency domain representation outperformed amplitude/phase and IQ data representation up to 20%.},
keywords={feedforward neural nets;frequency-domain analysis;learning (artificial intelligence);modulation;radio spectrum management;radiofrequency interference;signal classification;signal representation;telecommunication computing;spectrum monitoring tasks;frequency domain representation;wireless data representation;wireless signals;spectrum data;deep learning approach;spectrum monitoring applications;end-to-end learning;simple wireless signal representations;end-to-end step;complex multistage machine learning processing pipelines;wireless signal classifiers;amplitude representation;wireless signal identification approaches;deep neural networks;modulation recognition;wireless technology interference detection;convolutional neural networks;phase representation;Wireless communication;Wireless sensor networks;Monitoring;Machine learning;Interference;Modulation;Pipelines;Big spectrum data;spectrum monitoring;end-to-end learning;deep learning;convolutional neural networks;wireless signal identification;IoT},
doi={10.1109/ACCESS.2018.2818794},
ISSN={2169-3536},
month={},}
@ARTICLE{8320362,
author={S. Walker-Roberts and M. Hammoudeh and A. Dehghantanha},
journal={IEEE Access},
title={A Systematic Review of the Availability and Efficacy of Countermeasures to Internal Threats in Healthcare Critical Infrastructure},
year={2018},
volume={6},
number={},
pages={25167-25177},
abstract={Insider attacks are becoming increasingly detrimental and frequent, affecting critical infrastructure at a massive scale. Recent attacks such as the U.K. National Health Service WannaCry ransomware attack which partly depends on internal users for initial infection highlight the increasing role of the malicious insiders in cyber-attack campaigns. The objective of this research is to ascertain the existing technological capability to mitigate insider threats within computer security systems by way of a mixed-method systematic review. Evidence was acquired from major sources of mainstream and grey literature by analyzing about 300 000 papers. Crude aggregated results were analyzed across the literature, and the results were TPR 0.75, FPR 0.32, σ 0.24 and 0.36, respectively, and σ2 0.06 and 0.13, respectively. In totality, the literature evidence suggests that there is high heterogeneity across crude data indicating that the effectiveness of security measures varies significantly. No solution is able to totally mitigate an insider threat. Themes when against that data suggest that most, if not all, security measures require breaches to occur before an analysis of malicious activity can prevent it in future through recall. Such a reactive approach is not effective protect our critical infrastructure including our healthcare systems. Consequently, there is a major theoretical shortfall in current cyber defence architecture.},
keywords={critical infrastructures;data analysis;health care;invasive software;insider attacks;healthcare critical infrastructure;internal threats;healthcare systems;malicious activity;security measures;crude data;mixed-method systematic review;computer security systems;cyber-attack campaigns;malicious insiders;internal users;National Health Service WannaCry ransomware attack;Systematics;Medical services;Critical infrastructure;Computer crime;Databases;Critical infrastructure security;personal data safety;healthcare;data breach;insider threat;meta-data;sabotage;systematic review;thematic analysis;unprivileged;untrusted;zero trust},
doi={10.1109/ACCESS.2018.2817560},
ISSN={2169-3536},
month={},}
@ARTICLE{8038053,
author={F. Palomba and A. Panichella and A. Zaidman and R. Oliveto and A. De Lucia},
journal={IEEE Transactions on Software Engineering},
title={The Scent of a Smell: An Extensive Comparison Between Textual and Structural Smells},
year={2018},
volume={44},
number={10},
pages={977-1000},
abstract={Code smells are symptoms of poor design or implementation choices that have a negative effect on several aspects of software maintenance and evolution, such as program comprehension or change- and fault-proneness. This is why researchers have spent a lot of effort on devising methods that help developers to automatically detect them in source code. Almost all the techniques presented in literature are based on the analysis of structural properties extracted from source code, although alternative sources of information (e.g., textual analysis) for code smell detection have also been recently investigated. Nevertheless, some studies have indicated that code smells detected by existing tools based on the analysis of structural properties are generally ignored (and thus not refactored) by the developers. In this paper, we aim at understanding whether code smells detected using textual analysis are perceived and refactored by developers in the same or different way than code smells detected through structural analysis. To this aim, we set up two different experiments. We have first carried out a software repository mining study to analyze how developers act on textually or structurally detected code smells. Subsequently, we have conducted a user study with industrial developers and quality experts in order to qualitatively analyze how they perceive code smells identified using the two different sources of information. Results indicate that textually detected code smells are easier to identify and for this reason they are considered easier to refactor with respect to code smells detected using structural properties. On the other hand, the latter are often perceived as more severe, but more difficult to exactly identify and remove.},
keywords={data mining;software maintenance;software quality;source code (software);source code;textually detected code smells;structurally detected code smells;software maintenance;software repository mining;qualitatively analyze;Tools;Data mining;Software systems;Detectors;Maintenance engineering;Large scale integration;Code smells;empirical study;mining software repositories},
doi={10.1109/TSE.2017.2752171},
ISSN={0098-5589},
month={Oct},}
@ARTICLE{4674351,
author={P. Cunningham},
journal={IEEE Transactions on Knowledge and Data Engineering},
title={A Taxonomy of Similarity Mechanisms for Case-Based Reasoning},
year={2009},
volume={21},
number={11},
pages={1532-1543},
abstract={Assessing the similarity between cases is a key aspect of the retrieval phase in case-based reasoning (CBR). In most CBR work, similarity is assessed based on feature value descriptions of cases using similarity metrics, which use these feature values. In fact, it might be said that this notion of a feature value representation is a defining part of the CBR worldview-it underpins the idea of a problem space with cases located relative to each other in this space. Recently, a variety of similarity mechanisms have emerged that are not founded on this feature space idea. Some of these new similarity mechanisms have emerged in CBR research and some have arisen in other areas of data analysis. In fact, research on kernel-based learning is a rich source of novel similarity representations because of the emphasis on encoding domain knowledge in the kernel function. In this paper, we present a taxonomy that organizes these new similarity mechanisms and more established similarity mechanisms in a coherent framework.},
keywords={case-based reasoning;learning (artificial intelligence);machine learning;case-based reasoning;nearest neighbor classifiers;feature value representation;kernel function;taxonomy;similarity mechanisms;Taxonomy;Data analysis;Kernel;Encoding;Nearest neighbor searches;Shape;Visualization;Noise reduction;Noise shaping;Feature extraction;Machine learning;case-based reasoning;nearest neighbor classifiers.;Knowledge and data engineering tools and techniques;Knowledge base management;Machine learning;Knowledge Management;Artificial Intelligence;Computing Methodologies},
doi={10.1109/TKDE.2008.227},
ISSN={1041-4347},
month={Nov},}
@ARTICLE{5959167,
author={J. Wu},
journal={IEEE Transactions on Knowledge and Data Engineering},
title={A Framework for Learning Comprehensible Theories in XML Document Classification},
year={2012},
volume={24},
number={1},
pages={1-14},
abstract={XML has become the universal data format for a wide variety of information systems. The large number of XML documents existing on the web and in other information storage systems makes classification an important task. As a typical type of semistructured data, XML documents have both structures and contents. Traditional text learning techniques are not very suitable for XML document classification as structures are not considered. This paper presents a novel complete framework for XML document classification. We first present a knowledge representation method for XML documents which is based on a typed higher order logic formalism. With this representation method, an XML document is represented as a higher order logic term where both its contents and structures are captured. We then present a decision-tree learning algorithm driven by precision/recall breakeven point (PRDT) for the XML classification problem which can produce comprehensible theories. Finally, a semi-supervised learning algorithm is given which is based on the PRDT algorithm and the cotraining framework. Experimental results demonstrate that our framework is able to achieve good performance in both supervised and semi-supervised learning with the bonus of producing comprehensible learning theories.},
keywords={formal logic;Internet;knowledge representation;learning (artificial intelligence);pattern classification;storage management;XML;comprehensible theories;XML document classification;universal data format;information systems;web;information storage systems;knowledge representation method;typed higher order logic formalism;precision/recall breakeven point;decision-tree learning algorithm;semi-supervised learning algorithm;XML;Knowledge representation;Unsupervised learning;Machine learning algorithms;Machine learning;Learning systems;Supervised learning;XML document;machine learning;knowledge representation;semi-supervised learning.},
doi={10.1109/TKDE.2011.158},
ISSN={1041-4347},
month={Jan},}
@ARTICLE{6894569,
author={T. Nguyen and H. W. Lauw and P. Tsaparas},
journal={IEEE Transactions on Knowledge and Data Engineering},
title={Review Selection Using Micro-Reviews},
year={2015},
volume={27},
number={4},
pages={1098-1111},
abstract={Given the proliferation of review content, and the fact that reviews are highly diverse and often unnecessarily verbose, users frequently face the problem of selecting the appropriate reviews to consume. Micro-reviews are emerging as a new type of online review content in the social media. Micro-reviews are posted by users of check-in services such as Foursquare. They are concise (up to 200 characters long) and highly focused, in contrast to the comprehensive and verbose reviews. In this paper, we propose a novel mining problem, which brings together these two disparate sources of review content. Specifically, we use coverage of micro-reviews as an objective for selecting a set of reviews that cover efficiently the salient aspects of an entity. Our approach consists of a two-step process: matching review sentences to micro-reviews, and selecting a small set of reviews that cover as many micro-reviews as possible, with few sentences. We formulate this objective as a combinatorial optimization problem, and show how to derive an optimal solution using Integer Linear Programming. We also propose an efficient heuristic algorithm that approximates the optimal solution. Finally, we perform a detailed evaluation of all the steps of our methodology using data collected from Foursquare and Yelp.},
keywords={combinatorial mathematics;data mining;heuristic programming;information retrieval;integer programming;linear programming;social networking (online);text analysis;review selection;microreviews;online review content;social media;check-in services;Foursquare;verbose review;mining problem;disparate review content sources;review sentence matching;combinatorial optimization problem;integer linear programming;heuristic algorithm;Yelp;Approximation algorithms;Approximation methods;Mobile communication;Greedy algorithms;Algorithm design and analysis;Educational institutions;Optimization;Micro-review;review selection;coverage},
doi={10.1109/TKDE.2014.2356456},
ISSN={1041-4347},
month={April},}
@ARTICLE{8423185,
author={M. Manso-Vázquez and M. Caeiro-Rodríguez and M. Llamas-Nistal},
journal={IEEE Access},
title={An xAPI Application Profile to Monitor Self-Regulated Learning Strategies},
year={2018},
volume={6},
number={},
pages={42467-42481},
abstract={Self-regulated learning (SRL) is being promoted and adopted increasingly due to the needs of current education, student centered and focused on competence development. One of the main components of SRL is learners' self-monitoring, which eventually contributes to a better performance. Monitoring is also important for teachers, as it enables them to know to what extent their learners are doing well and progressing properly. At the same time, the use of technology for learning is now common and facilitates monitoring. Nevertheless, the available software still offers poor support from the SRL point of view, especially, for SRL monitoring. This clashes with the growth of learning analytics and educational data mining. The main issue is the wide variety of SRL actions that need to be captured, commonly performed in different tools, and the need to integrate them to support the development of analytics and data mining developments, making imperative the search of interoperable solutions. This paper focuses on the standardization of SRL traces to enable data collection from multiple sources and data analysis with the goal of easing the monitoring process for teachers and learners. First, the paper analyzes current monitoring software and its limitations for SRL. Then, after a brief analysis of available standards on this area, an application profile for the eXperience API specification is proposed to enable the interoperable recording of the SRL traces. The paper describes the process followed to create the profile, from the analysis to the final implementation, including the selection of the interactions that represent relevant SRL actions, the selection of vocabularies to record them and a case study.},
keywords={application program interfaces;computer aided instruction;data analysis;data mining;open systems;xAPI application profile;competence development;SRL monitoring;learning analytics;educational data mining;data mining developments;data collection;data analysis;self-regulated learning strategies;monitoring software;experience API specification;Monitoring;Task analysis;Planning;Software;Cognition;Tools;Standards;Educational technology;learning analytics;self-regulated learning;standardization},
doi={10.1109/ACCESS.2018.2860519},
ISSN={2169-3536},
month={},}
@ARTICLE{8353219,
author={H. Yao and T. Qiao and M. Xu and N. Zheng},
journal={IEEE Access},
title={Robust Multi-Classifier for Camera Model Identification Based on Convolution Neural Network},
year={2018},
volume={6},
number={},
pages={24973-24982},
abstract={With the prevalence of adopting data-driven convolution neural network (CNN)-based algorithms into the community of digital image forensics, some novel supervised classifiers have indeed increasingly sprung up with nearly perfect detection rate, compared with the conventional supervised mechanism. The goal of this paper is to investigate a robust multi-classifier for dealing with one of the image forensic problems, referred to as source camera identification. The main contributions of this paper are threefold: (1) by mainly analyzing the image features characterizing different source camera models, we design an improved architecture of CNN for adaptively and automatically extracting characteristics, instead of hand-crafted extraction; (2) the proposed efficient CNN-based multi-classifier is capable of simultaneously classifying the tested images acquired by a large scale of different camera models, instead of utilizing a binary classifier; and (3) numerical experiments show that our proposed multi-classifier can effectively classify different camera models while achieving an average accuracy of nearly 100% relying on majority voting, which indeed outperforms some prior arts; meanwhile, its robustness has been verified by considering that the images are attacked by post-processing such as JPEG compression and noise adding.},
keywords={cameras;convolution;feature extraction;feedforward neural nets;image classification;image coding;image forensics;JPEG compression;supervised mechanism;image features;image forensic problems;digital image forensics;convolution neural network;camera model identification;robust multiclassifier;Cameras;Feature extraction;Robustness;Forensics;Numerical models;Digital images;Transform coding;Camera model identification;deep learning;convolution neural network (CNN);passive image forensics},
doi={10.1109/ACCESS.2018.2832066},
ISSN={2169-3536},
month={},}
@ARTICLE{7961149,
author={R. Zhang and H. Tao and L. Wu and Y. Guan},
journal={IEEE Access},
title={Transfer Learning With Neural Networks for Bearing Fault Diagnosis in Changing Working Conditions},
year={2017},
volume={5},
number={},
pages={14347-14357},
abstract={Traditional machine learning algorithms have made great achievements in data-driven fault diagnosis. However, they assume that all the data must be in the same working condition and have the same distribution and feature space. They are not applicable for real-world working conditions, which often change with time, so the data are hard to obtain. In order to utilize data in different working conditions to improve the performance, this paper presents a transfer learning approach for fault diagnosis with neural networks. First, it learns characteristics from massive source data and adjusts the parameters of neural networks accordingly. Second, the structure of neural networks alters for the change of data distribution. In the same time, some parameters are transferred from source task to target task. Finally, the new model is trained by a small amount of target data in another working condition. The Case Western Reserve University bearing data set is used to validate the performance of the proposed transfer learning approach. Experimental results show that the proposed transfer learning approach can improve the classification accuracy and reduce the training time comparing with the conventional neural network method when there are only a small amount of target data.},
keywords={data handling;fault diagnosis;learning (artificial intelligence);machine bearings;mechanical engineering computing;neural nets;data distribution;working conditions;bearing fault diagnosis;neural networks;transfer learning approach;Employee welfare;Fault diagnosis;Data models;Training;Vibrations;Biological neural networks;Fault diagnosis;transfer learning;neural networks;machine learning},
doi={10.1109/ACCESS.2017.2720965},
ISSN={2169-3536},
month={},}
@ARTICLE{8330743,
author={S. S. Kavuri and K. C. Veluvolu and Q. H. Chai},
journal={IEEE Access},
title={Evolutionary Based ICA With Reference for EEG$\mu$Rhythm Extraction},
year={2018},
volume={6},
number={},
pages={19702-19713},
abstract={Independent component analysis with reference (ICA-R), a paradigm of constrained ICA (cICA), incorporates textita priori information about the desired sources as reference signals into the contrast function of ICA. Reference signals direct the search toward the separation of desired sources more efficiently and accurately than the ICA. The penalized contrast function of ICA-R is non-smooth everywhere and the ICA-R algorithm does not always reach the global optimum due to the Newton-like learning used. In this paper, we propose a constrained differential evolutionary algorithm with an improved initialization strategy to solve the constrained optimization problem of ICA-R that can asymptotically converge to the optimum. It completely avoids the formulation of a penalized contrast function and scaling (due to the Lagrangian multipliers) by incorporating the ICA contrast function and the violation of the closeness constraint into the selection process of the evolution. Experiments with synthetic data and isolation of μ rhythmic activity from EEG showed improved source extraction performance over ICA-R and its recent enhancements.},
keywords={blind source separation;electroencephalography;evolutionary computation;independent component analysis;learning (artificial intelligence);medical signal processing;optimisation;EEG μ rhythm extraction;independent component analysis;constrained ICA;textita priori information;penalized contrast function;ICA-R algorithm;constrained differential evolutionary algorithm;constrained optimization problem;ICA contrast function;source extraction performance;source separation;Newton-like learning;Lagrangian multipliers;Convergence;Electroencephalography;Optimization;Data mining;Rhythm;Independent component analysis;Upper bound;Constrained ICA (cICA);differential evolution (DE);brain computer interface (BCI);electroencephalography (EEG);independent component analysis (ICA);ICA with reference (ICA-R)},
doi={10.1109/ACCESS.2018.2821838},
ISSN={2169-3536},
month={},}
@ARTICLE{5936066,
author={Y. Shin and J. Lim and J. Park},
journal={IEEE Transactions on Knowledge and Data Engineering},
title={Joint Optimization of Index Freshness and Coverage in Real-Time Search Engines},
year={2012},
volume={24},
number={12},
pages={2203-2217},
abstract={Real-time search engines are increasingly indexing web content using data streams, since a number of web sources including news and social media sites are now delivering up-to-date information via streams. Accordingly, it is a crucial challenge for a real-time search engine using data streams to improve index freshness that primarily depends on the latencies involved during fetching and indexing processes. Retrieval latency is a time lag between document publication and fetching while indexing latency is a delay required for a fetched document to be indexed, which is caused by finiteness of indexing capacity. The problem of retrieval latency can be satisfactorily addressed by use of appropriate fetching scheduling or recent real-time content notification protocols. However, as the entire volume of real-time content rapidly grows, the indexing latency becomes a challenging problem. Furthermore, the need for maximizing index coverage makes it more difficult to reduce the indexing latency under the limited indexing capacity. We consider a problem of jointly optimizing the indexing latency as well as indexindexing latency coverage, in which their relative importance can be adjusted, and propose an optimization model based on inventory control theory. Extensive experiments have been conducted to validate the proposed model, and suggest that the proposed approach outperforms the other alternatives.},
keywords={document handling;indexing;information retrieval;Internet;optimisation;protocols;real-time systems;scheduling;search engines;joint optimization;index freshness;index coverage;real-time search engines;Web content indexing;data streams;fetching process;retrieval latency;document publication;document fetching;indexing capacity finiteness;fetching scheduling;real-time content notification protocols;optimization model;inventory control theory;indexing latency;Indexing;Real time systems;Search engines;Inventory control;Feed;index freshness;index coverage;real-time search;search engine;information retrieval},
doi={10.1109/TKDE.2011.144},
ISSN={1041-4347},
month={Dec},}
@ARTICLE{5710949,
author={P. McMinn and M. Harman and K. Lakhotia and Y. Hassoun and J. Wegener},
journal={IEEE Transactions on Software Engineering},
title={Input Domain Reduction through Irrelevant Variable Removal and Its Effect on Local, Global, and Hybrid Search-Based Structural Test Data Generation},
year={2012},
volume={38},
number={2},
pages={453-477},
abstract={Search-Based Test Data Generation reformulates testing goals as fitness functions so that test input generation can be automated by some chosen search-based optimization algorithm. The optimization algorithm searches the space of potential inputs, seeking those that are “fit for purpose,” guided by the fitness function. The search space of potential inputs can be very large, even for very small systems under test. Its size is, of course, a key determining factor affecting the performance of any search-based approach. However, despite the large volume of work on Search-Based Software Testing, the literature contains little that concerns the performance impact of search space reduction. This paper proposes a static dependence analysis derived from program slicing that can be used to support search space reduction. The paper presents both a theoretical and empirical analysis of the application of this approach to open source and industrial production code. The results provide evidence to support the claim that input domain reduction has a significant effect on the performance of local, global, and hybrid search, while a purely random search is unaffected.},
keywords={automatic test pattern generation;optimisation;program compilers;program slicing;program testing;public domain software;search problems;input domain reduction;irrelevant variable removal;hybrid search-based structural test data generation;fitness functions;test input generation;search-based optimization algorithm;key determining factor;search-based software testing;search space reduction;static dependence analysis;program slicing;open source approach;industrial production code;Input variables;Software testing;Optimization;Algorithm design and analysis;Search problems;Software algorithms;Search-based software testing;evolutionary testing;automated test data generation;input domain reduction.},
doi={10.1109/TSE.2011.18},
ISSN={0098-5589},
month={March},}
@ARTICLE{8272334,
author={M. Marjanović and A. Antonić and I. P. Žarko},
journal={IEEE Access},
title={Edge Computing Architecture for Mobile Crowdsensing},
year={2018},
volume={6},
number={},
pages={10662-10674},
abstract={Mobile crowdsensing (MCS) is a human-driven Internet of Things service empowering citizens to observe the phenomena of individual, community, or even societal value by sharing sensor data about their environment while on the move. Typical MCS service implementations utilize cloud-based centralized architectures, which consume a lot of computational resources and generate significant network traffic, both in mobile networks and toward cloud-based MCS services. Mobile edge computing (MEC) is a natural choice to distribute MCS solutions by moving computation to network edge, since an MEC-based architecture enables significant performance improvements due to the partitioning of problem space based on location, where real-time data processing and aggregation is performed close to data sources. This in turn reduces the associated traffic in mobile core and will facilitate MCS deployments of massive scale. This paper proposes an edge computing architecture adequate for massive scale MCS services by placing key MCS features within the reference MEC architecture. In addition to improved performance, the proposed architecture decreases privacy threats and permits citizens to control the flow of contributed sensor data. It is adequate for both data analytics and real-time MCS scenarios, in line with the 5G vision to integrate a huge number of devices and enable innovative applications requiring low network latency. Our analysis of service overhead introduced by distributed architecture and service reconfiguration at network edge performed on real user traces shows that this overhead is controllable and small compared with the aforementioned benefits. When enhanced by interoperability concepts, the proposed architecture creates an environment for the establishment of an MCS marketplace for bartering and trading of both raw sensor data and aggregated/processed information.},
keywords={cloud computing;data privacy;Internet;Internet of Things;mobile computing;open systems;telecommunication computing;telecommunication traffic;real-time MCS scenarios;low network;service overhead;distributed architecture;service reconfiguration;network edge;MCS marketplace;raw sensor data;edge computing architecture;mobile crowdsensing;citizens;societal value;typical MCS service implementations;centralized architectures;computational resources;mobile networks;mobile edge computing;natural choice;problem space;real-time data processing;aggregation;data sources;associated traffic;mobile core;MCS deployments;massive scale MCS services;reference MEC architecture;contributed sensor data;data analytics;network traffic;performance improvements;human-driven Internet of Things service;Computer architecture;Task analysis;Mobile communication;Edge computing;Real-time systems;Mobile handsets;Mobile computing;Mobile crowdsensing;mobile edge computing;MCS functional architecture;MEC reference architecture},
doi={10.1109/ACCESS.2018.2799707},
ISSN={2169-3536},
month={},}
@ARTICLE{8365677,
author={F. Ahmad and V. N. L. Franqueira and A. Adnane},
journal={IEEE Access},
title={TEAM: A Trust Evaluation and Management Framework in Context-Enabled Vehicular Ad-Hoc Networks},
year={2018},
volume={6},
number={},
pages={28643-28660},
abstract={Vehicular ad-hoc network (VANET) provides a unique platform for vehicles to intelligently exchange critical information, such as collision avoidance messages. It is, therefore, paramount that this information remains reliable and authentic, i.e., originated from a legitimate and trusted vehicle. Trust establishment among vehicles can ensure security of a VANET by identifying dishonest vehicles and revoking messages with malicious content. For this purpose, several trust models (TMs) have been proposed but, currently, there is no effective way to compare how they would behave in practice under adversary conditions. To this end, we propose a novel trust evaluation and management (TEAM) framework, which serves as a unique paradigm for the design, management, and evaluation of TMs in various contexts and in presence of malicious vehicles. Our framework incorporates an asset-based threat model and ISO-based risk assessment for the identification of attacks against critical risks. The TEAM has been built using VEINS, an open source simulation environment which incorporates SUMO traffic simulator and OMNET++ discrete event simulator. The framework created has been tested with the implementation of three types of TMs (data oriented, entity oriented, and hybrid) under four different contexts of VANET based on the mobility of both honest and malicious vehicles. Results indicate that the TEAM is effective to simulate a wide range of TMs, where the efficiency is evaluated against different quality of service and security-related criteria. Such framework may be instrumental for planning smart cities and for car manufacturers.},
keywords={discrete event simulation;mobility management (mobile radio);public domain software;quality of service;risk management;telecommunication computing;telecommunication security;traffic engineering computing;vehicular ad hoc networks;TEAM;malicious vehicles;threat model;critical risks;open source simulation environment;SUMO traffic simulator;VANET;critical information;collision avoidance messages;trust establishment;malicious content;trust models;TM;trust evaluation-and-management framework;context-enabled vehicular ad-hoc networks;dishonest vehicle identification;revoking message identification;asset-based threat model;ISO-based risk assessment;VEINS;OMNET++ discrete event simulator;quality-of-service;security-related criteria;Vehicular ad hoc networks;Smart cities;Data models;Quality of service;Vehicular networks;trust management;smart cities;security;intelligent transportation systems;VEINS;SUMO;OMNET++;simulation},
doi={10.1109/ACCESS.2018.2837887},
ISSN={2169-3536},
month={},}
@ARTICLE{8081765,
author={Y. Nan and W. Li and W. Bao and F. C. Delicato and P. F. Pires and Y. Dou and A. Y. Zomaya},
journal={IEEE Access},
title={Adaptive Energy-Aware Computation Offloading for Cloud of Things Systems},
year={2017},
volume={5},
number={},
pages={23947-23957},
abstract={Cloud computing has become the de facto computing platform for application processing in the era of the Internet of Things (IoT). However, limitations of the cloud model, such as the high transmission latency and high costs are giving birth to a new computing paradigm called edge computing (a.k.a fog computing). Fog computing aims to move the data processing close to the network edge so as to reduce Internet traffic. However, since the servers at the fog layer are not as powerful as the ones in the cloud, there is a need to balance the data processing in between the fog and the cloud. Moreover, besides the data offloading issue, the energy efficiency of fog computing nodes has become an increasing concern. Densely deployed fog nodes are a major source of carbon footprint in IoT systems. To reduce the usage of the brown energy resources (e.g. powered by energy produced through fossil fuels), green energy is an alternative option. In this paper, we propose employing dual energy sources for supporting the fog nodes, where solar power is the primary energy supply and grid power is the backup supply. Based on that, we present a comprehensive analytic framework for incorporating green energy sources to support the running of IoT and fog computingbased systems, and to handle the tradeoff in terms of average response time, average monetary, and energy costs in the IoT. This paper describes an online algorithm, Lyapunov optimization on time and energy cost (LOTEC), based on the technique of Lyapunov optimization. LOTEC is a quantified near optimal solution and is able to make control decision on application offloading by adjusting the two-way tradeoff between average response time and average cost. We evaluate the performance of our proposed algorithm by a number of experiments. Rigorous analysis and simulations have demonstrated its performance.},
keywords={cloud computing;energy conservation;Internet;Internet of Things;mobile computing;optimisation;power aware computing;telecommunication traffic;Internet of Things systems;green energy sources;Cloud of Things systems;application offloading;average response time;grid power;primary energy supply;solar power;dual energy sources;brown energy resources;IoT systems;fog computing nodes;energy efficiency;data offloading issue;fog layer;Internet traffic;network edge;data processing close;edge computing;cloud model;application processing;facto computing platform;cloud computing;adaptive energy-aware computation offloading;Servers;Cloud computing;Logic gates;Edge computing;Time factors;Green products;Delays;Internet of things;fog computing;Lyapunov optimization;green energy},
doi={10.1109/ACCESS.2017.2766165},
ISSN={2169-3536},
month={},}
@ARTICLE{8369420,
author={M. M. N. Mannan and M. A. Kamran and M. Y. Jeong},
journal={IEEE Access},
title={Identification and Removal of Physiological Artifacts From Electroencephalogram Signals: A Review},
year={2018},
volume={6},
number={},
pages={30630-30652},
abstract={Electroencephalogram (EEG), boasting the advantages of portability, low cost, and hightemporal resolution, is a non-invasive brain-imaging modality that can be used to measure different brain states. However, EEG recordings are always contaminated with artifacts from different sources other than neurons, which renders EEG data analysis more difficult, and which potentially results in misleading findings. Therefore, it is essential for many medical and practical applications to remove these artifacts in the preprocessing stage before analyzing EEG data. In the last thirty years, various methods have been developed to remove different types of artifacts from contaminated EEG data; still though, there is no standard method that can be used optimally, and therefore, the research remains attractive as well as challenging. This paper presents an extensive overview of the existing methods for ocular, muscle, and cardiac artifact identification and removal with their comparative advantages and limitations. We also reviewed the schemes developed for validating the performances of algorithms with simulated and real EEG data. In future studies, researchers should focus not only on the combining of different methods with multiple processing stages for efficient removal of artifactual interferences but also on the development of standard criteria for validation of recorded EEG signals.},
keywords={brain;data analysis;electroencephalography;medical signal processing;neurophysiology;reviews;physiological artifacts;electroencephalogram signals;noninvasive brain-imaging modality;EEG data analysis;medical applications;contaminated EEG data;standard method;cardiac artifact identification;recorded EEG signals;high-temporal resolution;review;muscle;Electroencephalography;Filtering;Electrooculography;Physiology;Contamination;Muscles;Electrodes;Electroencephalography;physiological artifacts;artifact removal;regression;filtering;blind source separation;independent component analysis;principal component analysis;canonical correlation analysis;morphological component analysis;empirical-mode decomposition;wavelet transform;signal space projection;beamformers;hybrid methods;brain-computer interface;high-density EEG;clinical EEG},
doi={10.1109/ACCESS.2018.2842082},
ISSN={2169-3536},
month={},}
@ARTICLE{6815752,
author={Y. Liu and C. Xu and S. C. Cheung and J. Lü},
journal={IEEE Transactions on Software Engineering},
title={GreenDroid: Automated Diagnosis of Energy Inefficiency for Smartphone Applications},
year={2014},
volume={40},
number={9},
pages={911-940},
abstract={Smartphone applications' energy efficiency is vital, but many Android applications suffer from serious energy inefficiency problems. Locating these problems is labor-intensive and automated diagnosis is highly desirable. However, a key challenge is the lack of a decidable criterion that facilitates automated judgment of such energy problems. Our work aims to address this challenge. We conducted an in-depth study of 173 open-source and 229 commercial Android applications, and observed two common causes of energy problems: missing deactivation of sensors or wake locks, and cost-ineffective use of sensory data. With these findings, wepropose an automated approach to diagnosing energy problems in Android applications. Our approach explores an application's state space by systematically executing the application using Java PathFinder (JPF). It monitors sensor and wake lock operations to detect missing deactivation of sensors and wake locks. It also tracks the transformation and usage of sensory data and judges whether they are effectively utilized by the application using our state-sensitive data utilization metric. In this way, our approach can generate detailed reports with actionable information to assist developers in validating detected energy problems. We built our approach as a tool, GreenDroid, on top of JPF. Technically, we addressed the challenges of generating user interaction events and scheduling event handlers in extending JPF for analyzing Android applications. We evaluated GreenDroid using 13 real-world popular Android applications. GreenDroid completed energy efficiency diagnosis for these applications in a few minutes. It successfully located real energy problems in these applications, and additionally found new unreported energy problems that were later confirmed by developers.},
keywords={Android (operating system);Java;power aware computing;program diagnostics;public domain software;smart phones;GreenDroid;automated energy inefficiency diagnosis;smartphone applications;labor-intensive diagnosis;automated diagnosis;open-source Android applications;commercial Android applications;Java PathFinder;JPF;wake lock operations;state-sensitive data utilization metric;user interaction events;scheduling event handlers;Androids;Humanoid robots;Computer bugs;Sensors;Open source software;Green products;Google;Smartphone application;energy inefficiency;automated diagnosis;sensory data utilization;green computing},
doi={10.1109/TSE.2014.2323982},
ISSN={0098-5589},
month={Sept},}
@ARTICLE{8063326,
author={J. H. Brenas and M. S. Al-Manir and C. J. O. Baker and A. Shaban-Nejad},
journal={IEEE Access},
title={A Malaria Analytics Framework to Support Evolution and Interoperability of Global Health Surveillance Systems},
year={2017},
volume={5},
number={},
pages={21605-21619},
abstract={Malaria is a leading cause of death in Africa. Many organizations, NGO's, and government agencies are collaborating to prevent, control, and eliminate malaria. In order to succeed in these shared goals, an integrated, consistent knowledge source to empower informed decision-making is required. Malaria surveillance is currently performed using dynamic, interconnected, systems which require rapid data exchange between different platforms. An important challenge these systems must overcome is the occurrence of dynamic changes in one or more interacting components, which can lead to inconsistencies and mismatches between components of the infrastructure. In this paper, we present our efforts toward the design and development of the semantic interoperability and evolution for malaria analytics platform, with the goal of improving data and semantic interoperability for dynamic malaria surveillance and to support the integration of data across multiple scales. The long term target is to deliver transparent and scalable tools for decision making for malaria elimination. Our analysis is focused on sentinel sites in selected African countries, including Uganda and Gabon.},
keywords={data analysis;diseases;electronic data interchange;health care;management of change;open systems;surveillance;Web services;decision making;malaria elimination;global health surveillance systems;Africa;integrated knowledge source;informed decision-making;dynamic systems;interconnected systems;semantic interoperability;dynamic malaria surveillance;data exchange;malaria analytics;semantic evolution;Diseases;Surveillance;Ontologies;Distributed databases;Semantics;Interoperability;Interoperability;change management;malaria surveillance;graph transformation;web services;semantics},
doi={10.1109/ACCESS.2017.2761232},
ISSN={2169-3536},
month={},}
@ARTICLE{7855802,
author={Y. Xu and S. J. Pan and H. Xiong and Q. Wu and R. Luo and H. Min and H. Song},
journal={IEEE Transactions on Knowledge and Data Engineering},
title={A Unified Framework for Metric Transfer Learning},
year={2017},
volume={29},
number={6},
pages={1158-1171},
abstract={Transfer learning has been proven to be effective for the problems where training data from a source domain and test data from a target domain are drawn from different distributions. To reduce the distribution divergence between the source domain and the target domain, many previous studies have been focused on designing and optimizing objective functions with the Euclidean distance to measure dissimilarity between instances. However, in some real-world applications, the Euclidean distance may be inappropriate to capture the intrinsic similarity or dissimilarity between instances. To deal with this issue, in this paper, we propose a metric transfer learning framework (MTLF) to encode metric learning in transfer learning. In MTLF, instance weights are learned and exploited to bridge the distributions of different domains, while Mahalanobis distance is learned simultaneously to maximize the intra-class distances and minimize the inter-class distances for the target domain. Unlike previous work where instance weights and Mahalanobis distance are trained in a pipelined framework that potentially leads to error propagation across different components, MTLF attempts to learn instance weights and a Mahalanobis distance in a parallel framework to make knowledge transfer across domains more effective. Furthermore, we develop general solutions to both classification and regression problems on top of MTLF, respectively. We conduct extensive experiments on several real-world datasets on object recognition, handwriting recognition, and WiFi location to verify the effectiveness of MTLF compared with a number of state-of-the-art methods.},
keywords={learning (artificial intelligence);Euclidean distance;metric transfer learning framework;MTLF;Mahalanobis distance;parallel framework;object recognition;handwriting recognition;WiFi location;Kernel;Euclidean distance;Training data;Mobile handsets;Learning systems;Knowledge transfer;Transfer learning;metric learning;density ratio reweighting;Mahalanobis distance;learning framework},
doi={10.1109/TKDE.2017.2669193},
ISSN={1041-4347},
month={June},}
@ARTICLE{7454756,
author={F. M. F. Wong and C. W. Tan and S. Sen and M. Chiang},
journal={IEEE Transactions on Knowledge and Data Engineering},
title={Quantifying Political Leaning from Tweets, Retweets, and Retweeters},
year={2016},
volume={28},
number={8},
pages={2158-2172},
abstract={The widespread use of online social networks (OSNs) to disseminate information and exchange opinions, by the general public, news media, and political actors alike, has enabled new avenues of research in computational political science. In this paper, we study the problem of quantifying and inferring the political leaning of Twitter users. We formulate political leaning inference as a convex optimization problem that incorporates two ideas: (a) users are consistent in their actions of tweeting and retweeting about political issues, and (b) similar users tend to be retweeted by similar audience. We then apply our inference technique to 119 million election-related tweets collected in seven months during the 2012 U.S. presidential election campaign. On a set of frequently retweeted sources, our technique achieves 94 percent accuracy and high rank correlation as compared with manually created labels. By studying the political leaning of 1,000 frequently retweeted sources, 232,000 ordinary users who retweeted them, and the hashtags used by these sources, our quantitative study sheds light on the political demographics of the Twitter population, and the temporal dynamics of political polarization as events unfold.},
keywords={convex programming;inference mechanisms;learning (artificial intelligence);politics;social networking (online);social sciences computing;political leaning quantification;tweets;retweets;retweeters;online social networks;OSN;information dissemination;opinion exchange;computational political science;Twitter users;political leaning inference;convex optimization problem;US presidential election campaign;United States;hashtags;political polarization;quantitative study;Twitter;Nominations and elections;Media;Sociology;Statistics;Electronic mail;Twitter;political science;data analytics;inference;convex programming;signal processing},
doi={10.1109/TKDE.2016.2553667},
ISSN={1041-4347},
month={Aug},}
@ARTICLE{6714448,
author={B. N. Nguyen and A. M. Memon},
journal={IEEE Transactions on Software Engineering},
title={An Observe-Model-Exercise* Paradigm to Test Event-Driven Systems with Undetermined Input Spaces},
year={2014},
volume={40},
number={3},
pages={216-234},
abstract={System testing of software applications with a graphical-user interface (GUI) front-end requires that sequences of GUI events, that sample the application's input space, be generated and executed as test cases on the GUI. However, the context-sensitive behavior of the GUI of most of today's non-trivial software applications makes it practically impossible to fully determine the software's input space. Consequently, GUI testers-both automated and manual-working with undetermined input spaces are, in some sense, blindly navigating the GUI, unknowingly missing allowable event sequences, and failing to realize that the GUI implementation may allow the execution of some disallowed sequences. In this paper, we develop a new paradigm for GUI testing, one that we call Observe-Model-Exercise* (OME*) to tackle the challenges of testing context-sensitive GUIs with undetermined input spaces. Starting with an incomplete model of the GUI's input space, a set of coverage elements to test, and test cases, OME* iteratively observes the existence of new events during execution of the test cases, expands the model of the GUI's input space, computes new coverage elements, and obtains new test cases to exercise the new elements. Our experiment with 8 open-source software subjects, more than 500,000 test cases running for almost 1,100 machine-days, shows that OME* is able to expand the test space on average by 464.11 percent; it detected 34 faults that had never been detected before.},
keywords={graphical user interfaces;program testing;public domain software;observe-model-exercise paradigm;test event-driven system;undetermined input spaces;software system testing;graphical-user interface front-end;GUI context-sensitive behavior;open-source software subjects;test generation;Graphical user interfaces;Computational modeling;Blogs;Testing;Software;Context;Layout;Test generation;user interfaces;quality concepts},
doi={10.1109/TSE.2014.2300857},
ISSN={0098-5589},
month={March},}
@ARTICLE{7120143,
author={B. Ozcelik and C. Yilmaz},
journal={IEEE Transactions on Software Engineering},
title={Seer: A Lightweight Online Failure Prediction Approach},
year={2016},
volume={42},
number={1},
pages={26-46},
abstract={Online failure prediction approaches aim to predict the manifestation of failures at runtime before the failures actually occur. Existing approaches generally refrain themselves from collecting internal execution data, which can further improve the prediction quality. One reason behind this general trend is the runtime overhead incurred by the measurement instruments that collect the data. Since these approaches are targeted at deployed software systems, excessive runtime overhead is generally undesirable. In this work we conjecture that large cost reductions in collecting internal execution data for online failure prediction may derive from pushing the substantial parts of the data collection work onto the hardware. To test this hypothesis, we present a lightweight online failure prediction approach, called Seer, in which most of the data collection work is performed by fast hardware performance counters. The hardware-collected data is augmented with further data collected by a minimal amount of software instrumentation that is added to the systems software. In our empirical evaluations conducted on three open source projects, Seer performed significantly better than other related approaches in predicting the manifestation of failures.},
keywords={data handling;public domain software;quality assurance;software cost estimation;software quality;software reliability;system recovery;Seer;lightweight online failure prediction;internal execution data collection;prediction quality improvement;measurement instruments;software systems;runtime overhead;cost reduction;fast hardware performance counters;software instrumentation;open source projects;software quality assurance;software reliability;Radiation detectors;Hardware;Runtime;Predictive models;Instruments;Software;Indexes;Online failure prediction;hardware performance counters;software quality assurance;software reliability;Online failure prediction;hardware performance counters;software quality assurance;software reliability},
doi={10.1109/TSE.2015.2442577},
ISSN={0098-5589},
month={Jan},}
@ARTICLE{5332231,
author={F. Ricca and M. Di Penta and M. Torchiano and P. Tonella and M. Ceccato},
journal={IEEE Transactions on Software Engineering},
title={How Developers' Experience and Ability Influence Web Application Comprehension Tasks Supported by UML Stereotypes: A Series of Four Experiments},
year={2010},
volume={36},
number={1},
pages={96-118},
abstract={In recent years, several design notations have been proposed to model domain-specific applications or reference architectures. In particular, Conallen has proposed the UML Web Application Extension (WAE): a UML extension to model Web applications. The aim of our empirical investigation is to test whether the usage of the Conallen notation supports comprehension and maintenance activities with significant benefits, and whether such benefits depend on developers ability and experience. This paper reports and discusses the results of a series of four experiments performed in different locations and with subjects possessing different experience-namely, undergraduate students, graduate students, and research associates-and different ability levels. The experiments aim at comparing performances of subjects in comprehension tasks where they have the source code complemented either by standard UML diagrams or by diagrams stereotyped using the Conallen notation. Results indicate that, although, in general, it is not possible to observe any significant benefit associated with the usage of stereotyped diagrams, the availability of stereotypes reduces the gap between subjects with low skill or experience and highly skilled or experienced subjects. Results suggest that organizations employing developers with low experience can achieve a significant performance improvement by adopting stereotyped UML diagrams for Web applications.},
keywords={Internet;Unified Modeling Language;Web application comprehension tasks;UML stereotypes;source code;stereotyped diagrams;Unified modeling language;Application software;Object oriented modeling;Software maintenance;Web pages;Software engineering;Computer Society;Computer architecture;Service oriented architecture;Testing;Documentation;maintenance;and enhancement;software engineering;software/software engineering.},
doi={10.1109/TSE.2009.69},
ISSN={0098-5589},
month={Jan},}
@ARTICLE{6095557,
author={D. Bollegala and Y. Matsuo and M. Ishizuka},
journal={IEEE Transactions on Knowledge and Data Engineering},
title={Minimally Supervised Novel Relation Extraction Using a Latent Relational Mapping},
year={2013},
volume={25},
number={2},
pages={419-432},
abstract={The World Wide Web includes semantic relations of numerous types that exist among different entities. Extracting the relations that exist between two entities is an important step in various Web-related tasks such as information retrieval (IR), information extraction, and social network extraction. A supervised relation extraction system that is trained to extract a particular relation type (source relation) might not accurately extract a new type of a relation (target relation) for which it has not been trained. However, it is costly to create training data manually for every new relation type that one might want to extract. We propose a method to adapt an existing relation extraction system to extract new relation types with minimum supervision. Our proposed method comprises two stages: learning a lower dimensional projection between different relations, and learning a relational classifier for the target relation type with instance sampling. First, to represent a semantic relation that exists between two entities, we extract lexical and syntactic patterns from contexts in which those two entities co-occur. Then, we construct a bipartite graph between relation-specific (RS) and relation-independent (RI) patterns. Spectral clustering is performed on the bipartite graph to compute a lower dimensional projection. Second, we train a classifier for the target relation type using a small number of labeled instances. To account for the lack of target relation training instances, we present a one-sided under sampling method. We evaluate the proposed method using a data set that contains 2,000 instances for 20 different relation types. Our experimental results show that the proposed method achieves a statistically significant macroaverage F-score of 62.77. Moreover, the proposed method outperforms numerous baselines and a previously proposed weakly supervised relation extraction method.},
keywords={graph theory;Internet;learning (artificial intelligence);pattern classification;sampling methods;minimally supervised novel relation extraction;latent relational mapping;World Wide Web;Web-related tasks;information retrieval;IR;information extraction;social network extraction;relational classifier learning;lower dimensional projection learning;bipartite graph;relation-specific patterns;relation-independent patterns;RI;RS;sampling method;statistically significant macroaverage F-score;Context awareness;Semantics;Syntactics;Bipartite graph;Data mining;Feature extraction;Web and internet services;Relation extraction;domain adaptation;Web mining},
doi={10.1109/TKDE.2011.250},
ISSN={1041-4347},
month={Feb},}
@ARTICLE{7451264,
author={T. Chen and W. Shang and Z. M. Jiang and A. E. Hassan and M. Nasser and P. Flora},
journal={IEEE Transactions on Software Engineering},
title={Finding and Evaluating the Performance Impact of Redundant Data Access for Applications that are Developed Using Object-Relational Mapping Frameworks},
year={2016},
volume={42},
number={12},
pages={1148-1161},
abstract={Developers usually leverage Object-Relational Mapping (ORM) to abstract complex database accesses for large-scale systems. However, since ORM frameworks operate at a lower-level (i.e., data access), ORM frameworks do not know how the data will be used when returned from database management systems (DBMSs). Therefore, ORM cannot provide an optimal data retrieval approach for all applications, which may result in accessing redundant data and significantly affect system performance. Although ORM frameworks provide ways to resolve redundant data problems, due to the complexity of modern systems, developers may not be able to locate such problems in the code; hence, may not proactively resolve the problems. In this paper, we propose an automated approach, which we implement as a Java framework, to locate redundant data problems. We apply our framework on one enterprise and two open source systems. We find that redundant data problems exist in 87 percent of the exercised transactions. Due to the large number of detected redundant data problems, we propose an automated approach to assess the impact and prioritize the resolution efforts. Our performance assessment result shows that by resolving the redundant data problems, the system response time for the studied systems can be improved by an average of 17 percent.},
keywords={database management systems;Java;program diagnostics;public domain software;software performance evaluation;performance impact evaluation;redundant data access;object-relational mapping framework;ORM framework;database abstraction;database management system;DBMS;Java framework;open source system;program analysis;Databases;System performance;Java;Computer bugs;Complexity theory;Time factors;Object tracking;Performance;object-relational mapping (ORM);program analysis;database},
doi={10.1109/TSE.2016.2553039},
ISSN={0098-5589},
month={Dec},}
@ARTICLE{7909036,
author={L. Moreau and B. V. Batlajery and T. D. Huynh and D. Michaelides and H. Packer},
journal={IEEE Transactions on Software Engineering},
title={A Templating System to Generate Provenance},
year={2018},
volume={44},
number={2},
pages={103-121},
abstract={PROV-TEMPLATEIS a declarative approach that enables designers and programmers to design and generate provenance compatible with the PROV standard of the World Wide Web Consortium. Designers specify the topology of the provenance to be generated by composing templates, which are provenance graphs containing variables, acting as placeholders for values. Programmers write programs that log values and package them up in sets of bindings, a data structure associating variables and values. An expansion algorithm generates instantiated provenance from templates and sets of bindings in any of the serialisation formats supported by PROV. A quantitative evaluation shows that sets of bindings have a size that is typically 40 percent of that of expanded provenance templates and that the expansion algorithm is suitably tractable, operating in fractions of milliseconds for the type of templates surveyed in the article. Furthermore, the approach shows four significant software engineering benefits: separation of responsibilities, provenance maintenance, potential runtime checks and static analysis, and provenance consumption. The article gathers quantitative data and qualitative benefits descriptions from four different applications making use of PROV-TEMPLATE. The system is implemented and released in the open-source library ProvToolbox for provenance processing.},
keywords={data structures;graph theory;Internet;software engineering;software maintenance;data structure;PROV-TEMPLATE;provenance generation;open-source library ProvToolbox;software engineering;templating system;provenance processing;quantitative data;provenance consumption;provenance maintenance;expanded provenance templates;expansion algorithm;provenance graphs;World Wide Web Consortium;PROV standard;Electronic publishing;Instruments;Standards;Maintenance engineering;Runtime;Libraries;Automobiles;Provenance;prov;provenance generation;template},
doi={10.1109/TSE.2017.2659745},
ISSN={0098-5589},
month={Feb},}
@ARTICLE{8352137,
author={L. Burgueño and J. Boubeta-Puig and A. Vallecillo},
journal={IEEE Access},
title={Formalizing Complex Event Processing Systems in Maude},
year={2018},
volume={6},
number={},
pages={23222-23241},
abstract={Complex event processing (CEP) is a cutting-edge technology for analyzing and correlating streams of information about events that happen in a system, and deriving conclusions from them. CEP permits defining complex events based on the events produced by the incoming sources, to identify complex meaningful circumstances and to respond to them as quickly as possible. Such event types and patterns are defined using event processing languages. However, as the complexity of CEP programs grows, they become difficult to understand and to prove correct. This paper proposes a formal framework for the specification of CEP applications, using rewriting logic and Maude, to allow developers to formally analyze and prove properties of their CEP programs. Several case studies are presented to illustrate the approach, as well as a discussion on the benefits of using Maude and its toolkit for modeling and analyzing CEP systems.},
keywords={data analysis;formal languages;formal specification;rewriting systems;complex meaningful circumstances;event types;event processing languages;CEP programs;formal framework;CEP applications;Maude;complex event processing systems;cutting-edge technology;complex events;incoming sources;Motorcycles;Sensors;Computer crashes;Microsoft Windows;Tires;Accidents;Analytical models;Formal modeling;complex event processing;event processing language;rewriting logic;Maude},
doi={10.1109/ACCESS.2018.2831185},
ISSN={2169-3536},
month={},}
@ARTICLE{5740884,
author={S. Park and J. Kang},
journal={IEEE Transactions on Knowledge and Data Engineering},
title={Using Rule Ontology in Repeated Rule Acquisition from Similar Web Sites},
year={2012},
volume={24},
number={6},
pages={1106-1119},
abstract={Inferential rules are as essential to the Semantic Web applications as ontology. Therefore, rule acquisition is also an important issue, and the Web that implies inferential rules can be a major source of rule acquisition. We expect that it will be easier to acquire rules from a site by using similar rules of other sites in the same domain rather than starting from scratch. We proposed an automatic rule acquisition procedure using a rule ontology RuleToOnto, which represents information about the rule components and their structures. The rule acquisition procedure consists of the rule component identification step and the rule composition step. We developed A* algorithm for the rule composition and we performed experiments demonstrating that our ontology-based rule acquisition approach works in a real-world application.},
keywords={data mining;ontologies (artificial intelligence);semantic Web;Web sites;similar Web sites;inferential rules;semantic Web applications;automatic repeated rule acquisition procedure;rule to onto ontology rule;rule component identification step;A* algorithm;ontology-based rule acquisition approach;real-world application;Ontologies;Web pages;Cognition;Semantic Web;Knowledge acquisition;Semantics;Rule acquisition;rule ontology;best-first search.},
doi={10.1109/TKDE.2011.72},
ISSN={1041-4347},
month={June},}
@ARTICLE{6964812,
author={M. Long and J. Wang and J. Sun and P. S. Yu},
journal={IEEE Transactions on Knowledge and Data Engineering},
title={Domain Invariant Transfer Kernel Learning},
year={2015},
volume={27},
number={6},
pages={1519-1532},
abstract={Domain transfer learning generalizes a learning model across training data and testing data with different distributions. A general principle to tackle this problem is reducing the distribution difference between training data and testing data such that the generalization error can be bounded. Current methods typically model the sample distributions in input feature space, which depends on nonlinear feature mapping to embody the distribution discrepancy. However, this nonlinear feature space may not be optimal for the kernel-based learning machines. To this end, we propose a transfer kernel learning (TKL) approach to learn a domain-invariant kernel by directly matching source and target distributions in the reproducing kernel Hilbert space (RKHS). Specifically, we design a family of spectral kernels by extrapolating target eigensystem on source samples with Mercer's theorem. The spectral kernel minimizing the approximation error to the ground truth kernel is selected to construct domain-invariant kernel machines. Comprehensive experimental evidence on a large number of text categorization, image classification, and video event recognition datasets verifies the effectiveness and efficiency of the proposed TKL approach over several state-of-the-art methods.},
keywords={eigenvalues and eigenfunctions;Hilbert spaces;learning (artificial intelligence);domain invariant transfer kernel learning;nonlinear feature mapping;distribution discrepancy;nonlinear feature space;kernel-based learning machines;kernel Hilbert space;eigensystem;Mercer theorem;Kernel;Approximation error;Eigenvalues and eigenfunctions;Testing;Standards;Hilbert space;Transfer learning;kernel learning;Nystrom method;text mining;image classification;video recognition;Transfer learning;kernel learning;Nyström method;text mining;image classification;video recognition},
doi={10.1109/TKDE.2014.2373376},
ISSN={1041-4347},
month={June},}
@ARTICLE{7936476,
author={K. Hua},
journal={IEEE Access},
title={A Bandwidth-Efficient Stereo Video Streaming System},
year={2017},
volume={5},
number={},
pages={10263-10276},
abstract={Stereo video streaming is an emerging topic ever since the popularity of stereo video in the recent years. In this paper, we consider the anaglyph video, which displays the red channel in left view and green/blue channel in right view, respectively. The traditional anaglyph streaming system transmits full RGB components of both views and those results in greater bandwidth consumption. In this paper, we present two novel algorithms to improve the traditional anaglyph system. For the first, we transmit the color channels that will be viewed by the users, and then we rearrange the data to adapt to the standard video compression format. For the second, we propose to perform video demosaicing in the post-processing stage instead of the pre-processing stage as in the traditional system. In addition, we develop a gradient-based stereo matching technique integrated with our demosaicing algorithm for better retrieval of observed data. Experimental results demonstrate that the proposed anaglyph video streaming system outperforms the state-of-the-art algorithms in both objective and subjective comparisons.},
keywords={data compression;image matching;stereo image processing;video coding;video streaming;RGB components;bandwidth consumption;color channels;standard video compression format;video demosaicing;gradient-based stereo matching technique;demosaicing algorithm;anaglyph video streaming system;anaglyph streaming system;green-blue channel;red channel;bandwidth-efficient stereo video streaming system;Streaming media;Image color analysis;Image coding;Image reconstruction;Data preprocessing;Three-dimensional displays;Channel estimation;Anaglyph;stereo video compression;color filter array;video demosaicing;stereo matching},
doi={10.1109/ACCESS.2017.2710100},
ISSN={2169-3536},
month={},}
@ARTICLE{7924403,
author={D. Wang and Y. Jiang and H. Song and F. He and M. Gu and J. Sun},
journal={IEEE Access},
title={Verification of Implementations of Cryptographic Hash Functions},
year={2017},
volume={5},
number={},
pages={7816-7825},
abstract={Cryptographic hash functions have become the basis of modern network computing for identity authorization and secure computing; protocol consistency of cryptographic hash functions is one of the most important properties that affect the security and correctness of cryptographic implementations, and protocol consistency should be well proven before being applied in practice. Software verification has seen substantial application in safety-critical areas and has shown the ability to deliver better quality assurance for modern software; thus, applying software verification to a protocol consistency proof for cryptographic hash functions is a reasonable approach to prove their correctness. Verification of protocol consistency of cryptographic hash functions includes modeling of the cryptographic protocol and program analysis of the cryptographic implementation; these require a dedicated cryptographic implementation model that preserves the semantics of the code, efficient analysis of cryptographic operations on arrays and bits, and the ability to verify large-scale implementations. In this paper, we propose a fully automatic software verification framework, VeriHash, that brings software verification to protocol consistency proofs for cryptographic hash function implementations. It solves the above challenges by introducing a novel cryptographic model design for modeling the semantics of cryptographic hash function implementations, extended array theories for analysis of operations, and compositional verification for scalability. We evaluated our verification framework on two SHA-3 cryptographic hash function implementations: the winner of the NIST SHA-3 competition, Keccack; and an open-source hash program, RHash. We successfully verified the core parts of the two implementations and reproduced a bug in the published edition of RHash.},
keywords={authorisation;cryptographic protocols;program diagnostics;program verification;public domain software;software quality;implementation verification;identity authorization;secure computing;protocol consistency;cryptographic implementation security;cryptographic implementation correctness;software verification;safety-critical areas;quality assurance;cryptographic protocol;program analysis;code semantics;cryptographic operations;VeriHash;cryptographic model design;extended array theories;operation analysis;compositional verification;SHA-3 cryptographic hash function implementations;NIST SHA-3 competition;Keccack;open-source hash program;RHash;Cryptography;Software;Protocols;Analytical models;Tools;Algorithm design and analysis;Semantics;Model-predictive control;smoothening;gradient-based optimization;emission control;urban traffic control},
doi={10.1109/ACCESS.2017.2697918},
ISSN={2169-3536},
month={},}
@ARTICLE{7932434,
author={W. Yao and J. Zhao and M. J. Till and S. You and Y. Liu and Y. Cui and Y. Liu},
journal={IEEE Access},
title={Source Location Identification of Distribution-Level Electric Network Frequency Signals at Multiple Geographic Scales},
year={2017},
volume={5},
number={},
pages={11166-11175},
abstract={The distribution-level electric network frequency (ENF) extracted from an electric power signal is a promising forensic tool for multimedia recording authentication. Local characteristics in ENF signals recorded in different locations act as environmental signatures, which can be potentially used as a fingerprint for location identification. In this paper, a reference database is established for distribution-level ENF using FNET/GridEye system. An ENF identification method that combines a wavelet-based signature extraction and feedforward artificial neural network-based machine learning is presented to identify the location of unsourced ENF signals without relying on the availability of concurrent signals. Experiments are performed to validate the effectiveness of the proposed method using ambient frequency measurements at multiple geographic scales. Identification accuracy is presented, and the factors that affect identification performance are discussed.},
keywords={feedforward neural nets;frequency measurement;learning (artificial intelligence);power distribution reliability;power engineering computing;wavelet transforms;ambient frequency measurement;concurrent signal availability;unsourced ENF signal;machine learning;feedforward artificial neural network-based;wavelet-based signature extraction;ENF identification method;FNET-GridEye system;distribution-level ENF signal extraction;fingerprint location identification;environmental signature;multimedia recording authentication;forensic tool;electric power signal;multiple geographic scale;distribution-level electric network frequency signal extraction;source location identification;Frequency measurement;Data mining;Servers;Multimedia communication;Wavelet transforms;Multimedia databases;Distribution-level;ENF signal;frequency measurement;signature extraction;location identification},
doi={10.1109/ACCESS.2017.2707060},
ISSN={2169-3536},
month={},}
@ARTICLE{6165288,
author={Z. Guan and G. Miao and R. McLoughlin and X. Yan and D. Cai},
journal={IEEE Transactions on Knowledge and Data Engineering},
title={Co-Occurrence-Based Diffusion for Expert Search on the Web},
year={2013},
volume={25},
number={5},
pages={1001-1014},
abstract={Expert search has been studied in different contexts, e.g., enterprises, academic communities. We examine a general expert search problem: searching experts on the web, where millions of webpages and thousands of names are considered. It has mainly two challenging issues: 1) webpages could be of varying quality and full of noises; 2) The expertise evidences scattered in webpages are usually vague and ambiguous. We propose to leverage the large amount of co-occurrence information to assess relevance and reputation of a person name for a query topic. The co-occurrence structure is modeled using a hypergraph, on which a heat diffusion based ranking algorithm is proposed. Query keywords are regarded as heat sources, and a person name which has strong connection with the query (i.e., frequently co-occur with query keywords and co-occur with other names related to query keywords) will receive most of the heat, thus being ranked high. Experiments on the ClueWeb09 web collection show that our algorithm is effective for retrieving experts and outperforms baseline algorithms significantly. This work would be regarded as one step toward addressing the more general entity search problem without sophisticated NLP techniques.},
keywords={graph theory;Internet;query processing;co-occurrence-based diffusion;general expert search problem;Webpages;co-occurrence information;query topic;hypergraph;heat diffusion based ranking algorithm;query keywords;ClueWeb09 Web collection;general entity search problem;Web pages;Search problems;Noise;Computational modeling;Space heating;Conductivity;Expert search;web mining;hypergraph;diffusion},
doi={10.1109/TKDE.2012.49},
ISSN={1041-4347},
month={May},}
@ARTICLE{8262636,
author={K. Zhang and E. L. Xu and Z. Feng and P. Zhang},
journal={IEEE Access},
title={A Dictionary Learning Based Automatic Modulation Classification Method},
year={2018},
volume={6},
number={},
pages={5607-5617},
abstract={As the process of identifying the modulation format of the received signal, automatic modulation classification (AMC) has various applications in spectrum monitoring and signal interception. In this paper, we propose a dictionary learning-based AMC framework, where a dictionary is trained using signals with known modulation formats and the modulation format of the target signal is determined by its sparse representation on the dictionary. We also design a dictionary learning algorithm called block coordinate descent dictionary learning (BCDL). Furthermore, we prove the convergence of BCDL and quantify its convergence speed in a closed form. Simulation results show that our proposed AMC scheme offers superior performance than the existing methods with low complexity.},
keywords={adaptive modulation;learning (artificial intelligence);signal classification;signal representation;AMC framework;known modulation formats;target signal;automatic modulation classification method;received signal;spectrum monitoring;signal interception;sparse representation;block coordinate descent dictionary learning;Machine learning;Dictionaries;Training;Convergence;Frequency modulation;Computational modeling;Modulation classification;data driven;dictionary learning;block coordinate descent;sparse representation},
doi={10.1109/ACCESS.2018.2794587},
ISSN={2169-3536},
month={},}
@ARTICLE{7817894,
author={M. Tufano and F. Palomba and G. Bavota and R. Oliveto and M. D. Penta and A. De Lucia and D. Poshyvanyk},
journal={IEEE Transactions on Software Engineering},
title={When and Why Your Code Starts to Smell Bad (and Whether the Smells Go Away)},
year={2017},
volume={43},
number={11},
pages={1063-1088},
abstract={Technical debt is a metaphor introduced by Cunningham to indicate “not quite right code which we postpone making it right”. One noticeable symptom of technical debt is represented by code smells, defined as symptoms of poor design and implementation choices. Previous studies showed the negative impact of code smells on the comprehensibility and maintainability of code. While the repercussions of smells on code quality have been empirically assessed, there is still only anecdotal evidence on when and why bad smells are introduced, what is their survivability, and how they are removed by developers. To empirically corroborate such anecdotal evidence, we conducted a large empirical study over the change history of 200 open source projects. This study required the development of a strategy to identify smell-introducing commits, the mining of over half a million of commits, and the manual analysis and classification of over 10K of them. Our findings mostly contradict common wisdom, showing that most of the smell instances are introduced when an artifact is created and not as a result of its evolution. At the same time, 80 percent of smells survive in the system. Also, among the 20 percent of removed instances, only 9 percent are removed as a direct consequence of refactoring operations.},
keywords={data mining;public domain software;software maintenance;software quality;source code (software);open source projects;code smells;code comprehensibility;code maintainability;commits mining;smell instances;smell-introducing commits;code quality;technical debt;Ecosystems;History;Androids;Humanoid robots;Software systems;Maintenance engineering;Code smells;empirical study;mining software repositories},
doi={10.1109/TSE.2017.2653105},
ISSN={0098-5589},
month={Nov},}
@ARTICLE{8444617,
author={O. Kwon and S. Choi and B. Lee},
journal={IEEE Access},
title={A Watermark-Based Scheme for Authenticating JPEG Image Integrity},
year={2018},
volume={6},
number={},
pages={46194-46205},
abstract={JPEG is the most common image format in smartphones, computers, or digital cameras. Because numerous JPEG images are easily shared and distributed, there are privacy and security concerns for these images. Hence, the JPEG committee has started a standardization called JPEG privacy and security to resolve these issues. The forgery detection of a JPEG image is the key objective of JPEG privacy and security. In this paper, we propose a novel forgery detection method that is watermark-based, for authenticating JPEG image integrity in the discrete cosine transform (DCT) domain. The proposed method aims at 100% detection accuracy for typical forgeries on JPEG image DCT blocks and to counter a well-known forgery attack called collage attack. For this purpose, the method splits the host image into a group of blocks (GOB). A watermark is generated by collaborating with the neighboring GOBs and is embedded into the GOBs. The experimental results using various sample images show the superiority of the proposed method, exhibiting a negligible visual difference between the original and watermarked JPEG images.},
keywords={data compression;data privacy;discrete cosine transforms;image coding;image forensics;image watermarking;JPEG image watermarking;forgery detection method;sample images;host image;JPEG image DCT blocks;JPEG committee;security concerns;common image format;watermark-based scheme;original JPEG images;Transform coding;Watermarking;Privacy;Discrete cosine transforms;Forgery;Image coding;JPEG privacy and security;image watermarking;integrity authentication;image forgery detection},
doi={10.1109/ACCESS.2018.2866153},
ISSN={2169-3536},
month={},}
@ARTICLE{8258989,
author={T. Ma and J. Qu and W. Shen and Y. Tian and A. Al-Dhelaan and M. Al-Rodhaan},
journal={IEEE Access},
title={Weighted Greedy Dual Size Frequency Based Caching Replacement Algorithm},
year={2018},
volume={6},
number={},
pages={7214-7223},
abstract={Caches are used to improve the performance of the internet, and to reduce the latency of data access time and the low speed of repeated computing processes. Cache replacement is one of the most important issues in a caching system; therefore, it must be coordinated with the caching system to minimize the access latency and maximize the hit rate or byte hit rate. In this paper, we presented a novel caching replacement algorithm named Weighted Greedy Dual Size Frequency (WGDSF) algorithm, which is an improvement on the Greedy Dual Size Frequency (GDSF) algorithm. The WGDSF algorithm mainly adds weighted frequency-based time and weighted document type to GDSF. By increasing the above two weighted parameters, WGDSF performs fairly well at keeping popular objects in the cache and replacing rarely used ones. Our experiment shows that this algorithm has a better hit rate, byte hit rate and access latency than state-of-the-art algorithms, such least Recently Used, least Frequently Used, and GDSF.},
keywords={cache storage;data handling;greedy algorithms;Internet;Weighted Greedy Dual Size Frequency algorithm;GDSF;WGDSF algorithm;weighted frequency;weighted document type;access latency;data access time;caching system;byte hit rate;caching replacement algorithm;Internet performance;data source;data requester;Software algorithms;Algorithm design and analysis;Complexity theory;Time-frequency analysis;Program processors;Classification algorithms;Cache replacement;hit rate;weighted frequency},
doi={10.1109/ACCESS.2018.2790381},
ISSN={2169-3536},
month={},}
@ARTICLE{7058381,
author={Y. Brun and J. y. Bang and G. Edwards and N. Medvidovic},
journal={IEEE Transactions on Software Engineering},
title={Self-Adapting Reliability in Distributed Software Systems},
year={2015},
volume={41},
number={8},
pages={764-780},
abstract={Developing modern distributed software systems is difficult in part because they have little control over the environments in which they execute. For example, hardware and software resources on which these systems rely may fail or become compromised and malicious. Redundancy can help manage such failures and compromises, but when faced with dynamic, unpredictable resources and attackers, the system reliability can still fluctuate greatly. Empowering the system with self-adaptive and self-managing reliability facilities can significantly improve the quality of the software system and reduce reliance on the developer predicting all possible failure conditions. We present iterative redundancy, a novel approach to improving software system reliability by automatically injecting redundancy into the system's deployment. Iterative redundancy self-adapts in three ways: (1) by automatically detecting when the resource reliability drops, (2) by identifying unlucky parts of the computation that happen to deploy on disproportionately many compromised resources, and (3) by not relying on a priori estimates of resource reliability. Further, iterative redundancy is theoretically optimal in its resource use: Given a set of resources, iterative redundancy guarantees to use those resources to produce the most reliable version of that software system possible; likewise, given a desired increase in the system's reliability, iterative redundancy guarantees achieving that reliability using the least resources possible. Iterative redundancy handles even the Byzantine threat model, in which compromised resources collude to attack the system. We evaluate iterative redundancy in three ways. First, we formally prove its self-adaptation, efficiency, and optimality properties. Second, we simulate it at scale using discrete event simulation. Finally, we modify the existing, open-source, volunteer-computing BOINC software system and deploy it on the globally-distributed PlanetLab testbed network to empirically evaluate that iterative redundancy is self-adaptive and more efficient than existing techniques.},
keywords={discrete event simulation;distributed processing;public domain software;resource allocation;security of data;software quality;software reliability;system recovery;self-adapting reliability;distributed software systems;hardware resources;software resources;failure management;compromise management;dynamic unpredictable resources;system reliability;self-adaptive reliability;self-managing reliability;software system quality;failure condition;iterative redundancy;resource reliability estimate;Byzantine threat model;compromised resource collusion;optimality property;discrete event simulation;open-source volunteer-computing BOINC software system;globally-distributed PlanetLab testbed network;Redundancy;Software reliability;Software systems;Computational modeling;Servers;Reliability engineering;Redundancy;reliability;fault-tolerance;iterative redundancy;self-adaptation;optimal redundancy;Redundancy;reliability;fault-tolerance;iterative redundancy;self-adaptation;optimal redundancy},
doi={10.1109/TSE.2015.2412134},
ISSN={0098-5589},
month={Aug},}
@ARTICLE{7932851,
author={C. Huang and D. Wang},
journal={IEEE Access},
title={Exploring Scalability and Time-Sensitiveness in Reliable Social Sensing With Accuracy Assessment},
year={2017},
volume={5},
number={},
pages={14405-14418},
abstract={This paper presents a scalable estimation theoretic framework to address the time-sensitive truth discovery problem with accuracy assessment in social sensing applications. Social sensing has emerged as a new application paradigm that provides us with an unprecedented opportunity to collect observations about the physical world from humans or devices on their behalf. A fundamental challenge in social sensing applications lies in ascertaining the correctness of claims and the reliability of data sources without knowing either of them a priori, which is referred to as truth discovery. While significant progress has been made to solve the truth discovery problem, there exists three important limitations: (1) The information of users and claims in time dimension has not been fully exploited in the truth discovery solutions; (2) An analytical framework to rigorously assess the accuracy of the truth discovery results is lacking; and (3) Many current truth discovery schemes perform sequential operations, which are not scalable to large-scale social sensing events. To address the above limitations, we propose a scalable time-sensitive truth discovery (TS-TD) scheme that explicitly incorporates the source responsiveness and the claim lifespan into an estimation theoretical framework. Furthermore, we develop new confidence bounds to rigorously assess the accuracy of the truth discovery results. We also implement a parallel TS-TD algorithm on a graphic processing unit platform with thousands of cores to improve the computational efficiency. Finally, we evaluate the TS-TD scheme through three real-world case studies using Twitter data feeds and a simulation study. The evaluation results demonstrate the effectiveness and efficiency of our scheme.},
keywords={data mining;estimation theory;social networking (online);scalability;social sensing;accuracy assessment;scalable estimation;time sensitiveness;truth discovery;Twitter;Sensors;Reliability;Estimation;Graphics processing units;Scalability;Twitter;Data models;Time-sensitive;social sensing;truth discovery;scalability;accuracy assessment;performance bounds},
doi={10.1109/ACCESS.2017.2707480},
ISSN={2169-3536},
month={},}
@ARTICLE{7898472,
author={M. Choetkiertikul and H. K. Dam and T. Tran and A. Ghose and J. Grundy},
journal={IEEE Transactions on Software Engineering},
title={Predicting Delivery Capability in Iterative Software Development},
year={2018},
volume={44},
number={6},
pages={551-573},
abstract={Iterative software development has become widely practiced in industry. Since modern software projects require fast, incremental delivery for every iteration of software development, it is essential to monitor the execution of an iteration, and foresee a capability to deliver quality products as the iteration progresses. This paper presents a novel, data-driven approach to providing automated support for project managers and other decision makers in predicting delivery capability for an ongoing iteration. Our approach leverages a history of project iterations and associated issues, and in particular, we extract characteristics of previous iterations and their issues in the form of features. In addition, our approach characterizes an iteration using a novel combination of techniques including feature aggregation statistics, automatic feature learning using the Bag-of-Words approach, and graph-based complexity measures. An extensive evaluation of the technique on five large open source projects demonstrates that our predictive models outperform three common baseline methods in Normalized Mean Absolute Error and are highly accurate in predicting the outcome of an ongoing iteration.},
keywords={graph theory;learning (artificial intelligence);project management;software development management;software prototyping;software quality;iterative software development;incremental delivery;data-driven approach;project iterations;open source projects;delivery capability;software projects;feature aggregation statistics;automatic feature learning;Bag-of-Words;graph-based complexity;Normalized Mean Absolute Error;Software;Feature extraction;Predictive models;Data mining;Complexity theory;Iterative methods;Agile software development;Mining software engineering repositories;empirical software engineering;iterative software development},
doi={10.1109/TSE.2017.2693989},
ISSN={0098-5589},
month={June},}
@ARTICLE{8418357,
author={T. Tsai and W. Kuo},
journal={IEEE Access},
title={An Efficient ECG Lossless Compression System for Embedded Platforms With Telemedicine Applications},
year={2018},
volume={6},
number={},
pages={42207-42215},
abstract={This paper presents a method for wireless ECG compression and zero lossless decompression using a combination of three different techniques in order to increase storage space while reducing transmission time. The first technique used in the proposed algorithm is an adaptive linear prediction; it achieves high sensitivity and positive prediction. The second technique is content-adaptive Golomb-Rice coding, used with a window size to encode the residual of prediction error. The third technique is the use of a suitable packing format; this enables the real-time decoding process. The proposed algorithm is evaluated and verified using over 48 recordings from the MIT-BIH arrhythmia database, and it shown to be able to achieve a lossless bit compression rate of 2.83× in Lead V1 and 2.77× in Lead V2. The proposed algorithm shows better performance results in comparison to previous lossless ECG compression studies in real time; it can be used in data transmission methods for superior biomedical signals for bounded bandwidth across e-health devices. The overall compression system is also built with an ARM M4 processor, which ensures high accuracy performance and consistent results in the timing operation of the system.},
keywords={adaptive codes;data compression;decoding;electrocardiography;embedded systems;medical signal processing;telemedicine;lossless bit compression rate;data transmission methods;high accuracy performance;efficient ECG lossless compression system;embedded platforms;telemedicine applications;wireless ECG compression;zero lossless decompression;storage space;adaptive linear prediction;content-adaptive Golomb-Rice coding;window size;prediction error;real-time decoding process;MIT-BIH arrhythmia database;ARM M4 processor;e-health devices;bounded bandwidth;biomedical signals;transmission time reduction;Electrocardiography;Encoding;Prediction algorithms;Monitoring;Decoding;Biomedical monitoring;Transforms;Electro-cardiogram (ECG);Golomb-Rice coding;lossless data compression;wearable devices;healthcare monitoring;telemedicine},
doi={10.1109/ACCESS.2018.2858857},
ISSN={2169-3536},
month={},}
@ARTICLE{7933065,
author={Q. Zhang and D. Zhou and X. Zeng},
journal={IEEE Access},
title={HeartID: A Multiresolution Convolutional Neural Network for ECG-Based Biometric Human Identification in Smart Health Applications},
year={2017},
volume={5},
number={},
pages={11805-11816},
abstract={Body area networks, including smart sensors, are widely reshaping health applications in the new era of smart cities. To meet increasing security and privacy requirements, physiological signalbased biometric human identification is gaining tremendous attention. This paper focuses on two major impediments: the signal processing technique is usually both complicated and data-dependent and the feature engineering is time-consuming and can fit only specific datasets . To enable a data-independent and highly generalizable signal processing and feature learning process, a novel wavelet domain multiresolution convolutional neural network is proposed. Specifically, it allows for blindly selecting a physiological signal segment for identification purpose, avoiding the complicated signal fiducial characteristics extraction process. To enrich the data representation, the random chosen signal segment is then transformed to the wavelet domain, where multiresolution time-frequency representation is achieved. An auto-correlation operation is applied to the transformed data to remove the phase difference as the result of the blind segmentation operation. Afterward, a multiresolution 1-D-convolutional neural network (1-D-CNN) is introduced to automatically learn the intrinsic hierarchical features from the wavelet domain raw data without datadependent and heavy feature engineering, and perform the user identification task. The effectiveness of the proposed algorithm is thoroughly evaluated on eight electrocardiogram datasets with diverse behaviors, such as with or without severe heart diseases, and with different sensor placement methods. Our evaluation is much more extensive than the state-of-the-art works, and an average identification rate of 93.5% is achieved. The proposed multiresolution 1-D-CNN algorithm can effectively identify human subjects, even from randomly selected signal segments and without heavy feature engineering. This paper is expected to demonstrate the feasibility and effectiveness of applying the blind signal processing and deep learning techniques to biometric human identification, to enable a low algorithm engineering effort and also a high generalization ability.},
keywords={blind source separation;electrocardiography;learning (artificial intelligence);medical signal processing;neural nets;HeartID;multiresolution convolutional neural network;ECG-based biometric human identification;smart health applications;signal processing technique;blind signal processing;deep learning techniques;biometric human identification;body sensor networks;Electrocardiography;Signal resolution;Feature extraction;Heart rate variability;Convolution;Wavelet domain;Wavelet transforms;ECG;wavelet transformation;convolutional neural network;deep learning;machine learning;feature learning;blind signal processing;data representation},
doi={10.1109/ACCESS.2017.2707460},
ISSN={2169-3536},
month={},}
@ARTICLE{8055540,
author={P. Zhang and X. Zhou and P. Pelliccione and H. Leung},
journal={IEEE Access},
title={RBF-MLMR: A Multi-Label Metamorphic Relation Prediction Approach Using RBF Neural Network},
year={2017},
volume={5},
number={},
pages={21791-21805},
abstract={Metamorphic testing has been successfully used in many different fields to solve the test oracle problem. However, how to find a set of appropriate metamorphic relations for metamorphic testing remains a complicated and tedious task. Recently some machine learning approaches have been proposed to predict metamorphic relations. These approaches predicting single label metamorphic relation can alleviate this problem to some extent. However, many applications involve multi-group metamorphic relations, and these approaches are clearly inefficient. To address this problem, in this paper we propose a Multi-Label Metamorphic Relations prediction approach based on an improved radial basis function (RBF) neural network named RBF-MLMR. First, RBF-MLMR uses state-of-the-art soot analysis tool to generate control flow graph and corresponds labels from the source codes of programs. Second, the extracted nodes and the path properties constitute multi-label data sets for the control flow graph. Finally, a multi-label RBF neural network prediction model is established to predict whether the program satisfies multiple metamorphic relations. In order to improve the prediction results, affinity propagation and k-means clustering algorithms are used to optimize the RBF neural network structure of RBF-MLMR. A set of dedicated experiments based on public programs is conducted to validate RBF-MLMR. The experimental results show that RBF-MLMR can achieve accuracy of around 80% for predicting two and three metamorphic relations.},
keywords={learning (artificial intelligence);pattern clustering;program testing;radial basis function networks;RBF-MLMR;metamorphic testing;test oracle problem;appropriate metamorphic relations;single label metamorphic relation;multigroup metamorphic relations;multilabel data sets;multilabel RBF neural network prediction model;multiple metamorphic relations;RBF neural network structure;machine learning;radial basis function neural network;multilabel metamorphic relations prediction;Testing;Neural networks;Prediction algorithms;Software;Predictive models;Mathematical model;Algorithm design and analysis;Multi-label;metamorphic testing;metamorphic relation;label count vector;RBF neural network},
doi={10.1109/ACCESS.2017.2758790},
ISSN={2169-3536},
month={},}
@ARTICLE{7970115,
author={L. Chen and Y. Ho and H. Lee and H. Wu and H. Liu and H. Hsieh and Y. Huang and S. C. Lung},
journal={IEEE Access},
title={An Open Framework for Participatory PM2.5 Monitoring in Smart Cities},
year={2017},
volume={5},
number={},
pages={14441-14454},
abstract={As the population in cities continues to increase rapidly, air pollution becomes a serious issue from public health to social economy. Among all pollutants, fine particulate matters (PM2.5) directly related to various serious health concerns, e.g., lung cancer, premature death, asthma, and cardiovascular and respiratory diseases. To enhance the quality of urban living, sensors are deployed to create smart cities. In this paper, we present a participatory urban sensing framework for PM2.5 monitoring with more than 2500 devices deployed in Taiwan and 29 other countries. It is one of the largest deployment project for PM2.5 monitor in the world as we know until May 2017. The key feature of the framework is its open system architecture, which is based on the principles of open hardware, open source software, and open data. To facilitate the deployment of the framework, we investigate the accuracy issue of low-cost particle sensors with a comprehensive set of comparison evaluations to identify the most reliable sensor. By working closely with government authorities, industry partners, and maker communities, we can construct an effective eco-system for participatory urban sensing of PM2.5 particles. Based on our deployment achievements to date, we provide a number of data services to improve environmental awareness, trigger on-demand responses, and assist future government policymaking. The proposed framework is highly scalable and sustainable with the potential to facilitate the Internet of Things, smart cities, and citizen science in the future.},
keywords={air pollution;environmental monitoring (geophysics);government data processing;public administration;public domain software;smart cities;open framework;government policymaking;environmental awareness;open source software;open hardware;open system architecture;Taiwan;participatory urban sensing;health concerns;PM2.5 monitoring;fine particulate matters monitoring;smart cities;Sensors;Monitoring;Lungs;Air pollution;Hardware;Atmospheric modeling;Air pollution;crowdsourcing;environmental monitoring;Internet of Things},
doi={10.1109/ACCESS.2017.2723919},
ISSN={2169-3536},
month={},}
@ARTICLE{8049276,
author={Y. Cai and H. Wang and Z. Zheng and X. Sun},
journal={IEEE Access},
title={Scene-Adaptive Vehicle Detection Algorithm Based on a Composite Deep Structure},
year={2017},
volume={5},
number={},
pages={22804-22811},
abstract={Existing machine-learning-based vehicle detection algorithms for intelligent vehicles have an obvious disadvantage in that the detection effect decreases dramatically when the distribution of training samples and the scene target samples do not match. To address this issue, a scene-adaptive vehicle detection algorithm based on a composite deep structure is proposed in this paper. Inspired by the Bagging (Bootstrap aggregating) mechanism, multiple relatively independent source samples are first used to build multiple classifiers and then voting is used to generate target training samples with confidence scores. The automatic feature extraction ability of deep convolutional neural network is then used to perform source-target scene feature similarity calculations with a deep auto-encoder in order to design a composite deep-structure-based scene-adaptive classifier and its training method. Experiments on the KITTI data set and a data set captured by our group demonstrate that the proposed method performs better than existing machine-learning-based vehicle detection methods. In addition, compared with existing scene-adaptive object detection methods, our method improves the detection rate by an average of approximately 3%.},
keywords={computer vision;feature extraction;image classification;learning (artificial intelligence);neural nets;object detection;pattern classification;machine-learning;scene-adaptive object detection methods;vehicle detection algorithms;detection rate;vehicle detection methods;scene-adaptive classifier;composite deep-structure;deep auto-encoder;source-target scene;deep convolutional neural network;target training samples;multiple relatively independent source samples;composite deep structure;scene-adaptive vehicle detection algorithm;scene target samples;detection effect;intelligent vehicles;Training;Feature extraction;Vehicle detection;Training data;Classification algorithms;Detectors;Image recognition;vehicle detection;scene adaptive;composite deep structure;deep convolutional neural network},
doi={10.1109/ACCESS.2017.2756081},
ISSN={2169-3536},
month={},}
@ARTICLE{6954519,
author={Y. Yang and Y. Zhou and H. Lu and L. Chen and Z. Chen and B. Xu and H. Leung and Z. Zhang},
journal={IEEE Transactions on Software Engineering},
title={Are Slice-Based Cohesion Metrics Actually Useful in Effort-Aware Post-Release Fault-Proneness Prediction? An Empirical Study},
year={2015},
volume={41},
number={4},
pages={331-357},
abstract={Background. Slice-based cohesion metrics leverage program slices with respect to the output variables of a module to quantify the strength of functional relatedness of the elements within the module. Although slice-based cohesion metrics have been proposed for many years, few empirical studies have been conducted to examine their actual usefulness in predicting fault-proneness. Objective. We aim to provide an in-depth understanding of the ability of slice-based cohesion metrics in effort-aware post-release fault-proneness prediction, i.e. their effectiveness in helping practitioners find post-release faults when taking into account the effort needed to test or inspect the code. Method. We use the most commonly used code and process metrics, including size, structural complexity, Halstead's software science, and code churn metrics, as the baseline metrics. First, we employ principal component analysis to analyze the relationships between slice-based cohesion metrics and the baseline metrics. Then, we use univariate prediction models to investigate the correlations between slice-based cohesion metrics and post-release fault-proneness. Finally, we build multivariate prediction models to examine the effectiveness of slice-based cohesion metrics in effort-aware post-release fault-proneness prediction when used alone or used together with the baseline code and process metrics. Results. Based on open-source software systems, our results show that: 1) slice-based cohesion metrics are not redundant with respect to the baseline code and process metrics; 2) most slice-based cohesion metrics are significantly negatively related to post-release fault-proneness; 3) slice-based cohesion metrics in general do not outperform the baseline metrics when predicting post-release fault-proneness; and 4) when used with the baseline metrics together, however, slice-based cohesion metrics can produce a statistically significant and practically important improvement of the effectiveness in effort-aware post-release fault-proneness prediction. Conclusion. Slice-based cohesion metrics are complementary to the most commonly used code and process metrics and are of practical value in the context of effort-aware post-release fault-proneness prediction.},
keywords={principal component analysis;public domain software;software metrics;effort aware post-release fault proneness prediction;slice-based cohesion metrics leverage program slices;structural complexity;Halstead's software science;code churn metrics;baseline metrics;principal component analysis;univariate prediction models;multivariate prediction models;baseline code;process metrics;open source software systems;Measurement;Software;Predictive models;Context;Complexity theory;Correlation;Laboratories;Cohesion;metrics;slice-based;fault-proneness;prediction;effort-aware},
doi={10.1109/TSE.2014.2370048},
ISSN={0098-5589},
month={April},}
@ARTICLE{8438457,
author={Y. Zhang and W. Xiao},
journal={IEEE Access},
title={Keyphrase Generation Based on Deep Seq2seq Model},
year={2018},
volume={6},
number={},
pages={46047-46057},
abstract={Keyphrase can provide highly summative information which can help us improve information utilization efficiency in the era of information overload. Though previous researches about keyphrase generation have provided some workable solutions, they generate keyphrase by ranking and selecting meaningful words from the source text. These approaches belong to an extractive method, by which they cannot effectively use semantic meaning of the source text, and are unable to generate keyphrases which do not appear in the source text. So we propose a sequence-to-sequence framework with attention mechanism, copy mechanism, and coverage mechanism, which can effectively deal with the above-mentioned drawbacks. The experimental results on five data sets reveal that our proposed model can achieve a better performance than the traditional extraction approaches and can also generate absent keyphrases which do not appear in the source text.},
keywords={feature extraction;information retrieval;text analysis;deep seq2seq model;information utilization efficiency;information overload;keyphrase generation;sequence-to-sequence framework;attention mechanism;copy mechanism;coverage mechanism;Task analysis;Semantics;Decoding;Correlation;Logic gates;Recurrent neural networks;Machine learning algorithms;Abstraction;seq2seq;attention mechanism;copy mechanism;coverage mechanism},
doi={10.1109/ACCESS.2018.2865589},
ISSN={2169-3536},
month={},}
@ARTICLE{8375093,
author={S. Vishwakarma and S. S. Ram},
journal={IEEE Access},
title={Dictionary Learning With Low Computational Complexity for Classification of Human Micro-Dopplers Across Multiple Carrier Frequencies},
year={2018},
volume={6},
number={},
pages={29793-29805},
abstract={Recently, several machine learning algorithms have been applied for classifying micro-Doppler signatures from different human motions. However, these algorithms must demonstrate versatility in handling diversity in test and training data to be used for real-life scenarios. For example, situations may arise where the propagation channel or the presence of interference sources in the test site will permit only specific frequency bands of radar operation. These bands may differ from those used previously while training. In this paper, we examine the performances of three sparsity driven dictionary learning algorithms-synthesis, deep, and analysis-for learning unique features extracted from training data gathered across multiple carrier frequencies. These features are subsequently used for classifying test data from another distinct carrier frequency. Our experimental results, from measurement data, show that the dictionary learning algorithms are capable of extracting meaningful representations of the micro-Dopplers despite the rich frequency diversity in the data. In particular, the deep dictionary learning algorithm yields a high classification accuracy of 91% with a very low computational time for testing.},
keywords={computational complexity;Doppler radar;feature extraction;image classification;learning (artificial intelligence);different human motions;real-life scenarios;propagation channel;interference sources;specific frequency bands;radar operation;multiple carrier frequencies;distinct carrier frequency;measurement data;dictionary learning algorithms;rich frequency diversity;deep dictionary;human microDopplers;machine learning algorithms;microDoppler signatures;features extraction;computational complexity;computational time;Machine learning;Dictionaries;Radar;Training;Feature extraction;Classification algorithms;Training data;Radar;micro-Dopplers;sparse coding;synthesis dictionary learning;deep learning;analysis dictionary learning;classification},
doi={10.1109/ACCESS.2018.2843391},
ISSN={2169-3536},
month={},}
@ARTICLE{5560680,
author={Y. Shin and A. Meneely and L. Williams and J. A. Osborne},
journal={IEEE Transactions on Software Engineering},
title={Evaluating Complexity, Code Churn, and Developer Activity Metrics as Indicators of Software Vulnerabilities},
year={2011},
volume={37},
number={6},
pages={772-787},
abstract={Security inspection and testing require experts in security who think like an attacker. Security experts need to know code locations on which to focus their testing and inspection efforts. Since vulnerabilities are rare occurrences, locating vulnerable code locations can be a challenging task. We investigated whether software metrics obtained from source code and development history are discriminative and predictive of vulnerable code locations. If so, security experts can use this prediction to prioritize security inspection and testing efforts. The metrics we investigated fall into three categories: complexity, code churn, and developer activity metrics. We performed two empirical case studies on large, widely used open-source projects: the Mozilla Firefox web browser and the Red Hat Enterprise Linux kernel. The results indicate that 24 of the 28 metrics collected are discriminative of vulnerabilities for both projects. The models using all three types of metrics together predicted over 80 percent of the known vulnerable files with less than 25 percent false positives for both projects. Compared to a random selection of files for inspection and testing, these models would have reduced the number of files and the number of lines of code to inspect or test by over 71 and 28 percent, respectively, for both projects.},
keywords={Linux;online front-ends;program testing;public domain software;software fault tolerance;software metrics;code churn;software vulnerabilities;developer activity metrics;security inspection;software metrics;source code;vulnerable code locations;open-source projects;Mozilla Firefox Web browser;Red Hat enterprise Linux kernel;Fault diagnosis;Software security;Complexity theory;Predictive models;Charge coupled devices;Fault prediction;software metrics;software security;vulnerability prediction.},
doi={10.1109/TSE.2010.81},
ISSN={0098-5589},
month={Nov},}
@ARTICLE{7426825,
author={W. Wei and G. Cong and C. Miao and F. Zhu and G. Li},
journal={IEEE Transactions on Knowledge and Data Engineering},
title={Learning to Find Topic Experts in Twitter via Different Relations},
year={2016},
volume={28},
number={7},
pages={1764-1778},
abstract={Expert finding has become a hot topic along with the flourishing of social networks, such as micro-blogging services like Twitter. Finding experts in Twitter is an important problem because tweets from experts are valuable sources that carry rich information (e.g., trends) in various domains. However, previous methods cannot be directly applied to Twitter expert finding problem. Recently, several attempts use the relations among users and Twitter Lists for expert finding. Nevertheless, these approaches only partially utilize such relations. To this end, we develop a probabilistic method to jointly exploit three types of relations (i.e., follower relation, user-list relation, and list-list relation) for finding experts. Specifically, we propose a Semi-Supervised Graph-based Ranking approach (SSGR) to offline calculate the global authority of users. In SSGR, we employ a normalized Laplacian regularization term to jointly explore the three relations, which is subject to the supervised information derived from Twitter crowds. We then online compute the local relevance between users and the given query. By leveraging the global authority and local relevance of users, we rank all of users and find top-N users with highest ranking scores. Experiments on real-world data demonstrate the effectiveness of our proposed approach for topic-specific expert finding in Twitter.},
keywords={graph theory;learning (artificial intelligence);probability;query processing;relevance feedback;search engines;social networking (online);topic-specific expert finding;ranking scores;top-N users;query processing;local relevance;Twitter crowds;supervised information;Laplacian regularization term;SSGR;global authority;semisupervised graph-based ranking approach;list-list relation;user-list relation;follower relation;probabilistic method;Twitter lists;Twitter expert finding problem;tweets;microblogging services;social networks;Twitter;Search problems;Natural language processing;Knowledge discovery;Probabilistic logic;Laplace equations;Expert search;micro-blogging;twitter;list;graph-based ranking;Expert search;micro-blogging;Twitter;list;graph-based ranking},
doi={10.1109/TKDE.2016.2539166},
ISSN={1041-4347},
month={July},}
@ARTICLE{8233154,
author={P. Jing and Y. Su and L. Nie and X. Bai and J. Liu and M. Wang},
journal={IEEE Transactions on Knowledge and Data Engineering},
title={Low-Rank Multi-View Embedding Learning for Micro-Video Popularity Prediction},
year={2018},
volume={30},
number={8},
pages={1519-1532},
abstract={Recently, a prevailing trend of user generated content (UGC) on social media sites is the emerging micro-videos. Microvideos afford many potential opportunities ranging from network content caching to online advertising, yet there are still little efforts dedicated to research on micro-video understanding. In this paper, we focus on popularity prediction of micro-videos by presenting a novel low-rank multi-view embedding learning framework. We name it as transductive low-rank multi-view regression (TLRMVR), and it is capable of boosting the performance of micro-video popularity prediction by jointly considering the intrinsic representations of the source and target samples. In particular, TLRMVR integrates low-rank multi-view embedding and regression analysis into a unified framework such that the lowest-rank representation shared by all views not only captures the global structure of all views, but also indicates the regression requirements. The framework is formulated as a regression model and it seeks a set of view-specific projection matrices with low-rank constraints to map multi-view features into a common subspace. In addition, a multi-graph regularization term is constructed to improve the generalization capability and further prevents the overfitting problem. Extensive experiments conducted on a publicly available dataset demonstrate that our proposed method achieve promising results as compared with state-of-the-art baselines.},
keywords={graph theory;image representation;learning (artificial intelligence);matrix algebra;regression analysis;social networking (online);view-specific projection matrices;low-rank constraints;map multiview features;multigraph regularization term;microvideo popularity prediction;social media sites;network content caching;regression analysis;lowest-rank representation;low-rank multiview embedding learning framework;transductive low-rank multiview regression;user generated content;UGC;online advertising;TLRMVR;Videos;Feature extraction;Sparse matrices;Matrix decomposition;Noise measurement;Social network services;Low-rank learning;multi-view fusion;subspace learning;popularity prediction;micro-video analysis},
doi={10.1109/TKDE.2017.2785784},
ISSN={1041-4347},
month={Aug},}
@ARTICLE{7270338,
author={M. Mirakhorli and J. Cleland-Huang},
journal={IEEE Transactions on Software Engineering},
title={Detecting, Tracing, and Monitoring Architectural Tactics in Code},
year={2016},
volume={42},
number={3},
pages={205-220},
abstract={Software architectures are often constructed through a series of design decisions. In particular, architectural tactics are selected to satisfy specific quality concerns such as reliability, performance, and security. However, the knowledge of these tactical decisions is often lost, resulting in a gradual degradation of architectural quality as developers modify the code without fully understanding the underlying architectural decisions. In this paper we present a machine learning approach for discovering and visualizing architectural tactics in code, mapping these code segments to tactic traceability patterns, and monitoring sensitive areas of the code for modification events in order to provide users with up-to-date information about underlying architectural concerns. Our approach utilizes a customized classifier which is trained using code extracted from fifty performance-centric and safety-critical open source software systems. Its performance is compared against seven off-the-shelf classifiers. In a controlled experiment all classifiers performed well; however our tactic detector outperformed the other classifiers when used within the larger context of the Hadoop Distributed File System. We further demonstrate the viability of our approach for using the automatically detected tactics to generate viable and informative messages in a simulation of maintenance events mined from Hadoop's change management system.},
keywords={learning (artificial intelligence);pattern classification;program diagnostics;public domain software;software architecture;system monitoring;code architectural tactics monitoring;code architectural tactics detection;code architectural tactics tracing;machine learning;code architectural tactics visualization;tactic traceability patterns;performance-centric software systems;safety-critical open source software systems;off-the-shelf classifiers;Hadoop Distributed File System;Hadoops change management system;Heart beat;Monitoring;Detectors;Reliability;Biomedical monitoring;Authentication;Architecture;traceability;tactics;traceability information models;Architecture;traceability;tactics;traceability information models},
doi={10.1109/TSE.2015.2479217},
ISSN={0098-5589},
month={March},}
@ARTICLE{7781667,
author={S. Chen and T. Liu and C. Shen and M. Tuan},
journal={IEEE Access},
title={VLSI Implementation of a Cost-Efficient Near-Lossless CFA Image Compressor for Wireless Capsule Endoscopy},
year={2016},
volume={4},
number={},
pages={10235-10245},
abstract={In this paper, a novel near-lossless color filter array (CFA) image compression algorithm based on JPEG-LS is proposed for VLSI implementation. It consists of a pixel restoration, a prediction, a run mode, and entropy coding modules. According to the information of the previous research, a context table and row memory consumed more than 81% hardware cost in a JPEG-LS encoder design. Hence, in this paper, a novel context-free and near-lossless image compression algorithm is presented. Since removing the context model causes decreasing of the compression performance, a novel prediction, run mode, and modified Golomb-Rice coding techniques were used to improve the compression efficiency. The VLSI architecture of the proposed image compressor consists of a register bank, a pixel restoration module, a predictor, a run mode module, and an entropy encoder. A pipeline technique was used to improve the performance of this. It contains only 10.9k gate count, and the core area is 30625 μm2, synthesized by using a 90-nm CMOS process. Compared with the previous JPEG-LS designs, this paper reduces the gate counts by 44.1% and 41.7%, respectively, for five standard and eight endoscopy testing images in CFA format. It also improves the average PSNR values by 0.96 and 0.43 dB, respectively, for the same test images.},
keywords={CMOS integrated circuits;data compression;endoscopes;entropy codes;image coding;image filtering;medical image processing;optical arrays;optical filters;VLSI;cost-efficient near-lossless CFA image compressor;wireless capsule endoscopy;near-lossless color filter array image compression algorithm;pixel restoration;entropy coding modules;context table;row memory;hardware cost;JPEG-LS encoder design;context-free image compression algorithm;context model removal;modified Golomb-Rice coding techniques;VLSI architecture;register bank;pixel restoration module;predictor;run mode module;pipeline technique;CMOS process;size 90 nm;Image coding;Prediction algorithms;Endoscopes;Predictive models;Algorithm design and analysis;Wireless communication;Image color analysis;Color filter array;context-free;Golomb-Rice coding;JPEG-LS;near-lossless;run mode;VLSI;wireless capsule endoscopy},
doi={10.1109/ACCESS.2016.2638475},
ISSN={2169-3536},
month={},}
@ARTICLE{7878524,
author={S. Hakak and A. Kamsin and O. Tayan and M. Y. Idna Idris and A. Gani and S. Zerdoumi},
journal={IEEE Access},
title={Preserving Content Integrity of Digital Holy Quran: Survey and Open Challenges},
year={2017},
volume={5},
number={},
pages={7305-7325},
abstract={In recent years, a new trend has come up, which is that of reading the digital Quran online. This text was revealed more than 1400 years ago in the Arabic language and has been protected from all possible ways of distortion until today. Unfortunately, driven by the desire to make profit or gain publicity, fraudsters have started modifying certain Quranic verses. These alterations are misleading many people who are thus deprived of the original and accurate message of the Holy Quran. This paper focuses on systematically analyzing and categorizing existing research related to preserving and verifying the content integrity of the Quran. This paper further assesses these existing studies in terms of their evaluation parameters and findings. We find that the existing studies can be classified according to their format and methods, i.e., the online formats in which the Quranic content is available, methods employed to protect the Quranic content from modification, and last methods of verification. This paper concludes with the issue of future challenges and their possible solutions.},
keywords={data integrity;fraud;humanities;Internet;security of data;content integrity;digital Holy Quran;Arabic language;fraudsters;Quranic verses;Quranic content;Internet;Authentication;Computer science;Market research;Monitoring;Libraries;Taxonomy;Quran;content integrity;hadith validation;Quran verse authentication;tampering;authentication},
doi={10.1109/ACCESS.2017.2682109},
ISSN={2169-3536},
month={},}
@ARTICLE{7867793,
author={Q. Tong and J. Cao and B. Han and X. Zhang and Z. Nie and J. Wang and Y. Lin and W. Zhang},
journal={IEEE Access},
title={A Fault Diagnosis Approach for Rolling Element Bearings Based on RSGWPT-LCD Bilayer Screening and Extreme Learning Machine},
year={2017},
volume={5},
number={},
pages={5515-5530},
abstract={The faults of rolling element bearings can result in the deterioration of machine operating conditions; how to assess the working condition and identify the fault of the rolling element bearing has become a key issue for ensuring the safe operation of modern rotating machineries. This paper presents a novel hybrid approach that detects bearing faults and monitors the operating status of rolling element bearings in modern rotating machineries. Based on redundant second-generation wavelet packet transform and local characteristic-scale decomposition, this method is implemented to extract the fault features, the vibration signal is adaptively decomposed into a number of desired intrinsic scale components by two-step screening processes based on the energy ratio, and reduce random noises and eliminate the pseudofrequency components. The fault features are then used to implement the identification classification of faults using singular value decomposition and extreme learning machine. The approach is evaluated by simulation and practical bearing vibration signals under different conditions. The experiment results show that the proposed approach is feasible and effective for the fault diagnosis of rolling element bearing.},
keywords={condition monitoring;fault diagnosis;learning (artificial intelligence);rolling bearings;singular value decomposition;transforms;vibrational signal processing;fault diagnosis;rolling element bearings;RSGWPT-LCD bilayer screening;extreme learning machine;condition monitoring;vibration signal;singular value decomposition;redundant second generation wavelet packet transform;local characteristic scale decomposition;Fault diagnosis;Rolling bearings;Wavelet transforms;Feature extraction;Signal resolution;Time-frequency analysis;Rolling element bearings;fault diagnosis;redundant second generation wavelet packet transform (RSGWPT);local characteristic-scale decomposition (LCD);energy ratio;singular value decomposition (SVD);extreme learning machine (ELM)},
doi={10.1109/ACCESS.2017.2675940},
ISSN={2169-3536},
month={},}
@ARTICLE{7805255,
author={W. Yang and X. Cui and J. Liu and Y. Liu},
journal={IEEE Access},
title={Identification of Potential Collective Actions Using Enhanced Gray System Theory on Social Media},
year={2016},
volume={4},
number={},
pages={9184-9192},
abstract={A collective action that considerably affects government management and public security, e.g., a mass demonstration, usually experiences a long development period, originating from small and uncertain variations called weak signals on social media. Researchers generally identify collective action by small changes in communication frequency, emerging key words, and sentiment. However, most studies only consider the present environment, which may not evolve into a collective action, or conduct a short-term prediction in which significant damage is already done when the collective action is identified. This paper proposes a predictive framework to identify potential collective actions, considering the future evolution as well as the present situation, and providing a reference for early decision making. In the framework, a future sign to describe events is improved and the enhanced gray system theory is used to predict the evolution of a future sign. Mentions of events surrounding the Arab Spring-using over 300000 different open-content Web sources crawled from social media in seven different languages-are analyzed, which suggests that the predictive framework can more precisely identify the weak signals of collective actions.},
keywords={government data processing;Internet;social networking (online);weak signals;open content Web sources;Arab Spring;gray system theory;predictive framework;collective action;communication frequency;public security;government management;social media;enhanced Gray system theory;potential collective action identification;Social network services;Solid modeling;Government policies;Decision making;Security;Collective action;Semiotics;Reflective binary codes;Future sign;weak signal;collective action;gray system theory},
doi={10.1109/ACCESS.2017.2647823},
ISSN={2169-3536},
month={},}
@ARTICLE{8085127,
author={Y. Liu and J. E. Fieldsend and G. Min},
journal={IEEE Access},
title={A Framework of Fog Computing: Architecture, Challenges, and Optimization},
year={2017},
volume={5},
number={},
pages={25445-25454},
abstract={Fog computing (FC) is an emerging distributed computing platform aimed at bringing computation close to its data sources, which can reduce the latency and cost of delivering data to a remote cloud. This feature and related advantages are desirable for many Internet-of-Things applications, especially latency sensitive and mission intensive services. With comparisons to other computing technologies, the definition and architecture of FC are presented in this paper. The framework of resource allocation for latency reduction combined with reliability, fault tolerance, privacy, and underlying optimization problems are also discussed. We then investigate an application scenario and conduct resource optimization by formulating the optimization problem and solving it via a genetic algorithm. The resulting analysis generates some important insights on the scalability of the FC systems.},
keywords={cloud computing;fault tolerance;genetic algorithms;Internet;resource allocation;optimization problem;fog computing;data sources;remote cloud;Internet-of-Things applications;latency sensitive mission intensive services;computing technologies;latency reduction;conduct resource optimization;distributed computing platform;Cloud computing;Computer architecture;Edge computing;Optimization;Logic gates;Big Data;Fog computing;genetic algorithms;Internet of Things;optimization},
doi={10.1109/ACCESS.2017.2766923},
ISSN={2169-3536},
month={},}
@ARTICLE{7938609,
author={J. Pati and B. Kumar and D. Manjhi and K. K. Shukla},
journal={IEEE Access},
title={A Comparison Among ARIMA, BP-NN, and MOGA-NN for Software Clone Evolution Prediction},
year={2017},
volume={5},
number={},
pages={11841-11851},
abstract={Software evolution continues throughout the life cycle of the software. During the evolution of software system, it has been observed that the developers have a tendency to copy the modules completely or partially and modify them. This practice gives rise to identical or very similar code fragments called software clones. This paper examines the evolution of clone components by using advanced time series analysis. In the first phase, software clone components are extracted from the source repository of the software application by using the abstract syntax tree approach. Then, the evolution of software clone components is analyzed. In this paper, three models, Autoregressive Integrated Moving Average, back propagation neural network, and multi-objective genetic algorithm-based neural network, have been compared for the prediction of the evolution of software clone components. Evaluation is performed on the large open-source software application, ArgoUML. The ability to predict the clones helps the software developer to reduce the effort during software maintenance activities.},
keywords={autoregressive moving average processes;backpropagation;genetic algorithms;neural nets;public domain software;software maintenance;time series;software maintenance activities;ArgoUML;open-source software application;multiobjective genetic algorithm-based neural network;back propagation neural network;autoregressive integrated moving average;abstract syntax tree approach;software clone components;time series analysis;software clone evolution prediction;MOGA-NN;BP-NN;ARIMA;Software;Cloning;Time series analysis;Unified modeling language;Sociology;Predictive models;Software clones;software clone evolution;ARIMA;back propagation;MOGA-NN;time series analysis},
doi={10.1109/ACCESS.2017.2707539},
ISSN={2169-3536},
month={},}
@ARTICLE{7270978,
author={B. S. Riggan and C. Reale and N. M. Nasrabadi},
journal={IEEE Access},
title={Coupled Auto-Associative Neural Networks for Heterogeneous Face Recognition},
year={2015},
volume={3},
number={},
pages={1620-1632},
abstract={Several models have been previously suggested for learning correlated representations between source and target modalities. In this paper, we propose a novel coupled autoassociative neural network for learning a target-to-source image representation for heterogenous face recognition. This coupled network is unique, because a cross-modal transformation is learned by forcing the hidden units (latent features) of two neural networks to be as similar as possible, while simultaneously preserving information from the input. The effectiveness of this model is demonstrated using multiple existing heterogeneous face recognition databases. Moreover, the empirical results show that the learned image representation-common latent features-by the coupled auto-associative produces competitive cross-modal face recognition results. These results are obtained by training a softmax classifier using only the latent features from the source domain and testing using only the latent features from the target domain.},
keywords={face recognition;feature extraction;image classification;learning (artificial intelligence);neural nets;visual databases;coupled autoassociative neural networks;correlated representation learning;source modalities;target modalities;target-to-source image representation learning;cross-modal transformation;hidden units;information preservation;heterogeneous face recognition databases;empirical analysis;competitive cross-modal face recognition;softmax classifier training;source domain;target domain;latent features;Face recognition;Feature extraction;Biometrics;Neural networks;Cross modality;cross-modality;heterogeneous face recognition;common latent features;biometrics;neural networks;Cross-modality;heterogeneous face recognition;common latent features;biometrics;neural networks},
doi={10.1109/ACCESS.2015.2479620},
ISSN={2169-3536},
month={},}
@ARTICLE{7463060,
author={J. Xuan and M. Martinez and F. DeMarco and M. Clément and S. L. Marcote and T. Durieux and D. Le Berre and M. Monperrus},
journal={IEEE Transactions on Software Engineering},
title={Nopol: Automatic Repair of Conditional Statement Bugs in Java Programs},
year={2017},
volume={43},
number={1},
pages={34-55},
abstract={We propose Nopol, an approach to automatic repair of buggy conditional statements (i.e., if-then-else statements). This approach takes a buggy program as well as a test suite as input and generates a patch with a conditional expression as output. The test suite is required to contain passing test cases to model the expected behavior of the program and at least one failing test case that reveals the bug to be repaired. The process of Nopol consists of three major phases. First, Nopol employs angelic fix localization to identify expected values of a condition during the test execution. Second, runtime trace collection is used to collect variables and their actual values, including primitive data types and objected-oriented features (e.g., nullness checks), to serve as building blocks for patch generation. Third, Nopol encodes these collected data into an instance of a Satisfiability Modulo Theory (SMT) problem; then a feasible solution to the SMT instance is translated back into a code patch. We evaluate Nopol on 22 real-world bugs (16 bugs with buggy if conditions and six bugs with missing preconditions) on two large open-source projects, namely Apache Commons Math and Apache Commons Lang. Empirical analysis on these bugs shows that our approach can effectively fix bugs with buggy if conditions and missing preconditions. We illustrate the capabilities and limitations of Nopol using case studies of real bug fixes.},
keywords={computability;Java;object-oriented programming;program debugging;public domain software;software fault tolerance;software maintenance;Nopol;automatic conditional statement bug repairing;Java programs;buggy program;conditional expression;angelic fix localization;test execution;runtime trace collection;objected-oriented features;patch generation;satisfiability modulo theory problem;SMT problem;code patch;buggy IF conditions;open-source projects;Apache Commons Math;Apache Commons Lang;Maintenance engineering;Computer bugs;Runtime;Java;Encoding;Open source software;Indexes;Automatic repair;patch generation;SMT;fault localization},
doi={10.1109/TSE.2016.2560811},
ISSN={0098-5589},
month={Jan},}
@ARTICLE{8306877,
author={L. Du and L. Zhang and X. Tian and J. Lei},
journal={IEEE Access},
title={Efficient Forecasting Scheme and Optimal Delivery Approach of Energy for the Energy Internet},
year={2018},
volume={6},
number={},
pages={15026-15038},
abstract={The energy Internet (EI) is an important infrastructure for effectively utilizing and intelligently managing renewable energy sources (RES). In this paper, we study the architecture design of the EI under the backdrop of large-scale RES grid connection and the efficient forecasting and optimal utilization of energy. The contribution of this paper is threefold. First, we design a hierarchical integration architecture for the EI and attempt to solve the issues of energy and information management that stem from large-scale RES grid connection. Second, we propose a novel energy forecasting scheme that significantly reduces the amount of effort and ensures the accuracy of formulating the energy forecasting as an instance of the matrix completion issue. Third, we take electric vehicle charging as a typical case and propose the use of reinforcement learning to achieve optimal energy delivery. An experimental evaluation of real-world data sets validates the expectations of the study and highlights the superiorities of our proposed approaches.},
keywords={electric vehicle charging;Internet;learning (artificial intelligence);matrix algebra;power engineering computing;power grids;renewable energy sources;hierarchical integration architecture;EI;information management;large-scale RES grid connection;energy forecasting scheme;optimal energy delivery;energy Internet;renewable energy sources;architecture design;matrix completion issue;reinforcement learning;electric vehicle charging;Forecasting;Electric vehicle charging;Real-time systems;Internet;Telecommunications;Information management;Learning (artificial intelligence);Energy internet;energy forecasting and delivery;matrix completion;reinforcement learning},
doi={10.1109/ACCESS.2018.2812211},
ISSN={2169-3536},
month={},}
@ARTICLE{8403213,
author={L. Guo and D. Zhou and J. Zhou and S. Kimura and S. Goto},
journal={IEEE Access},
title={Lossy Compression for Embedded Computer Vision Systems},
year={2018},
volume={6},
number={},
pages={39385-39397},
abstract={Computer vision applications are rapidly gaining popularity in embedded systems, which typically involve a difficult tradeoff between vision performance and energy consumption under a constraint of real-time processing throughput. Recently, hardware (FPGA and ASIC-based) implementations have emerged, which significantly improves the energy efficiency of vision computation. These implementations, however, often involve intensive memory traffic that retains a significant portion of energy consumption at the system level. To address this issue, we are the first researchers to present a lossy compression framework to exploit the tradeoff between vision performance and memory traffic for input images. To meet various requirements for memory access patterns in the vision system, a line-to-block format conversion is designed for the framework. Differential pulse-code modulation-based gradient-oriented quantization is developed as the lossy compression algorithm. We also present its hardware design that supports up to 12-scale 1080p@60fps real-time processing. For histogram of oriented gradient-based deformable part models on VOC2007, the proposed framework achieves a 49.6%-60.5% memory traffic reduction at a detection rate degradation of 0.05%-0.34%. For AlexNet on ImageNet, memory traffic reduction achieves up to 60.8% with less than 0.61% classification rate degradation. Compared with the power consumption reduction from memory traffic, the overhead involved for the proposed input image compression is less than 5%.},
keywords={computer vision;data compression;differential pulse code modulation;embedded systems;gradient methods;energy efficiency;vision computation;intensive memory traffic;energy consumption;lossy compression framework;vision performance;memory access patterns;vision system;line-to-block format conversion;lossy compression algorithm;hardware design;power consumption reduction;input image compression;embedded computer vision systems;computer vision applications;real-time processing throughput;differential pulse-code modulation-based gradient-oriented quantization;Image coding;Random access memory;Feature extraction;Hardware;Memory management;Computer vision;Power demand;Computer vision;feature extraction;lossy compression;memory traffic reduction},
doi={10.1109/ACCESS.2018.2852809},
ISSN={2169-3536},
month={},}
@ARTICLE{7954949,
author={J. Liao and X. Zhuang and R. Fan and X. Peng},
journal={IEEE Access},
title={Toward a General Distributed Messaging Framework for Online Transaction Processing Applications},
year={2017},
volume={5},
number={},
pages={18166-18178},
abstract={This paper presents a general distributed messaging framework for online transaction processing applications in e-business industry, and we name it as Metamorphosis (MetaQ). Specifically, this messaging framework has features of high availability, scalability, and high data throughput. The current implementation of MetaQ supports distributed XA transactions, asynchronous messaging, and multiple ways for storing message offsets. As a consequence, it is suited to the application contexts having a large quantity of messages, transaction-support, and real-time requirements. More important, another branch of MetaQ implementation, i.e., RocketMQ has been deployed and used in Taobao.com and Alipay.com. The real usages in both typical online transaction applications have proven that the nature of MetaQ can perform well for such big Internet applications.},
keywords={data mining;electronic commerce;Internet;transaction processing;Taobao.com;Alipay.com;distributed XA transactions;Metamorphosis;e-business industry;Internet applications;MetaQ implementation;asynchronous messaging;online transaction processing applications;general distributed messaging framework;Servers;Message service;Computer architecture;Industries;Distributed messaging middleware;high performance;XA transaction support;Internet application;open source software},
doi={10.1109/ACCESS.2017.2717930},
ISSN={2169-3536},
month={},}
@ARTICLE{8027035,
author={M. Hooshmand and D. Zordan and T. Melodia and M. Rossi},
journal={IEEE Access},
title={SURF: Subject-Adaptive Unsupervised ECG Signal Compression for Wearable Fitness Monitors},
year={2017},
volume={5},
number={},
pages={19517-19535},
abstract={Recent advances in wearable devices allow non-invasive and inexpensive collection of biomedical signals including electrocardiogram (ECG), blood pressure, respiration, among others. Collection and processing of various biomarkers are expected to facilitate preventive healthcare through personalized medical applications. Since wearables are based on sizeand resource-constrained hardware, and are battery operated, they need to run lightweight algorithms to efficiently manage energy and memory. To accomplish this goal, this paper proposes SURF, a subject-adaptive unsupervised signal compressor for wearable fitness monitors. The core idea is to perform a specialized lossy compression algorithm on the ECG signal at the source (wearable device), to decrease the energy consumption required for wireless transmission and thus prolong the battery lifetime. SURF leverages unsupervised learning techniques to build and maintain, at runtime, a subject-adaptive dictionary without requiring any prior information on the signal. Dictionaries are constructed within a suitable feature space, allowing the addition and removal of code words according to the signal's dynamics (for given target fidelity and energy consumption objectives). Extensive performance evaluation results, obtained with reference ECG traces and with our own measurements from a commercial wearable wireless monitor, show the superiority of SURF against state-of-the-art techniques, including: 1) compression ratios up to 90-times; 2) reconstruction errors between 2% and 7% of the signal's range (depending on the amount of compression sought); and 3) reduction in energy consumption of up to two orders of magnitude with respect to sending the signal uncompressed, while preserving its morphology. SURF, with artifact prone ECG signals, allows for typical compression efficiencies (CE) in the range CE ∈ [40, 50], which means that the data rate of 3 kbit/s that would be required to send the uncompressed ECG trace is lowered to 60 and 75 bit/s for CE = 40 and CE = 50, respectively.},
keywords={biomedical telemetry;data compression;electrocardiography;health care;medical signal processing;patient monitoring;unsupervised learning;subject-adaptive unsupervised ECG signal compression;wearable device;biomedical signals;blood pressure;preventive healthcare;personalized medical applications;lightweight algorithms;subject-adaptive unsupervised signal compressor;wearable fitness monitors;specialized lossy compression algorithm;subject-adaptive dictionary;energy consumption objectives;extensive performance evaluation results;commercial wearable wireless monitor;artifact prone ECG signals;uncompressed ECG trace;SURF;unsupervised learning techniques;compression efficiencies;CE;electrocardiogram;Electrocardiography;Dictionaries;Biomedical monitoring;Wireless communication;Vector quantization;Monitoring;Wireless sensor networks;Biomedical signal processing;data compression;energy efficiency;self-organizing feature maps;unsupervised learning;wearable sensors},
doi={10.1109/ACCESS.2017.2749758},
ISSN={2169-3536},
month={},}
@ARTICLE{8258967,
author={O. Kwon and S. Choi and D. Shin},
journal={IEEE Access},
title={Improvement of JPEG XT Floating-Point HDR Image Coding Using Region Adaptive Prediction},
year={2018},
volume={6},
number={},
pages={3321-3335},
abstract={Displays capable of representing high-dynamic range (HDR) images were recently released in the digital consumer electronics market, and consumers have become increasingly interested in HDR images. However, the majority of today's digital imaging devices are still low dynamic range (LDR) using an 8-bit representation for each RGB color components; hence, a backward-compatible HDR image format with an existing LDR-based imaging environment is required. Accordingly, JPEG XT, a new HDR image coding standard providing JPEG backward compatibility was established by the JPEG committee based on the market needs. JPEG XT consists of a base and a residual layer. Both layers are independently encoded by a legacy JPEG encoder. Most importantly, the base layer contains a tone-mapped LDR version of the HDR image for compliance with a legacy JPEG decoder. To date, three profiles have been defined for JPEG XT to encode HDR images, which are represented using floating-point values. This paper proposes a new image-coding scheme that uses a region adaptive prediction method with modified current specifications on JPEG XT Part 7. The proposed method adaptively predicts the HDR pixel value using different prediction information for each block based on the ratio between the LDR and HDR blocks, which is invariant to the tone-mapping operator, while the existing JPEG XT profiles globally predict the HDR values using an inverse transformation of the LDR values. Experimental results using various sample images show the superiority of the proposed method in terms of objective and visual evaluations.},
keywords={consumer electronics;data compression;image coding;image colour analysis;image representation;legacy JPEG encoder;legacy JPEG decoder;image-coding scheme;region adaptive prediction method;JPEG XT Part 7;HDR pixel value;HDR blocks;HDR values;JPEG XT floating-point HDR image coding;high-dynamic range images;digital consumer electronics market;digital imaging devices;backward-compatible HDR image format;HDR image coding standard;JPEG backward compatibility;JPEG committee;JPEG XT profiles;Transform coding;Image coding;Decoding;Standards;Image reconstruction;Encoding;Imaging;JPEG XT;HDR from LDR predictor (HLP);high-dynamic range image;image coding},
doi={10.1109/ACCESS.2018.2793228},
ISSN={2169-3536},
month={},}
@ARTICLE{7219372,
author={Q. Chen and Z. Fan and D. Kaleshi and S. Armour},
journal={IEEE Access},
title={Rule Induction-Based Knowledge Discovery for Energy Efficiency},
year={2015},
volume={3},
number={},
pages={1423-1436},
abstract={Rule induction is a practical approach to knowledge discovery. Provided that a problem is developed, rule induction is able to return the knowledge that addresses the goal of this problem as if-then rules. The primary goals of knowledge discovery are for prediction and description. The rule format knowledge representation is easily understandable so as to enable users to make decisions. This paper presents the potential of rule induction for energy efficiency. In particular, three rule induction techniques are applied to derive knowledge from a dataset of thousands of Irish electricity customers' time-series power consumption records, socio-demographic details, and other information, in order to address the following four problems: 1) discovering mathematically interesting knowledge that could be found useful; 2) estimating power consumption features for customers, so that personalized tariffs can be assigned; 3) targeting a subgroup of customers with high potential for peak demand shifting; and 4) identifying customer attitudes that dominate energy conservation.},
keywords={data mining;energy conservation;power consumption;power engineering computing;time series;knowledge discovery;energy efficiency;if-then rules;rule format knowledge representation;rule induction techniques;Irish electricity customers;time-series power consumption records;socio-demographic details;peak demand shifting;customer attitudes;energy conservation;Energy efficiency;Knowledge discovery;Smart grids;energy efficiency;knowledge discovery;smart grids;subgroup discovery;Energy efficiency;knowledge discovery;smart grids;subgroup discovery},
doi={10.1109/ACCESS.2015.2472355},
ISSN={2169-3536},
month={},}
@ARTICLE{7921698,
author={G. Mujtaba and L. Shuib and R. G. Raj and N. Majeed and M. A. Al-Garadi},
journal={IEEE Access},
title={Email Classification Research Trends: Review and Open Issues},
year={2017},
volume={5},
number={},
pages={9044-9064},
abstract={Personal and business users prefer to use e-mail as one of the crucial sources of communication. The usage and importance of e-mails continuously grow despite the prevalence of alternative means, such as electronic messages, mobile applications, and social networks. As the volume of business-critical e-mails continues to grow, the need to automate the management of e-mails increases for several reasons, such as spam e-mail classification, phishing e-mail classification, and multi-folder categorization, among others. This paper comprehensively reviews articles on e-mail classification published in 2006-2016 by exploiting the methodological decision analysis in five aspects, namely, e-mail classification application areas, data sets used in each application area, feature space utilized in each application area, e-mail classification techniques, and the use of performance measures. A total of 98 articles (56 articles from Web of Science core collection databases and 42 articles from Scopus database) are selected. To achieve the objective of the study, a comprehensive review and analysis is conducted to explore the various areas where e-mail classification was applied. Moreover, various public data sets, features sets, classification techniques, and performance measures are examined and used in each identified application area. This review identifies five application areas of e-mail classification. The most widely used data sets, features sets, classification techniques, and performance measures are found in the identified application areas. The extensive use of these popular data sets, features sets, classification techniques, and performance measures is discussed and justified. The research directions, research challenges, and open issues in the field of e-mail classification are also presented for future researchers.},
keywords={data analysis;electronic mail;business users;personal users;electronic messages;mobile applications;social networks;business-critical e-mails;phishing e-mail classification;spam e-mail classification;performance measures;features sets;public data sets;e-mail classification application areas;multifolder categorization;Electronic mail;Feature extraction;Area measurement;Databases;Computer science;Information filters;Email classification;spam detection;phishing detection;multi-folder categorization;machine learning techniques},
doi={10.1109/ACCESS.2017.2702187},
ISSN={2169-3536},
month={},}
@ARTICLE{8255826,
author={C. Luo and L. Ma},
journal={IEEE Access},
title={Manifold Regularized Distribution Adaptation for Classification of Remote Sensing Images},
year={2018},
volume={6},
number={},
pages={4697-4708},
abstract={We perform unsupervised domain adaptation for the classification of remote sensing data by learning a shared subspace in this paper. Maximum mean discrepancy (MMD) is applied to each class, making the approach able to minimize the domain shift on a per-class basis. Furthermore, manifold regularization is employed to constrain the data manifold of both the source and target data to be preserved in the subspace. The manifold regularization in conjunction with the per-class MMD strategy is called manifold regularized distribution adaptation (MRDA) algorithm. Since the class mean of target data should be estimated by the predicted labels, we integrate spatial information and overall mean coincidence (OMC) method to improve the prediction accuracy, resulting in Spa_OMC_MRDA approach. Experimental results on both multispectral and hyperspectral remote sensing data indicated the good performances of the proposed approach.},
keywords={geophysical image processing;image classification;learning (artificial intelligence);remote sensing;unsupervised learning;multispectral remote sensing data;remote sensing image classification;manifold regularized distribution adaptation algorithm;hyperspectral remote sensing data;Spa_OMC_MRDA approach;mean coincidence method;per-class MMD strategy;data manifold;maximum mean discrepancy;shared subspace;unsupervised domain adaptation;Manifolds;Remote sensing;Support vector machines;Spatial filters;Classification algorithms;Eigenvalues and eigenfunctions;Classification;domain adaptation;manifold regularization;maximum mean discrepancy;remote sensing},
doi={10.1109/ACCESS.2018.2789932},
ISSN={2169-3536},
month={},}
@ARTICLE{8331069,
author={C. Fernández-Prades and J. Vilà-Valls and J. Arribas and A. Ramos},
journal={IEEE Access},
title={Continuous Reproducibility in GNSS Signal Processing},
year={2018},
volume={6},
number={},
pages={20451-20463},
abstract={This paper discusses the reproducibility of scientific experiments in which global navigation satellite system (GNSS) signals play a role. After analyzing the factors that impact the reproducibility of an experiment in the given context, this paper proposes a methodology that, leveraging on software containerization technologies and the best practices from professional software development, ensures the automated reproduction of scientific experiments involving GNSS data and software-defined GNSS receivers, including the generation of figures or tables of a research publication, while fostering scientific collaboration and contributing to mitigate the effects of software aging in mutating computer environments. In order to show a practical implementation of the proposed work flow, the authors propose a simple experiment based on a freely available open source software-defined receiver and the automation of its execution in a popular online platform.},
keywords={public domain software;satellite navigation;signal processing;software radio;telecommunication computing;GNSS signal processing;mutating computer environments;software-defined GNSS receivers;open source software;software aging;scientific collaboration;professional software development;software containerization technologies;global navigation satellite system;Global navigation satellite system;Receivers;Software;Signal processing;Standards;Topology;Tools;Digital signal processing;global navigation satellite system;open source software;reproducibility of results;software radio},
doi={10.1109/ACCESS.2018.2822835},
ISSN={2169-3536},
month={},}
@ARTICLE{7723818,
author={P. Rempel and P. Mäder},
journal={IEEE Transactions on Software Engineering},
title={Preventing Defects: The Impact of Requirements Traceability Completeness on Software Quality},
year={2017},
volume={43},
number={8},
pages={777-797},
abstract={Requirements traceability has long been recognized as an important quality of a well-engineered system. Among stakeholders, traceability is often unpopular due to the unclear benefits. In fact, little evidence exists regarding the expected traceability benefits. There is a need for empirical work that studies the effect of traceability. In this paper, we focus on the four main requirements implementation supporting activities that utilize traceability. For each activity, we propose generalized traceability completeness measures. In a defined process, we selected 24 medium to large-scale open-source projects. For each software project, we quantified the degree to which a studied development activity was enabled by existing traceability with the proposed measures. We analyzed that data in a multi-level Poisson regression analysis. We found that the degree of traceability completeness for three of the studied activities significantly affects software quality, which we quantified as defect rate. Our results provide for the first time empirical evidence that more complete traceability decreases the expected defect rate in the developed software. The strong impact of traceability completeness on the defect rate suggests that traceability is of great practical value for any kind of software development project, even if traceability is not mandated by a standard or regulation.},
keywords={public domain software;regression analysis;software quality;stochastic processes;software development project;multilevel Poisson regression analysis;software project;large-scale open-source projects;generalized traceability completeness measurement;requirement traceability completeness impact;software quality;Software quality;Software systems;Context;Software engineering;Stakeholders;Standards;Requirements traceability;traceability completeness;traceability metrics;change impact analysis;requirements satisfaction analysis;source code justification analysis;software quality;error proneness;defects;bugs;empirical validation;regression analysis},
doi={10.1109/TSE.2016.2622264},
ISSN={0098-5589},
month={Aug},}
@ARTICLE{7858692,
author={C. Dou and D. Yue and Q. Han and J. M. Guerrero},
journal={IEEE Access},
title={Multi-Agent System-Based Event-Triggered Hybrid Control Scheme for Energy Internet},
year={2017},
volume={5},
number={},
pages={3263-3272},
abstract={This paper is concerned with an event-triggered hybrid control for the energy Internet based on a multi-agent system approach with which renewable energy resources can be fully utilized to meet load demand with high security and well dynamical quality. In the design of control, a multi-agent system framework is first constructed. Then, to describe fully the hybrid behaviors of all distributed energy resources and logical relationships between them, a differential hybrid Petri-net model is established, which is an original work. The most important contributions based on this model propose four types of event-triggered hybrid control strategies whereby the multi-agent system implements the hierarchical hybrid control to achieve multiple control objectives. Finally, the effectiveness of the proposed control is validated by means of simulation results.},
keywords={continuous systems;discrete systems;Internet;multi-agent systems;Petri nets;power engineering computing;renewable energy sources;multiagent system;event-triggered hybrid control scheme;energy Internet;renewable energy resources;differential hybrid Petri-net model;hierarchical hybrid control;Switches;Internet;Multi-agent systems;Security;Voltage control;Senior members;Energy Internet;multi-agent system;hybrid control;event-triggered control;differential hybrid Petri-net},
doi={10.1109/ACCESS.2017.2670778},
ISSN={2169-3536},
month={},}
@ARTICLE{7279056,
author={H. Liu and X. Kong and X. Bai and W. Wang and T. M. Bekele and F. Xia},
journal={IEEE Access},
title={Context-Based Collaborative Filtering for Citation Recommendation},
year={2015},
volume={3},
number={},
pages={1695-1703},
abstract={Citation recommendation is an interesting and significant research area as it solves the information overload in academia by automatically suggesting relevant references for a research paper. Recently, with the rapid proliferation of information technology, research papers are rapidly published in various conferences and journals. This makes citation recommendation a highly important and challenging discipline. In this paper, we propose a novel citation recommendation method that uses only easily obtained citation relations as source data. The rationale underlying this method is that, if two citing papers are significantly co-occurring with the same citing paper(s), they should be similar to some extent. Based on the above rationale, an association mining technique is employed to obtain the paper representation of each citing paper from the citation context. Then, these paper representations are pairwise compared to compute similarities between the citing papers for collaborative filtering. We evaluate our proposed method through two relevant real-world data sets. Our experimental results demonstrate that the proposed method significantly outperforms the baseline method in terms of precision, recall, and F1, as well as mean average precision and mean reciprocal rank, which are metrics related to the rank information in the recommendation list.},
keywords={citation analysis;collaborative filtering;data mining;recommender systems;context-based collaborative filtering;information overload;information technology;citation recommendation method;association mining technique;Collaboration;Filtering;Citation recommendation;Context modeling;Text mining;Citation Recommendation;Collaborative Filtering;Citation Context;Citation Relation Matrix;Association Mining;Citation recommendation;collaborative filtering;citation context;citation relation matrix;association mining},
doi={10.1109/ACCESS.2015.2481320},
ISSN={2169-3536},
month={},}
@ARTICLE{8123925,
author={R. Alzubi and N. Ramzan and H. Alzoubi and A. Amira},
journal={IEEE Access},
title={A Hybrid Feature Selection Method for Complex Diseases SNPs},
year={2018},
volume={6},
number={},
pages={1292-1301},
abstract={Machine learning techniques have the potential to revolutionize medical diagnosis. Single Nucleotide Polymorphisms (SNPs) are one of the most important sources of human genome variability; thus, they have been implicated in several human diseases. To separate the affected samples from the normal ones, various techniques have been applied on SNPs. Achieving high classification accuracy in such a high-dimensional space is crucial for successful diagnosis and treatment. In this work, we propose an accurate hybrid feature selection method for detecting the most informative SNPs and selecting an optimal SNP subset. The proposed method is based on the fusion of a filter and a wrapper method, i.e., the Conditional Mutual Information Maximization (CMIM) method and the support vector machinerecursive feature elimination, respectively. The performance of the proposed method was evaluated against four state-of-the-art feature selection methods, minimum redundancy maximum relevancy, fast correlationbased feature selection, CMIM, and ReliefF, using four classifiers, support vector machine, naive Bayes, linear discriminant analysis, and k nearest neighbors on five different SNP data sets obtained from the National Center for Biotechnology Information gene expression omnibus genomics data repository. The experimental results demonstrate the efficiency of the adopted feature selection approach outperforming all of the compared feature selection algorithms and achieving up to 96% classification accuracy for the used data set. In general, from these results we conclude that SNPs of the whole genome can be efficiently employed to distinguish affected individuals with complex diseases from the healthy ones.},
keywords={bioinformatics;data analysis;diseases;feature extraction;feature selection;genetics;genomics;learning (artificial intelligence);medical computing;optimisation;patient diagnosis;patient treatment;pattern classification;polymorphism;support vector machines;complex diseases SNPs;machine learning techniques;medical diagnosis;human genome variability;human diseases;high classification accuracy;high-dimensional space;treatment;wrapper method;Conditional Mutual Information Maximization method;CMIM;single nucleotide polymorphisms;diagnosis;hybrid feature selection method;filter method;support vector machine recursive feature elimination;National Center for Biotechnology Information gene expression omnibus genomics data repository;Feature extraction;Diseases;Support vector machines;Genomics;Bioinformatics;Mutual information;Algorithm design and analysis;Single nucleotide polymorphism (SNP);feature selection;hybrid algorithms;complex diseases;machine learning},
doi={10.1109/ACCESS.2017.2778268},
ISSN={2169-3536},
month={},}
@ARTICLE{6963448,
author={F. Palomba and G. Bavota and M. D. Penta and R. Oliveto and D. Poshyvanyk and A. De Lucia},
journal={IEEE Transactions on Software Engineering},
title={Mining Version Histories for Detecting Code Smells},
year={2015},
volume={41},
number={5},
pages={462-489},
abstract={Code smells are symptoms of poor design and implementation choices that may hinder code comprehension, and possibly increase changeand fault-proneness. While most of the detection techniques just rely on structural information, many code smells are intrinsically characterized by how code elements change overtime. In this paper, we propose Historical Information for Smell deTection (HIST), an approach exploiting change history information to detect instances of five different code smells, namely Divergent Change, Shotgun Surgery, Parallel Inheritance, Blob, and Feature Envy. We evaluate HIST in two empirical studies. The first, conducted on 20 open source projects, aimed at assessing the accuracy of HIST in detecting instances of the code smells mentioned above. The results indicate that the precision of HIST ranges between 72 and 86 percent, and its recall ranges between 58 and 100 percent. Also, results of the first study indicate that HIST is able to identify code smells that cannot be identified by competitive approaches solely based on code analysis of a single system's snapshot. Then, we conducted a second study aimed at investigating to what extent the code smells detected by HIST (and by competitive code analysis techniques) reflect developers' perception of poor design and implementation choices. We involved 12 developers of four open source projects that recognized more than 75 percent of the code smell instances identified by HIST as actual design/implementation problems.},
keywords={data mining;program compilers;public domain software;code smell detection;historical information for smell detection;divergent change;shotgun surgery;parallel inheritance;blob;feature envy;HIST;code analysis;single system snapshot;open source project;mining version history;History;Feature extraction;Surgery;Accuracy;Association rules;Detectors;Code smells;mining software repositories;empirical studies;Code smells;mining software repositories;empirical studies},
doi={10.1109/TSE.2014.2372760},
ISSN={0098-5589},
month={May},}
@ARTICLE{8047939,
author={S. Conti and G. Faraci and R. Nicolosi and S. A. Rizzo and G. Schembra},
journal={IEEE Access},
title={Battery Management in a Green Fog-Computing Node: a Reinforcement-Learning Approach},
year={2017},
volume={5},
number={},
pages={21126-21138},
abstract={In the last years, Internet is evolving towards the cloud-computing paradigm complemented by fog-computing in order to distribute computing, storage, control, networking resources, and services close to end-user devices as much as possible, while sending heavy jobs to the remote cloud. When fog-computing nodes cannot be powered by the main electric grid, some environmental-friendly solutions, such as the use of solaror wind-based generators could be adopted. Their relatively unpredictable power output makes it necessary to include an energy storage system in order to provide power, when a peak of work occurs during periods of low-power generation. An optimized management of such an energy storage system in a green fog-computing node is necessary in order to improve the system performance, allowing the system to cope with high job arrival peaks even during low-power generation periods. In this perspective, this paper adopts reinforcement learning to choose a server activation policy that ensures the minimum job loss probability. A case study is presented to show how the proposed system works, and an extensive performance analysis of a fog-computing node highlights the importance of optimizing battery management according to the size of the Renewable-Energy Generator system and the number of available servers.},
keywords={battery management systems;distributed processing;learning (artificial intelligence);network servers;power engineering computing;probability;battery management;green fog-computing node;reinforcement-learning approach;cloud-computing paradigm;energy storage system;renewable-energy generator system;wind-based generators;Internet;networking resources;electric grid;solar based generator;low-power generation;server activation policy;minimum job loss probability;Servers;Generators;Cloud computing;Batteries;Renewable energy sources;Learning (artificial intelligence);Fog computing;renewable energy;battery management;reinforcement learning;Markov model},
doi={10.1109/ACCESS.2017.2755588},
ISSN={2169-3536},
month={},}
@ARTICLE{5674060,
author={J. Lawrance and C. Bogart and M. Burnett and R. Bellamy and K. Rector and S. D. Fleming},
journal={IEEE Transactions on Software Engineering},
title={How Programmers Debug, Revisited: An Information Foraging Theory Perspective},
year={2013},
volume={39},
number={2},
pages={197-215},
abstract={Many theories of human debugging rely on complex mental constructs that offer little practical advice to builders of software engineering tools. Although hypotheses are important in debugging, a theory of navigation adds more practical value to our understanding of how programmers debug. Therefore, in this paper, we reconsider how people go about debugging in large collections of source code using a modern programming environment. We present an information foraging theory of debugging that treats programmer navigation during debugging as being analogous to a predator following scent to find prey in the wild. The theory proposes that constructs of scent and topology provide enough information to describe and predict programmer navigation during debugging, without reference to mental states such as hypotheses. We investigate the scope of our theory through an empirical study of 10 professional programmers debugging a real-world open source program. We found that the programmers' verbalizations far more often concerned scent-following than hypotheses. To evaluate the predictiveness of our theory, we created an executable model that predicted programmer navigation behavior more accurately than comparable models that did not consider information scent. Finally, we discuss the implications of our results for enhancing software engineering tools.},
keywords={cognition;program debugging;public domain software;software maintenance;topology;information foraging theory;human debugging theories;complex mental constructs;navigation theory;programming environment;topology constructs;information scent constructs;open source code program debugging;programmer verbalizations;programmer navigation behavior prediction;software engineering tool enhancement;Debugging;Navigation;Topology;Programming environments;Predictive models;Approximation methods;Information foraging theory;debugging;software maintenance;programmer navigation;information scent;empirical software engineering},
doi={10.1109/TSE.2010.111},
ISSN={0098-5589},
month={Feb},}
@ARTICLE{8466564,
author={A. Garbo and S. Quer},
journal={IEEE Access},
title={A Fast MPEG’s CDVS Implementation for GPU Featured in Mobile Devices},
year={2018},
volume={6},
number={},
pages={52027-52046},
abstract={The Moving Picture Experts Group's Compact Descriptors for Visual Search (MPEG's CDVS) intends to standardize technologies in order to enable an interoperable, efficient, and cross-platform solution for internet-scale visual search applications and services. Among the key technologies within CDVS, we recall the format of visual descriptors, the descriptor extraction process, and the algorithms for indexing and matching. Unfortunately, these steps require precision and computation accuracy. Moreover, they are very time-consuming, as they need running times in the order of seconds when implemented on the central processing unit (CPU) of modern mobile devices. In this paper, to reduce computation times and maintain precision and accuracy, we re-design, for many-cores embedded graphical processor units (GPUs), all main local descriptor extraction pipeline phases of the MPEG's CDVS standard. To reach this goal, we introduce new techniques to adapt the standard algorithm to parallel processing. Furthermore, to reduce memory accesses and efficiently distribute the kernel workload, we use new approaches to store and retrieve CDVS information on proper GPU data structures. We present a complete experimental analysis on a large and standard test set. Our experiments show that our GPU-based approach is remarkably faster than the CPU-based reference implementation of the standard, and it maintains a comparable precision in terms of true and false positive rates.},
keywords={data structures;graphics processing units;image matching;image retrieval;Internet;mobile computing;multiprocessing systems;parallel algorithms;storage management;interoperable cross-platform solution;services;visual descriptors;descriptor extraction process;central processing unit;computation times;many-cores embedded graphical processor units;main local descriptor extraction pipeline phases;CDVS information;GPU-based approach;CPU-based reference implementation;comparable precision;Moving Picture Experts Group;indexing algorithm;matching algorithm;mobile devices;fast MPEG CDVS implementation;Compact Descriptors for Visual Search;Internet-scale visual search applications;MPEG CDVS standard;GPU data structures;parallel processing;memory access;Standards;Graphics processing units;Feature extraction;Parallel processing;Kernel;Visualization;Mobile handsets;Computer applications;concurrent computing;embedded software;image analysis;object detection},
doi={10.1109/ACCESS.2018.2870283},
ISSN={2169-3536},
month={},}
@ARTICLE{8425033,
author={Y. Shen and Y. Li and Y. Deng and J. Zhang and M. Yang and J. Chen and S. Si and K. Lei},
journal={IEEE Access},
title={Gastroenterology Ontology Construction Using Synonym Identification and Relation Extraction},
year={2018},
volume={6},
number={},
pages={52095-52104},
abstract={Ontology plays an increasingly important role in knowledge management and the semantic Web. However, ontology cannot perform well in realistic diagnosis reasoning unless it contains timely and accurate medical information and its individual items display all attributes of the categories they belong to. In this paper, we present a method that extracts synonyms along with concepts and their relationships for gastroenterology ontology construction. Specifically, we reuse the existing ontology as the basis for ontology completion. In addition, we conduct synonym identification through a combined application of global context features, local context features, and medical-specific features, and incorporate dependency information into deep neural networks for relation extraction. The extracted information is merged for ontology completion. Experimental results demonstrate that the proposed synonym identification and relation extraction method achieves the best performance compared with state-of-the-art methods and also builds a more complete ontology compared with existing gastroenterology disease ontologies. Our results are reproducible, and we will release the source code and ontology of this work after publication: https://github.com/shenyingpku/gastrointestinal_owl.},
keywords={diseases;feature extraction;information retrieval;knowledge management;medical diagnostic computing;medical information systems;neural nets;ontologies (artificial intelligence);semantic Web;text analysis;word processing;synonym identification;knowledge management;semantic Web;realistic diagnosis reasoning;medical information;global context features;local context features;medical-specific features;gastroenterology ontology construction;information extraction;relation extraction;gastroenterology disease ontologies;deep neural networks;Ontologies;Feature extraction;Data mining;Semantics;Medical diagnostic imaging;Gastroenterology;Neural networks;Artificial neural networks;data acquisition;knowledge representation;machine learning;text mining},
doi={10.1109/ACCESS.2018.2862885},
ISSN={2169-3536},
month={},}
@ARTICLE{6216383,
author={X. Wang and L. Zhang and T. Xie and H. Mei and J. Sun},
journal={IEEE Transactions on Software Engineering},
title={Locating Need-to-Externalize Constant Strings for Software Internationalization with Generalized String-Taint Analysis},
year={2013},
volume={39},
number={4},
pages={516-536},
abstract={Nowadays, a software product usually faces a global market. To meet the requirements of different local users, the software product must be internationalized. In an internationalized software product, user-visible hard-coded constant strings are externalized to resource files so that local versions can be generated by translating the resource files. In many cases, a software product is not internationalized at the beginning of the software development process. To internationalize an existing product, the developers must locate the user-visible constant strings that should be externalized. This locating process is tedious and error-prone due to 1) the large number of both user-visible and non-user-visible constant strings and 2) the complex data flows from constant strings to the Graphical User Interface (GUI). In this paper, we propose an automatic approach to locating need-to-externalize constant strings in the source code of a software product. Given a list of precollected API methods that output values of their string argument variables to the GUI and the source code of the software product under analysis, our approach traces from the invocation sites (within the source code) of these methods back to the need-to-externalize constant strings using generalized string-taint analysis. In our empirical evaluation, we used our approach to locate need-to-externalize constant strings in the uninternationalized versions of seven real-world open source software products. The results of our evaluation demonstrate that our approach is able to effectively locate need-to-externalize constant strings in uninternationalized software products. Furthermore, to help developers understand why a constant string requires translation and properly translate the need-to-externalize strings, we provide visual representation of the string dependencies related to the need-to-externalize strings.},
keywords={globalisation;graphical user interfaces;public domain software;software engineering;need-to-externalize constant string location;software internationalization;generalized string-taint analysis;user requirement;internationalized software product;user-visible hard-coded constant string;software development process;graphical user interface;GUI;data flow;API method;application program interface;string argument variable;open source software product;string dependency;Software;Graphical user interfaces;Prototypes;Java;Libraries;Production;Globalization;Software internationalization;need-to-externalize constant strings;string-taint analysis},
doi={10.1109/TSE.2012.40},
ISSN={0098-5589},
month={April},}
@ARTICLE{6878435,
author={B. Dagenais and M. P. Robillard},
journal={IEEE Transactions on Software Engineering},
title={Using Traceability Links to Recommend Adaptive Changes for Documentation Evolution},
year={2014},
volume={40},
number={11},
pages={1126-1146},
abstract={Developer documentation helps developers learn frameworks and libraries, yet developing and maintaining accurate documentation requires considerable effort and resources. Contributors who work on developer documentation often need to manually track all changes in the code, determine which changes are significant enough to document, and then, adapt the documentation. We propose AdDoc, a technique that automatically discovers documentation patterns, i.e., coherent sets of code elements that are documented together, and that reports violations of these patterns as the code and the documentation evolves. We evaluated our approach in a retrospective analysis of four Java open source projects and found that at least 50 percent of all the changes in the documentation were related to existing documentation patterns. Our technique allows contributors to quickly adapt existing documentation, so that they can focus their documentation effort on the new features.},
keywords={data mining;Java;program diagnostics;public domain software;system documentation;traceability links;adaptive changes;documentation evolution;developer documentation;AdDoc;automatic documentation pattern discovery;code elements;Java open source projects;Documentation;Java;Manuals;Libraries;Sections;Joining processes;Concrete;Documentation;maintainability;frameworks},
doi={10.1109/TSE.2014.2347969},
ISSN={0098-5589},
month={Nov},}
@ARTICLE{8336869,
author={Y. Fang and H. Tan and J. Zhang},
journal={IEEE Access},
title={Multi-Strategy Sentiment Analysis of Consumer Reviews Based on Semantic Fuzziness},
year={2018},
volume={6},
number={},
pages={20625-20631},
abstract={Since Internet has become an excellent source of consumer reviews, the area of sentiment analysis (also called sentiment extraction, opinion mining, opinion extraction, and sentiment mining) has seen a large increase in academic interest over the last few years. Sentiment analysis mines opinions at word, sentence, and document levels, and gives sentiment polarities and strengths of articles. As known, the opinions of consumers are expressed in sentiment Chinese phrases. But due to the fuzziness of Chinese characters, traditional machine learning techniques can not represent the opinion of articles very well. In this paper, we propose a multi-strategy sentiment analysis method with semantic fuzziness to solve the problem. The results show that this hybrid sentiment analysis method can achieve a good level of effectiveness.},
keywords={data mining;Internet;sentiment analysis;consumer reviews;semantic fuzziness;sentiment extraction;opinion mining;opinion extraction;sentiment mining;sentiment polarities;sentiment Chinese phrases;multistrategy sentiment analysis method;hybrid sentiment analysis method;Internet;Sentiment analysis;Support vector machines;Training;Semantics;Compounds;Mathematical model;Dictionaries;Sentiment analysis;machine learning;semantic fuzziness;consumer reviews},
doi={10.1109/ACCESS.2018.2820025},
ISSN={2169-3536},
month={},}
@ARTICLE{8395268,
author={J. Yang and X. He and L. Qing and S. Xiong and Y. Peng},
journal={IEEE Access},
title={A New Progressively Refined Wyner-Ziv Video Coding for Low-Power Human-Centered Telehealth},
year={2018},
volume={6},
number={},
pages={38315-38325},
abstract={With the increase of the global aging population, elderly care has become an important social issue around the world. Human-centered telehealth provides more efficient and comfortable health-care services for elderly people through collecting the elderly's information remotely. Video taken by wearable cameras is one of the most efficient carriers for human-centered telehealth. Whereas wearable cameras are mainly limited in energy supply and computation, the conventional video codecs such as H.26x requiring encoders with powerful processing ability are thus not suitable. Distributed video coding (DVC) based on the Wyner-Ziv (WZ) coding architecture, namely, WZ video coding, can exploit the source statistics only at decoders. It thus provides an efficient solution for low-power wearable cameras. Nevertheless, the compression performance gap between the DVC and the conventional video coding still exists. One of the main reasons for this weakness is the quality of the side information (SI). As the estimation of the current WZ frame, the SI provides the important inter-frame correlation for the correlation noise statistics. In this paper, a novel algorithm is proposed for the SI refinement first. The proposed refinement algorithm iteratively learns the difference between the already decoded information of the current WZ frame and the SI, and makes a targeted refinement for the SI quality. Subsequently, a progressively refined correlation noise model is proposed based on the novel SI refinement algorithm. The progressively refined WZ video coding is thus achieved. The performance evaluations show that the proposed technique advances over the existing DVC systems. The proposed technique provides an efficient way to improve the video compression performance for the low-power wearable cameras in human-centered telehealth.},
keywords={data compression;decoding;distributed processing;geriatrics;health care;low-power electronics;power aware computing;telemedicine;video codecs;video coding;SI refinement algorithm;video codecs;health-care services;Wyner-Ziv video coding;video compression;correlation noise model;distributed video coding;elderly care;global aging population;low-power human-centered telehealth;WZ video coding;correlation noise statistics;low-power wearable cameras;Decoding;Video coding;Cameras;Discrete wavelet transforms;Biomedical monitoring;Discrete cosine transforms;Correlation;Human-centered telehealth;low-power wearable cameras;side information;Wyner-Ziv video coding},
doi={10.1109/ACCESS.2018.2850052},
ISSN={2169-3536},
month={},}
@ARTICLE{8387831,
author={G. Luo and C. Yao and Y. Liu and Y. Tan and J. He and K. Wang},
journal={IEEE Access},
title={Stacked Auto-Encoder Based Fault Location in VSC-HVDC},
year={2018},
volume={6},
number={},
pages={33216-33224},
abstract={This paper presents an end-to-end approach for locating faults on high-voltage dc (HVDC) transmission lines. Different from traditional methods which rely on communications between different measuring units or feature extraction of post fault transients, the proposed algorithm takes the raw data of locally detected traveling current surges as the only-input and outputs the fault locations directly. Especially, the stacked auto-encoder (SAE) is utilized to model the relationship between fault currents and fault locations. The SAE-based method is performed in time domain and tested with a simulated HVDC transmission line modeled in PSCAD/EMTDC. The simulation results show that this method is effective in locating faulted points and robust against attenuation, overlapping of traveling surges, and various ground resistances.},
keywords={fault location;feature extraction;HVDC power convertors;HVDC power transmission;power transmission faults;voltage-source convertors;feature extraction;post fault transients;fault locations;fault currents;SAE-based method;VSC-HVDC;high-voltage dc transmission lines;PSCAD-EMTDC;time domain;traveling current surge detection;fault location;stacked auto-encoder;HVDC transmission line;VSC-HVDC;fault location;stacked auto-encoder;deep learning},
doi={10.1109/ACCESS.2018.2848841},
ISSN={2169-3536},
month={},}
@ARTICLE{5639020,
author={J. Al Dallal},
journal={IEEE Transactions on Software Engineering},
title={Measuring the Discriminative Power of Object-Oriented Class Cohesion Metrics},
year={2011},
volume={37},
number={6},
pages={788-804},
abstract={Several object-oriented cohesion metrics have been proposed in the literature. These metrics aim to measure the relationship between class members, namely, methods and attributes. Different metrics use different models to represent the connectivity pattern of cohesive interactions (CPCI) between class members. Most of these metrics are normalized to allow for easy comparison of the cohesion of different classes. However, in some cases, these metrics obtain the same cohesion values for different classes that have the same number of methods and attributes but different CPCIs. This leads to incorrectly considering the classes to be the same in terms of cohesion, even though their CPCIs clearly indicate that the degrees of cohesion are different. We refer to this as a lack of discrimination anomaly (LDA) problem. In this paper, we list and discuss cases in which the LDA problem exists, as expressed through the use of 16 cohesion metrics. In addition, we empirically study the frequent occurrence of the LDA problem when the considered metrics are applied to classes in five open source Java systems. Finally, we propose a metric and a simulation-based methodology to measure the discriminative power of cohesion metrics. The discrimination metric measures the probability that a cohesion metric will produce distinct cohesion values for classes with the same number of attributes and methods but different CPCIs. A highly discriminating cohesion metric is more desirable because it exhibits a lower chance of incorrectly considering classes to be cohesively equal when they have different CPCIs.},
keywords={Java;laser Doppler anemometry;object-oriented methods;public domain software;discriminative power measurement;object oriented class cohesion metrics;cohesive interactions;discrimination anomaly;open source Java systems;simulation based methodology;Power measurement;Object oriented modeling;Software measurement;Phase measurement;Cohesive interactions;connectivity pattern;discrimination metric;discriminative power;lack of discrimination anomaly;object-oriented class cohesion.},
doi={10.1109/TSE.2010.97},
ISSN={0098-5589},
month={Nov},}
@ARTICLE{7109113,
author={S. L. Hong and C. Liu},
journal={IEEE Access},
title={Sensor-Based Random Number Generator Seeding},
year={2015},
volume={3},
number={},
pages={562-568},
abstract={Random number generators (RNGs) are the foundation of strong security and privacy measures. With an increasing number of smart devices being connected to the Internet, the demand for secure communication will only increase. An important outgrowth of Internet-connected devices is the embedding of sensors. Yet, there remains a paucity of good protocols to provide sensor-based secure RNG seeds. In their raw form, sensor data are a weak source of RNG seeds for two reasons: 1) adversarial control - a malicious party gaining control of the sensor and generating a known data sequence and 2) collinearity across sensors - inherent correlated sequences generated because sensors are embedded in the same device. We propose a new seeding technique that leverages sensor data to provide secure seeds for RNG. Given the current proliferation of sensors and Internet-connectivity on smart devices, this technique could increase cybersecurity in a variety of domains, without additional cost.},
keywords={cryptography;data privacy;Internet;random number generation;sensors;sensor-based random number generator seeding;security measures;privacy measures;smart devices;secure communication;Internet-connected devices;sensors;sensor-based secure RNG seeds;cybersecurity;Random number generation;Intelligent sensors;Generators;Computer secuity;Privacy;Protocols;Internet;Industry applications;security;computer security;encryption;random number generation},
doi={10.1109/ACCESS.2015.2432140},
ISSN={2169-3536},
month={},}
@ARTICLE{8401501,
author={Q. Umer and H. Liu and Y. Sultan},
journal={IEEE Access},
title={Emotion Based Automated Priority Prediction for Bug Reports},
year={2018},
volume={6},
number={},
pages={35743-35752},
abstract={Issue tracking systems allow users to report bugs. Bug reports often contain product name, product component, description, and severity. Based on such information, triagers often manually prioritize the bug reports for investigation. However, manual prioritization is time consuming and cumbersome. DRONE is an automated state-of-the-art approach that recommends the priority level information of the bug reports. However, its performance for all levels of priorities is not uniform and may be improved. To this end, in this paper, we propose an emotion-based automatic approach to predict the priority for a report. First, we exploit natural language processing techniques to preprocess the bug report. Second, we identify the emotion-words that are involved in the description of the bug report and assign it an emotion value. Third, we create a feature vector for the bug report and predict its priority with a machine learning classifier that is trained with history data collected from the Internet. We evaluate the proposed approach on Eclipse open-source projects and the results of the cross-project evaluation suggest that the proposed approach outperforms the state-of-the-art. On average, it improves the F1 score by more than 6%.},
keywords={Internet;learning (artificial intelligence);natural language processing;pattern classification;program debugging;public domain software;bug reports;Emotion Based Automated Priority Prediction;issue tracking systems;product name;product component;DRONE;emotion-based automatic approach;natural language processing;emotion-words;emotion value;feature vector;machine learning classifier;history data;Internet;Eclipse open-source projects;cross-project evaluation;F1 score;Computer bugs;Software;Manuals;Natural language processing;History;Task analysis;Feature extraction;Bug reports;classification;machine learning;priority prediction;software maintenance},
doi={10.1109/ACCESS.2018.2850910},
ISSN={2169-3536},
month={},}
@ARTICLE{8402090,
author={T. Wu and J. Redouté and M. R. Yuce},
journal={IEEE Access},
title={A Wireless Implantable Sensor Design With Subcutaneous Energy Harvesting for Long-Term IoT Healthcare Applications},
year={2018},
volume={6},
number={},
pages={35801-35808},
abstract={In this paper, a wireless implantable sensor prototype with subcutaneous solar energy harvesting is proposed. To evaluate the performance of a flexible solar panel under skin, ex-vivo experiments are conducted under natural sunlight and artificial light sources. The results show that the solar panel covered by a 3 mm thick porcine flap can output tens of microWatts to a few milliWatts depending on the light conditions. The subcutaneous solar energy harvester is tested on different body parts, which suggests the optimal position for the harvester to implant is between neck and shoulder. A wireless implantable system powered by the subcutaneous energy harvester is presented, which consists of a power management circuit, a temperature sensor, and a Bluetooth low energy module. An application is developed for data visualization on mobile devices, which can be a gateway for future IoT-based healthcare applications. The entire device is embedded in a transparent silicone housing (38 mm × 32 mm × 4 mm), including a 7 mAh rechargeable battery for energy storage. The average power consumption of the implants is about 30 μW in a 10 min operation cycle. With the subcutaneous solar energy harvester, the self-powered operation of the implantable sensor prototype is demonstrated by long-term experimental results. Two worst-case scenarios (no exposure to light and battery depletion) are considered with ex-vivo experiment simulations.},
keywords={biomedical transducers;building integrated photovoltaics;computerised instrumentation;energy harvesting;health care;Internet;Internet of Things;medical computing;prosthetics;solar cell arrays;solar power;wireless sensor networks;wireless implantable sensor design;long-term IoT healthcare applications;subcutaneous solar energy harvesting;flexible solar panel;Bluetooth low energy module;natural sunlight;artificial light sources;porcine flap;power management circuit;temperature sensor;data visualization;mobile devices;transparent silicone housing;rechargeable battery;energy storage;power consumption;time 10.0 min;size 3.0 mm;Solar panels;Skin;Neck;Supercapacitors;Solar energy;Wrist;Energy harvesting;Implantable biomedical device;energy harvesting;wireless communication;self-powered system;long-term healthcare},
doi={10.1109/ACCESS.2018.2851940},
ISSN={2169-3536},
month={},}
@ARTICLE{6835187,
author={W. Kessentini and M. Kessentini and H. Sahraoui and S. Bechikh and A. Ouni},
journal={IEEE Transactions on Software Engineering},
title={A Cooperative Parallel Search-Based Software Engineering Approach for Code-Smells Detection},
year={2014},
volume={40},
number={9},
pages={841-861},
abstract={We propose in this paper to consider code-smells detection as a distributed optimization problem. The idea is that different methods are combined in parallel during the optimization process to find a consensus regarding the detection of code-smells. To this end, we used Parallel Evolutionary algorithms (P-EA) where many evolutionary algorithms with different adaptations (fitness functions, solution representations, and change operators) are executed, in a parallel cooperative manner, to solve a common goal which is the detection of code-smells. An empirical evaluation to compare the implementation of our cooperative P-EA approach with random search, two single population-based approaches and two code-smells detection techniques that are not based on meta-heuristics search. The statistical analysis of the obtained results provides evidence to support the claim that cooperative P-EA is more efficient and effective than state of the art detection approaches based on a benchmark of nine large open source systems where more than 85 percent of precision and recall scores are obtained on a variety of eight different types of code-smells.},
keywords={evolutionary computation;public domain software;search problems;software engineering;statistical analysis;cooperative parallel search-based software engineering approach;code-smells detection;distributed optimization problem;optimization process;parallel evolutionary algorithms;P-EA approach;random search;single population-based approaches;statistical analysis;open source systems;Measurement;Sociology;Statistics;Evolutionary computation;Detectors;Optimization;Computational modeling;Search-based software engineering;code-smells;software quality;distributed evolutionary algorithms},
doi={10.1109/TSE.2014.2331057},
ISSN={0098-5589},
month={Sept},}
@ARTICLE{7930388,
author={W. Wang and Y. Zhao and H. Chen and J. Zhang and H. Zheng and Y. Lin and Y. Lee},
journal={IEEE Access},
title={Re-Provisioning of Advance Reservation Applications in Elastic Optical Networks},
year={2017},
volume={5},
number={},
pages={10959-10967},
abstract={Driven by the development of cloud computing and mobile Internet, network-based applications have become diverse. Besides the usual immediate reservation (IR) applications, which need to be served immediately, advance reservation (AR) applications have been introduced to support initial-delay-tolerance services, such as grid computing, virtual machine backup, and so on. The provisioning mechanism of AR applications is different from that of IR applications, because AR requests don't require to be served immediately, and the network operator must decide the exact serving time in the AR service provisioning process. Based on the scheduling features of AR requests, this paper formulates a resource model and sets up a control framework for elastic optical networks to support AR services. We propose an optimization algorithm with three reprovisioning policies to dynamically reprovision the AR applications, which have already been scheduled. Both the proposed control framework and algorithm are developed on the basis of an open network operating system, which is an open-source software-defined networking controller. Also, the performance of the proposed algorithm is evaluated via software simulation. Demonstration results show the benefits of reprovisioning, and simulation results show that the reprovisioning optimization algorithm can improve the efficiency of network spectrum resource.},
keywords={optical fibre networks;optimisation;public domain software;software defined networking;telecommunication scheduling;network spectrum resource efficiency;reprovisioning optimization algorithm;software simulation;open-source software-defined networking controller;open network operating system;reprovisioning policies;control framework;resource model;scheduling features;mobile Internet;cloud computing;elastic optical networks;advance reservation applications;Optical fiber networks;Optimization;Frequency selective surfaces;Heuristic algorithms;Optical fiber dispersion;Software algorithms;Algorithm design and analysis;Optical fiber communication;advanced reservation;software-defined networking},
doi={10.1109/ACCESS.2017.2705105},
ISSN={2169-3536},
month={},}
@ARTICLE{6589568,
author={K. T. Stolee and S. Elbaum},
journal={IEEE Transactions on Software Engineering},
title={Identification, Impact, and Refactoring of Smells in Pipe-Like Web Mashups},
year={2013},
volume={39},
number={12},
pages={1654-1679},
abstract={With the emergence of tools to support visual mashup creation, tens of thousands of users have started to access, manipulate, and compose data from web sources. We have observed, however, that mashups created by these users tend to suffer from deficiencies that propagate as mashups are reused, which happens frequently. To address these deficiencies, we would like to bring some of the benefits of software engineering techniques to the end users creating these programs. In this work, we focus on identifying code smells indicative of the deficiencies we observed in web mashups programmed in the popular Yahoo! Pipes environment. Through an empirical study, we explore the impact of those smells on the preferences of 61 users, and observe that a significant majority of users prefer mashups without smells. We then introduce refactorings targeting those smells. These refactorings reduce the complexity of the mashup programs, increase their abstraction, update broken data sources and dated components, and standardize their structures to fit the community development patterns. Our assessment of a sample of over 8,000 mashups shows that smells are present in 81 percent of them and that the proposed refactorings can reduce the number of smelly mashups to 16 percent, illustrating the potential of refactoring to support the thousands of end-users programming mashups. Further, we explore how the smells and refactorings can apply to other end-user programming domains to show the generalizability of our approach.},
keywords={Internet;software maintenance;smell identification;smell impact;smell refactoring;pipe-like Web mashups;visual mashup creation;Web sources;software engineering techniques;Yahoo! Pipes environment;end-users programming mashups;Mashups;Visualization;Factoring;Generators;Programming;End-user software engineering;end-user programming;web mashups;refactoring;code smells;empirical studies},
doi={10.1109/TSE.2013.42},
ISSN={0098-5589},
month={Dec},}
@ARTICLE{7865891,
author={S. Ge and Q. Yang and R. Wang and P. Lin and J. Gao and Y. Leng and Y. Yang and H. Wang},
journal={IEEE Access},
title={A Brain-Computer Interface Based on a Few-Channel EEG-fNIRS Bimodal System},
year={2017},
volume={5},
number={},
pages={208-218},
abstract={With the development of the wearable brain-computer interface (BCI), a few-channel BCI system is necessary for its application to daily life. In this paper, we proposed a bimodal BCI system that uses only a few channels of electroencephalograph (EEG) and functional near-infrared spectroscopy (fNIRS) signals to obtain relatively high accuracy. We developed new approaches for signal acquisition and signal processing to improve the performance of this few-channel BCI system. At the signal acquisition stage, source analysis was applied for both EEG and fNIRS signals to select the optimal channels for bimodal signal collection. At the feature extraction stage, phase-space reconstruction was applied to the selected three-channel EEG signals to expand them into multichannel signals, thus allowing the use of the traditional effective common spatial pattern to extract EEG features. For the fNIRS signal, the Hurst exponents for the selected ten channels were calculated and composed of the fNIRS data feature. At the classification stage, EEG and fNIRS features were joined and classified with the support vector machine. The averaged classification accuracy of 12 participants was 81.2% for the bimodal EEG-fNIRS signals, which was significantly higher than that for either single modality.},
keywords={brain-computer interfaces;electroencephalography;feature extraction;infrared spectroscopy;medical signal processing;few channel EEG-fNIRS bimodal system;wearable brain computer interface;few-channel BCI system;bimodal BCI system;electroencephalograph;functional near infrared spectroscopy;fNIRS signals;signal acquisition;signal processing;optimal channels;bimodal signal collection;feature extraction stage;phase-space reconstruction;spatial pattern;extract EEG features;Electroencephalography;Detectors;Spatial resolution;Electrooculography;Nickel;Scalp;BCI;EEG;fNIRS;phase-space reconstruction;common spatial pattern;data fusion;support vector machine},
doi={10.1109/ACCESS.2016.2637409},
ISSN={2169-3536},
month={},}
@ARTICLE{7434569,
author={E. Kougianos and S. P. Mohanty and G. Coelho and U. Albalawi and P. Sundaravadivel},
journal={IEEE Access},
title={Design of a High-Performance System for Secure Image Communication in the Internet of Things},
year={2016},
volume={4},
number={},
pages={1222-1242},
abstract={Image or video exchange over the Internet of Things (IoT) is a requirement in diverse applications, including smart health care, smart structures, and smart transportations. This paper presents a modular and extensible quadrotor architecture and its specific prototyping for automatic tracking applications. The architecture is extensible and based on off-the-shelf components for easy system prototyping. A target tracking and acquisition application is presented in detail to demonstrate the power and flexibility of the proposed design. Complete design details of the platform are also presented. The designed module implements the basic proportional-integral-derivative control and a custom target acquisition algorithm. Details of the sliding-window-based algorithm are also presented. This algorithm performs $20\times $ faster than comparable approaches in OpenCV with equal accuracy. Additional modules can be integrated for more complex applications, such as search-and-rescue, automatic object tracking, and traffic congestion analysis. A hardware architecture for the newly introduced Better Portable Graphics (BPG) compression algorithm is also introduced in the framework of the extensible quadrotor architecture. Since its introduction in 1987, the Joint Photographic Experts Group (JPEG) graphics format has been the de facto choice for image compression. However, the new compression technique BPG outperforms the JPEG in terms of compression quality and size of the compressed file. The objective is to present a hardware architecture for enhanced real-time compression of the image. Finally, a prototyping platform of a hardware architecture for a secure digital camera (SDC) integrated with the secure BPG (SBPG) compression algorithm is presented. The proposed architecture is suitable for high-performance imaging in the IoT and is prototyped in Simulink. To the best of our knowledge, this is the first ever proposed hardware architecture for SBPG compression integrated with an SDC.},
keywords={aerospace computing;control engineering computing;data compression;helicopters;image coding;image sensors;Internet of Things;object tracking;robot vision;telecommunication security;three-term control;variable structure systems;high-performance system design;secure image communication;Internet-of-things;image exchange;video exchange;smart health care;smart structures;smart transportations;IoT;modular quadrotor architecture;automatic tracking applications;off-the-shelf components;system prototyping;target tracking application;proportional-integral-derivative control;custom target acquisition algorithm;sliding-window-based algorithm;search-and-rescue;automatic object tracking;traffic congestion analysis;better portable graphics compression algorithm;hardware architecture;secure digital camera;SDC;secure BPG compression algorithm;SBPG compression algorithm;Simulink;Internet of Things;Object detection;Image compression;Video communication;Image coding;Algorithm design and analysis;Target tracking;Compression algorithms;Transform coding;Smart devices;Internet of Things (IoT);Object Detection;Robot Vision Systems;Secure Digital Camera (SDC);Better Portable Graphics (BPG);Image Compression;VLSI Architecture;Internet of things (IoT);object detection;robot vision systems;secure digital camera (SDC);better portable graphics (BPG);image compression;VLSI architecture},
doi={10.1109/ACCESS.2016.2542800},
ISSN={2169-3536},
month={},}
@ARTICLE{7956152,
author={S. C. Loke and K. A. Kasmiran and S. A. Haron},
journal={IEEE Access},
title={A Software Application for Survey Form Design and Processing for Scientific Use},
year={2017},
volume={5},
number={},
pages={11867-11876},
abstract={A form processing application (FPA) automates digitization of information contained in forms. Smaller research groups do not use FPAs as they cannot justify operation of an in-house commercial system. This paper describes the design and testing of a new FPA that is targeted toward the needs of this group, and is released as free open-source software. The new FPA covers form design, printing, scanning, and digitization. It has a flexible plug-in architecture and double-keying is used to reduce transcription error. A common content module (CCM) implements the form design based on the format-independent hierarchical content. The scan module has basic handwriting recognition and can process the input fields used by the CCM. The FPA was field-tested using data from a clinical study to compare the error rate with manual processing. A similar comparison was also made between interviewer and self-administered survey forms. The first comparison shows that the FPA with double-keying had no errors while manual transcription had three errors (0.06%) out of 4952 input fields (p=0.083). The second comparison shows that the FPA with double-keying had no errors for the interviewer-administered form (0/3681 fields, 0%), while the self-administered form had three errors (3/6096 fields, 0.05%) (p=0.178). When double-keying was not used, the error rate for tablet-type fields was not significantly different (p=0.120) between the interviewer (2/3400 fields, 0.06%) and self-administered forms (19/6000 fields, 0.32%). There was, however, a highly significant difference (p&lt;;0.001) for handwriting-type fields between the interviewer (11/881 fields, 1.25%) and self-administered forms (75/1896 fields, 3.96%).},
keywords={document handling;handwriting recognition;optical character recognition;public domain software;survey form design;survey from processing;scientific use;form processing application;FPA;free open-source software;flexible plug-in architecture;double keying;transcription error reduction;common content module;CCM;handwriting recognition;document handling;Computer architecture;Optical character recognition software;Character recognition;Gerontology;Training;Manuals;Document handling;handwriting recognition;image matching;image processing;image registration;object detection;optical character recognition software;pattern recognition;printers;software algorithms},
doi={10.1109/ACCESS.2017.2719284},
ISSN={2169-3536},
month={},}
@ARTICLE{7530877,
author={X. Jing and F. Wu and X. Dong and B. Xu},
journal={IEEE Transactions on Software Engineering},
title={An Improved SDA Based Defect Prediction Framework for Both Within-Project and Cross-Project Class-Imbalance Problems},
year={2017},
volume={43},
number={4},
pages={321-339},
abstract={Background. Solving the class-imbalance problem of within-project software defect prediction (SDP) is an important research topic. Although some class-imbalance learning methods have been presented, there exists room for improvement. For cross-project SDP, we found that the class-imbalanced source usually leads to misclassification of defective instances. However, only one work has paid attention to this cross-project class-imbalance problem. Objective. We aim to provide effective solutions for both within-project and cross-project class-imbalance problems. Method. Subclass discriminant analysis (SDA), an effective feature learning method, is introduced to solve the problems. It can learn features with more powerful classification ability from original metrics. For within-project prediction, we improve SDA for achieving balanced subclasses and propose the improved SDA (ISDA) approach. For cross-project prediction, we employ the semi-supervised transfer component analysis (SSTCA) method to make the distributions of source and target data consistent, and propose the SSTCA+ISDA prediction approach. Results. Extensive experiments on four widely used datasets indicate that: 1) ISDA-based solution performs better than other state-of-the-art methods for within-project class-imbalance problem; 2) SSTCA+ISDA proposed for cross-project class-imbalance problem significantly outperforms related methods. Conclusion. Within-project and cross-project class-imbalance problems greatly affect prediction performance, and we provide a unified and effective prediction framework for both problems.},
keywords={learning (artificial intelligence);software engineering;SDA based defect prediction;within-project class-imbalance problem;cross-project class-imbalance problem;software defect prediction;SDP;class-imbalance learning;subclass discriminant analysis;semisupervised transfer component analysis;SSTCA;Support vector machines;Learning systems;Predictive models;Software;Software engineering;Measurement;Software defect prediction (SDP);within-project class-imbalance;cross-project class-imbalance;improved subclass discriminant analysis (ISDA);ISDA based defect prediction framework},
doi={10.1109/TSE.2016.2597849},
ISSN={0098-5589},
month={April},}
@ARTICLE{8372248,
author={X. Zhong and S. Guo and H. Shan and L. Gao and D. Xue and N. Zhao},
journal={IEEE Access},
title={Feature-Based Transfer Learning Based on Distribution Similarity},
year={2018},
volume={6},
number={},
pages={35551-35557},
abstract={Transfer learning has been found helpful at enhancing the target domain's learning process by transferring useful knowledge from other different but related source domains. In many applications, however, collecting and labeling target information is not only very difficult but also expensive. At the same time, considerable prior experience in this regard exists in other application domains. This paper proposes a feature-based transfer learning method based on distribution similarity that aims at the partial overlap of features between two domains. The non-overlapping features are completed by leveraging the distribution similarity of other features within the source domain. Features of the two domains are then reweighted in accordance with the distribution similarity between the source and target domains. This, in turn, decreases the distribution discrepancy between the two domains, therefore achieving the desired feature transfer. Results of the experiments performed on Facebook and Sina Microblog data sets demonstrate that the proposed method is capable of effectively enhancing the accuracy of the prediction function.},
keywords={distribution strategy;feature extraction;learning (artificial intelligence);social networking (online);distribution similarity;application domains;nonoverlapping features;prediction function;Sina microblog data sets;Facebook;labeling target information;target domains learning process;feature based transfer learning method;Probability distribution;Labeling;Learning systems;Machine learning;Programmable logic arrays;Facebook;Training;Distribution similarity;feature transfer;KL divergence;transfer learning},
doi={10.1109/ACCESS.2018.2843773},
ISSN={2169-3536},
month={},}
@ARTICLE{7878604,
author={H. Ahmad and A. Daud and L. Wang and H. Hong and H. Dawood and Y. Yang},
journal={IEEE Access},
title={Prediction of Rising Stars in the Game of Cricket},
year={2017},
volume={5},
number={},
pages={4104-4124},
abstract={Online social databases are rich sources to retrieve appropriate information that is subsequently analyzed for forthcoming trends prediction. In this paper, we identify rising stars in cricket domain by employing machine learning techniques. More precisely, we predict rising stars from batting as well as from bowling realms. For this intent, the concepts of co-players, team, and opposite teams are incorporated and distinct features along with their mathematical formulations are presented. For classification purpose, generative and discriminative machine learning algorithms are employed, and two models from each category are evaluated. As a proof of applicability, the proposed approach is validated experimentally while analyzing the impact of individual features. Besides, model and categorywise assessment is also performed. Employing cross validation, we demonstrate high accuracy for rising star prediction that is both robust and statistically significant. Finally, ranking lists of top ten rising cricketers based on weighted average, performance evolution, and rising star scores are compared with the international cricket council rankings.},
keywords={learning (artificial intelligence);pattern classification;sport;cricket game;classification;discriminative machine learning algorithms;generative machine learning algorithms;categorywise assessment;rising star prediction;international cricket council rankings;Games;Proposals;Mathematical model;Measurement;Databases;Machine learning algorithms;Social network services;Cricket;machine learning;online social databases;prediction;rising stars},
doi={10.1109/ACCESS.2017.2682162},
ISSN={2169-3536},
month={},}
@ARTICLE{8356572,
author={W. Deng and S. Zhang and H. Zhao and X. Yang},
journal={IEEE Access},
title={A Novel Fault Diagnosis Method Based on Integrating Empirical Wavelet Transform and Fuzzy Entropy for Motor Bearing},
year={2018},
volume={6},
number={},
pages={35042-35056},
abstract={Motor bearing is subjected to the joint effects of much more loads, transmissions, and shocks that cause bearing fault and machinery breakdown. A vibration signal analysis method is the most popular technique that is used to monitor and diagnose the fault of motor bearing. However, the application of the vibration signal analysis method for motor bearing is very limited in engineering practice. In this paper, on the basis of comparing fault feature extraction by using empirical wavelet transform (EWT) and Hilbert transform with the theoretical calculation, a new motor bearing fault diagnosis method based on integrating EWT, fuzzy entropy, and support vector machine (SVM) called EWTFSFD is proposed. In the proposed method, a novel signal processing method called EWT is used to decompose vibration signal into multiple components in order to extract a series of amplitude modulated-frequency modulated (AM-FM) components with supporting Fourier spectrum under an orthogonal basis. Then, fuzzy entropy is utilized to measure the complexity of vibration signal, reflect the complexity changes of intrinsic oscillation, and compute the fuzzy entropy values of AM-FM components, which are regarded as the inputs of the SVM model to train and construct an SVM classifier for fulfilling fault pattern recognition. Finally, the effectiveness of the proposed method is validated by using the simulated signal and real motor bearing vibration signals. The experiment results show that the EWT outperforms empirical mode decomposition for decomposing the signal into multiple components, and the proposed EWTFSFD method can accurately and effectively achieve the fault diagnosis of motor bearing.},
keywords={amplitude modulation;entropy;fault diagnosis;feature extraction;frequency modulation;Hilbert transforms;machine bearings;machinery;mechanical engineering computing;pattern recognition;support vector machines;vibrational signal processing;vibrations;wavelet transforms;novel fault diagnosis method;vibration signal analysis method;fault feature extraction;motor bearing fault diagnosis method;signal processing method;fuzzy entropy values;fault pattern recognition;motor bearing vibration signals;empirical wavelet transform;EWT;Hilbert transform;support vector machine;EWTFSFD;SVM classifier;vibration signal decomposition;amplitude modulated-frequency modulated components extraction;AM-FM components extraction;Fourier spectrum;intrinsic oscillation;Fault diagnosis;Wavelet transforms;Entropy;Vibrations;Feature extraction;Support vector machines;Motor bearing;fault diagnosis;empirical wavelet transform;fuzzy entropy;support vector machine;Fourier spectrum segmentation;AM-FM components},
doi={10.1109/ACCESS.2018.2834540},
ISSN={2169-3536},
month={},}
@ARTICLE{7725957,
author={M. Khan and B. N. Silva and K. Han},
journal={IEEE Access},
title={Internet of Things Based Energy Aware Smart Home Control System},
year={2016},
volume={4},
number={},
pages={7556-7566},
abstract={The concept of smart home is widely favored, as it enhances the lifestyle of the residents involving multiple disciplines, i.e., lighting, security, and much more. As the smart home networks continue to grow in size and complexity, it is essential to address a handful among the myriads of challenges related to data loss due to the interference and efficient energy management. In this paper, we propose a smart home control system using a coordinator-based ZigBee networking. The working of the proposed system is three fold: smart interference control system controls the interference caused due to the co-existence of IEEE 802.11x-based wireless local area networks and wireless sensor networks; smart energy control system is developed to integrate sunlight with light source and optimizes the energy consumption of the household appliances by controlling the unnecessary energy demands; and smart management control system to efficiently control the operating time of the electronic appliances. The performance of the proposed smart home is testified through computer simulation. Simulation results show that the proposed smart home system is less affected by the interference and efficient in reducing the energy consumption of the appliances used in a smart home.},
keywords={computer network management;control engineering computing;home automation;home networks;intelligent control;Internet;Internet of Things;power control;radiofrequency interference;telecommunication control;wireless LAN;wireless sensor networks;Zigbee;Internet of Things;energy aware smart home control system;lighting;security;efficient energy management;coordinator-based ZigBee networking;smart interference control system;IEEE 802.11x-based wireless local area networks;wireless sensor network;sunlight;light source;energy consumption;household appliance;smart management control system;electronic appliance;Smart homes;Home automation;Energy management;Internet of things;ZigBee;Interference;Wireless LAN;IEEE 802.11x Standard;ZigBee;interference;IEEE 80211x;WLAN;smart home},
doi={10.1109/ACCESS.2016.2621752},
ISSN={2169-3536},
month={},}
@ARTICLE{8026581,
author={M. Lopez-Martin and B. Carro and A. Sanchez-Esguevillas and J. Lloret},
journal={IEEE Access},
title={Network Traffic Classifier With Convolutional and Recurrent Neural Networks for Internet of Things},
year={2017},
volume={5},
number={},
pages={18042-18050},
abstract={A network traffic classifier (NTC) is an important part of current network monitoring systems, being its task to infer the network service that is currently used by a communication flow (e.g., HTTP and SIP). The detection is based on a number of features associated with the communication flow, for example, source and destination ports and bytes transmitted per packet. NTC is important, because much information about a current network flow can be learned and anticipated just by knowing its network service (required latency, traffic volume, and possible duration). This is of particular interest for the management and monitoring of Internet of Things (IoT) networks, where NTC will help to segregate traffic and behavior of heterogeneous devices and services. In this paper, we present a new technique for NTC based on a combination of deep learning models that can be used for IoT traffic. We show that a recurrent neural network (RNN) combined with a convolutional neural network (CNN) provides best detection results. The natural domain for a CNN, which is image processing, has been extended to NTC in an easy and natural way. We show that the proposed method provides better detection results than alternative algorithms without requiring any feature engineering, which is usual when applying other models. A complete study is presented on several architectures that integrate a CNN and an RNN, including the impact of the features chosen and the length of the network flows used for training.},
keywords={Internet of Things;learning (artificial intelligence);recurrent neural nets;telecommunication computing;telecommunication traffic;network traffic classifier;convolutional networks;recurrent neural networks;NTC;current network monitoring systems;network service;communication flow;current network flow;traffic volume;heterogeneous devices;IoT traffic;convolutional neural network;Internet of Things networks;CNN;deep learning models;Ports (Computers);Telecommunication traffic;Feature extraction;Recurrent neural networks;Machine learning;Payloads;Biological neural networks;Convolutional neural network;deep learning;network traffic classification;recurrent neural network},
doi={10.1109/ACCESS.2017.2747560},
ISSN={2169-3536},
month={},}
@ARTICLE{8369419,
author={P. Bellavista and A. Corradi and L. Foschini and S. Monti},
journal={IEEE Access},
title={Improved Adaptation and Survivability via Dynamic Service Composition of Ubiquitous Computing Middleware},
year={2018},
volume={6},
number={},
pages={33604-33620},
abstract={These days, ubiquitous computing has radically changed the way users access and interact with services and content on the Internet: novel smart mobile devices and broadband wireless communication channels allow users to seamlessly access them anytime and anywhere. Middleware infrastructures to support ubiquitous computing need to support an extremely dynamic and ever-changing scenario, where novel contents/services, devices, formats, and media channels become available. Service-oriented architectures and service composition techniques have proven to be the key in designing flexible and extensible platforms that are able to reliably support ubiquitous computing. However, current trends in service composition for ubiquitous computing tend to be either too formal and, therefore, poorly used by average final users, or too vertical and poorly flexible and extensible. This paper proposes novel service composition middleware for ubiquitous computing that relies on a translucent composition model to achieve a flexible, extensible, highly-available, but also easily understandable and usable platform. The proposed system has been widely tested, benchmarked, and deployed on a number of different and heterogeneous ubiquitous scenarios.},
keywords={human computer interaction;Internet;middleware;mobile computing;service-oriented architecture;software reliability;dynamic service composition;ubiquitous computing middleware;smart mobile devices;broadband wireless communication channels;service-oriented architectures;service composition techniques;service composition middleware;translucent composition model;heterogeneous ubiquitous scenarios;Internet;Ubiquitous computing;Proposals;Semantics;Tools;Mashups;Wireless communication;Middleware;ubiquitous computing;context-aware services;service-oriented computing},
doi={10.1109/ACCESS.2018.2842683},
ISSN={2169-3536},
month={},}
@ARTICLE{8388198,
author={R. Li and W. Lu and H. Liang and Y. Mao and X. Wang},
journal={IEEE Access},
title={Multiple Features With Extreme Learning Machines For Clothing Image Recognition},
year={2018},
volume={6},
number={},
pages={36283-36294},
abstract={Clothing image recognition has recently received considerable attention from many communities, such as multimedia information processing and computer vision, due to its commercial and social applications. However, the large variations in clothing images' appearances and styles and their complicated formation conditions make the problem challenging. In addition, a generic treatment with convolutional neural networks (CNNs) cannot provide a satisfactory solution considering the training time and recognition performance. Therefore, how to balance those two factors for clothing image recognition is an interesting problem. Motivated by the fast training and straightforward solutions exhibited by extreme learning machines (ELMs), in this paper, we propose a recognition framework that is based on multiple sources of features and ELM neural networks. In this framework, three types of features are first extracted, including CNN features with pre-trained networks, histograms of oriented gradients and color histograms. Second, those low-level features are concatenated and taken as the inputs to an autoencoder version of the ELM for deep feature-level fusion. Third, we propose an ensemble of adaptive ELMs for decision-level fusion using the previously obtained feature-level fusion representations. Extensive experiments are conducted on an up-to-date large-scale clothing image data set. Those experimental results show that the proposed framework is competitive and efficient.},
keywords={clothing;computer vision;convolution;feature extraction;feedforward neural nets;image colour analysis;image recognition;learning (artificial intelligence);multimedia information processing;computer vision;convolutional neural networks;clothing image recognition;extreme learning machines;recognition framework;ELM neural networks;CNN features;pre-trained networks;deep feature-level fusion;feature-level fusion representations;multiple features;large-scale clothing image data;color histograms;Clothing;Image recognition;Task analysis;Feature extraction;Neural networks;Machine learning;Histograms;Clothing image recognition;extreme learning machines;feature fusion;autoencoder ELM;ensemble learning},
doi={10.1109/ACCESS.2018.2848966},
ISSN={2169-3536},
month={},}
@ARTICLE{7755736,
author={P. Shen and X. Du and C. Li},
journal={IEEE Access},
title={Distributed Semi-Supervised Metric Learning},
year={2016},
volume={4},
number={},
pages={8558-8571},
abstract={Over the last decade, many pairwise-constraint-based metric learning algorithms have been developed to automatically learn application-specific metrics from data under similarity/dissimilarity data-pair constraints (weak labels). Nevertheless, these existing methods are designed for the centralized learning case, in which all the data and constraints are supposed to be gathered together in one source, and the algorithms utilize the whole data and constraints information during the learning process. However, in many real applications, large amounts of data (constraints) are dispersedly generated/stored in geographically distributed nodes over networks. Thus, it might be impractical to centralize the whole data information to one fusion node. Besides, in such cases, it is often hard to have every data pair labeled due to the huge data-pair amounts, resulting in numerous unlabeled data pairs. Given these situations, in this paper, we propose two types, namely, a diffusion type and an alternating-direction-method-of-multipliers type, of distributed semi-supervised metric learning frameworks, which make use of both labeled and unlabeled data pairs. The proposed frameworks can be easily used to extend centralized metric learning methods of different objective functions to distributed cases. In particular, we apply our frameworks on a well-behaved centralized semi-supervised metric learning method called SERAPH and yield two new distributed semi-supervised metric learning algorithms. Our simulation results show that the metrics learned by the proposed distributed algorithms are very close to that of the corresponding centralized method in most cases.},
keywords={distributed algorithms;learning (artificial intelligence);SERAPH;centralized semisupervised metric learning method;centralized metric learning methods;alternating-direction-method-of-multipliers type;diffusion type;application-specific metrics;pairwise-constraint-based metric learning algorithms;distributed semisupervised metric learning algorithms;Semisupervised learning;Distributed databases;Linear programming;Entropy;Algorithm design and analysis;Learning systems;Distributed algorithms;Supervised learning;Distributed learning;information theoretic learning;semi-supervised learning;distance metric learning},
doi={10.1109/ACCESS.2016.2632158},
ISSN={2169-3536},
month={},}
@ARTICLE{8468017,
author={F. Dong and C. Shen and K. Zhang and H. Wang},
journal={IEEE Access},
title={Real-Valued Sparse DOA Estimation for MIMO Array System Under Unknown Nonuniform Noise},
year={2018},
volume={6},
number={},
pages={52218-52226},
abstract={In this paper, the problem of the direction of arrival (DOA) estimation for the multiple input multiple output (MIMO) array system is considered as a real-valued sparse signal recover procedure under the condition of unknown nonuniform noise. Then, a real-valued covariance vector-based sparse Bayesian learning framework is proposed, in which the reduced dimensional (RD) transformation is utilized to remove the redundant elements of MIMO array system, and a linear transformation is applied to eliminate the influence of unknown non-uniform noise. Then by supposing that the source powers follow an independent prior Gaussian distribution with zero-mean, a real-valued covariance vector-based sparse Bayesian model is formulated. And considering its unknown variance as hyper-parameters, they can be estimated by adopting the expectation-maximization algorithm. Finally, the DOA can be achieved according to the spatial spectrum of hyper-parameters. Simulation results have demonstrated that our proposed method not only achieves more superior performance but also provides robustness against nonuniform noise, compared with other recently reported sparse signal representation based methods.},
keywords={array signal processing;Bayes methods;covariance matrices;direction-of-arrival estimation;expectation-maximisation algorithm;Gaussian distribution;learning (artificial intelligence);signal representation;vectors;unknown nonuniform noise;multiple input multiple output array system;real-valued sparse signal;real-valued covariance vector-based sparse Bayesian learning framework;MIMO array system;linear transformation;independent prior Gaussian distribution;real-valued covariance vector-based sparse Bayesian model;sparse signal representation based methods;sparse DOA estimation;direction of arrival estimation;MIMO communication;Estimation;Direction-of-arrival estimation;Phased arrays;Bayes methods;Receiving antennas;MIMO array system;nonuniform noise;DOA estimation;real-valued sparse signal recover;expectation-maximization algorithm},
doi={10.1109/ACCESS.2018.2870257},
ISSN={2169-3536},
month={},}
@ARTICLE{8274960,
author={I. Ghribi and R. B. Abdallah and M. Khalgui and Z. Li and K. Alnowibet and M. Platzner},
journal={IEEE Access},
title={R-Codesign: Codesign Methodology for Real-Time Reconfigurable Embedded Systems Under Energy Constraints},
year={2018},
volume={6},
number={},
pages={14078-14092},
abstract={Hardware/software codesign involves various design problems, including system specification, design space exploration, hardware/software co-verification, and system synthesis. An effective codesign process requires accurately predicting the performance, cost and power consequence of any design trade-off in algorithms and hardware configuration. This paper presents a new co-design methodology called R-codesign. Based on new modeling and partitioning techniques for reconfigurable embedded systems, R-codesign creates a task allocation of SW functions and HW behaviors based on the user constraints and using heuristics. The modeling approach relies basically on probabilistic estimations of the executions of system tasks. Hardware and software specifications are the inputs of R-codesign which constructs partitions (clusters of tasks) and maps them to a specified heterogeneous multiprocessor system-on-chip execution platform with field-programmable gate array. Several design constraints are evaluated and tested during the partitioning and mapping process. We have developed a visual environment called SPEX that implements this methodology. SPEX computes a control matrix which is a pre-computation of validated mappings that will occur in a case of a system reconfiguration. SPEX is an open source, fast and provides efficient results for the codesign of reconfigurable embedded systems.},
keywords={embedded systems;field programmable gate arrays;formal specification;formal verification;hardware-software codesign;integrated circuit design;multiprocessing systems;probability;public domain software;reconfigurable architectures;system-on-chip;codesign methodology;real-time reconfigurable embedded systems;energy constraints;design problems;system specification;design space exploration;system synthesis;SPEX;visual environment;field-programmable gate array;system task executions;probabilistic estimations;HW behaviors;SW functions;hardware-software co-verification;heuristics;hardware-software codesign;system reconfiguration;mapping process;design constraints;specified heterogeneous multiprocessor system-on-chip execution platform;software specifications;user constraints;task allocation;partitioning techniques;R-codesign;hardware configuration;design trade-off;power consequence;effective codesign process;Hardware;Task analysis;Embedded systems;Partitioning algorithms;Electronic mail;Program processors;Embedded system;reconfiguration;real-time;co-design;MPSoC;FPGA},
doi={10.1109/ACCESS.2018.2799852},
ISSN={2169-3536},
month={},}
@ARTICLE{8391714,
author={R. Sanchez-Reillo and J. Liu-Jimenez and R. Blanco-Gonzalo},
journal={IEEE Access},
title={Forensic Validation of Biometrics Using Dynamic Handwritten Signatures},
year={2018},
volume={6},
number={},
pages={34149-34157},
abstract={Forensic examination of handwritten signatures is an important task that has been used to resolve conflicts for centuries. The incorporation of new technologies into the process of signing documents has created new challenges for this task. In particular, the use of electronic capture devices may compromise the capabilities of forensic examination. However, the forensic examination may not be challenged and may even be improved if, in addition to the signature graph, the temporal signals that are generated during the process of signing are captured. In biometric terms, the acquisition and processing of such temporal signals are referred to as dynamic signature biometric recognition. Unfortunately, the data are captured in a format that a forensic examiner is unable to understand. Therefore, there is a need of adapting this information to allow a forensic examiner to manipulate it and obtain the required measurements. This paper explains this need using the design and development of a desktop application as the guiding thread. After covering this need, a forensic examiner can extract the relevant graphometric features that are necessary for applying graphonomics to signatures and determining the authenticity of a questioned signature compared with a certain signature.},
keywords={feature extraction;handwriting recognition;image forensics;dynamic handwritten signatures;forensic examination;dynamic signature biometric recognition;biometrics forensic validation;electronic capture devices;signature graph;temporal signal processing;desktop application;graphometric feature extraction;graphonomics;Forensics;Biometrics (access control);Feature extraction;Error analysis;Smart phones;Performance evaluation;Task analysis;Dynamic handwritten signature;forensic application;forensic examination;graphometric features;graphonomics;questioned signatures},
doi={10.1109/ACCESS.2018.2849503},
ISSN={2169-3536},
month={},}
@ARTICLE{7762909,
author={J. Mertz and I. Nunes},
journal={IEEE Transactions on Software Engineering},
title={A Qualitative Study of Application-Level Caching},
year={2017},
volume={43},
number={9},
pages={798-816},
abstract={Latency and cost of Internet-based services are encouraging the use of application-level caching to continue satisfying users' demands, and improve the scalability and availability of origin servers. Despite its popularity, this level of caching involves the manual implementation by developers and is typically addressed in an ad-hoc way, given that it depends on specific details of the application. As a result, application-level caching is a time-consuming and error-prone task, becoming a common source of bugs. Furthermore, it forces application developers to reason about a crosscutting concern, which is unrelated to the application business logic. In this paper, we present the results of a qualitative study of how developers handle caching logic in their web applications, which involved the investigation of ten software projects with different characteristics. The study we designed is based on comparative and interactive principles of grounded theory, and the analysis of our data allowed us to extract and understand how developers address cache-related concerns to improve performance and scalability of their web applications. Based on our analysis, we derived guidelines and patterns, which guide developers while designing, implementing and maintaining application-level caching, thus supporting developers in this challenging task that is crucial for enterprise web applications.},
keywords={cache storage;Internet;project management;software management;application-level caching;Internet-based services;caching logic;Web applications;software projects;Databases;Guidelines;Maintenance engineering;Servers;Software;Scalability;HTML;Application-level caching;qualitative study;pattern;guideline;web application},
doi={10.1109/TSE.2016.2633992},
ISSN={0098-5589},
month={Sept},}
@ARTICLE{8361072,
author={R. R. Atallah and A. Kamsin and M. A. Ismail and S. A. Abdelrahman and S. Zerdoumi},
journal={IEEE Access},
title={Face Recognition and Age Estimation Implications of Changes in Facial Features: A Critical Review Study},
year={2018},
volume={6},
number={},
pages={28290-28304},
abstract={Facial features are considered as one of the important personal characteristics. This can be used in many applications, such as face recognition and age estimation. The value of these applications depends in several areas, such as security applications, law enforcement applications, and attendance systems. In addition, facial features are particularly the key usage in the finding of lost child. Present applications have achieved a high level of accuracy. This paper provides a survey of face recognition, including the age estimation, which was discussed. Moreover, the research outlines several challenges faced in face recognition area that had been explored. The research also provides a landscape mapping based on integrating into a critical and coherent taxonomy. In the methodology sections, the exploration the accomplished via a deep focused in every single article in &#x201C;Face Recognition&#x201D;, then &#x201C;Age Estimation&#x201D;, and later in &#x201C;Facial Features&#x201D;. The &#x201C;Articles extraction&#x201D;is mining from diverse sources, such as Web of Science, ACM, IEEE, Science Direct, and Springer databases. The research covers overall 72 articles; 32/72 articles were face recognition. Moreover, 39/72 of the articles were for age estimation. A comparison based on the objectives of the approaches is presented to underline the taxonomy. Ending by research conclusion on face techniques contributes to the understanding of the recognition approaches, which can be used in future researches. The research concluded that face techniques' performance is distinct from one data set to another. This paper contributes to display gaps for other researchers to join this line of research.},
keywords={face recognition;feature extraction;facial features;security applications;law enforcement applications;IEEE database;Springer database;Web of Science database;ACM database;Science Direct database;coherent taxonomy;critical taxonomy;landscape mapping;attendance systems;age estimation;face recognition;Face recognition;Face;Databases;Estimation;Support vector machines;Facial features;Feature extraction;Face recognition;age estimation;aging},
doi={10.1109/ACCESS.2018.2836924},
ISSN={2169-3536},
month={},}
@ARTICLE{8022867,
author={A. S. A. Mohamed Sid Ahmed and R. Hassan and N. E. Othman},
journal={IEEE Access},
title={IPv6 Neighbor Discovery Protocol Specifications, Threats and Countermeasures: A Survey},
year={2017},
volume={5},
number={},
pages={18187-18210},
abstract={Neighbor discovery protocol (NDP) is the core protocol of Internet protocol version 6 (IPv6) suite. The motive behind NDP is to replace address resolution protocol (ARP), router discovery, and redirect functions in Internet protocol version 4. NDP is known as the stateless protocol as it is utilized by the IPv6 nodes to determine joined hosts as well as routers in an IPv6 network without the need of dynamic host configuration protocol server. NDP is susceptible to attacks due to the deficiency in its authentication process. Securing NDP is extremely crucial as the Internet is prevalent nowadays and it is widely used in communal areas, for instance, airports, where trust does not exist among the users. A malicious host is able to expose denial of service or man-in-the-middle attacks by injecting spoofed address in NDP messages. With the intention to protect the NDP many solutions were proposed by researchers. However, these solutions either introduced new protocols that need to be supported by all nodes or built mechanisms that require the cooperation of all nodes. Moreover, some solutions are deviating from the layering principals of open system interconnection model. Therefore, the necessity to study NDP in details to recognize and identify the points that could be a source of enhancement has become mandatory task. This article revolves around the survey of the vulnerabilities mitigations approaches of NDP, since the time of the protocol development up to the date of finalized this paper. We described the technical specifications of NDP showing its components, functions, and working procedures. In addition, each threat of NDP is classified and explained in details. Open challenges of NDP and recommended future directions for scientific research are presented at the end of this paper.},
keywords={computer network security;Internet;IP networks;transport protocols;neighbor discovery protocol;Internet protocol version 6 suite;address resolution protocol;Internet protocol version 4;IPv6 nodes;IPv6 network;dynamic host configuration protocol server;NDP protocol;Routing protocols;Internet;Security;Open systems;IP networks;Unicast;IPv6;NDP;SEND;CGA},
doi={10.1109/ACCESS.2017.2737524},
ISSN={2169-3536},
month={},}
